{
 "metadata": {
  "name": "",
  "signature": "sha256:f4855e03109c4570fa30cc0b0e529b8bb7d1c6904fafa52413d34a6bc6d499ae"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"left\" src=\"images/laf-fabric-xsmall.png\"/></a>\n",
      "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"left\" src=\"images/VU-ETCBC-xsmall.png\"/></a>\n",
      "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"left\"src=\"images/etcbc4easy-small.png\"/></a>\n",
      "<a href=\"http://tla.mpi.nl\" target=\"_blank\"><img align=\"right\" src=\"images/TLA-xsmall.png\"/></a>\n",
      "<a href=\"http://www.dans.knaw.nl\" target=\"_blank\"><img align=\"right\"src=\"images/DANS-xsmall.png\"/></a>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Lexeme spaces"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Research Question"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to research characteristics that separate early biblical Hebrew from late biblical Hebrew and even the intermediate stages, we need to count lexemes in a sophisticated way.\n",
      "\n",
      "The problem is that there are books with many different lexemes and books with fewer lexemes. If we compare them in a straightforward manner, it is not obvious whether we can ascribe the differences to language variation over time or to genre or just random fluctuation.\n",
      "\n",
      "We need to get more grip on the potential lexeme choice for each book."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Scope of a lexeme"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Scope of a lexeme\n",
      "\n",
      "We approximate the *potential lexeme choice* by investigating *lexeme spaces*.\n",
      "\n",
      "For each lexeme we count as follows:\n",
      "\n",
      "For each book in which the lexeme occurs, count the number of distinct lexemes that have at least one occurrence in that book.\n",
      "Take the sum of all these values.\n",
      "\n",
      "This sum is a measure for the scope of the lexeme in question.\n",
      "\n",
      "## Lexeme space of a book\n",
      "\n",
      "Now for each book in the bible, plot the lexeme scopes as a cloud of points, whose $x$-coordinate is the scope of a lexeme,\n",
      "and whose $y$-coordinate is the number of lexemes with that scope."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Fire up"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import collections\n",
      "import matplotlib.pyplot as plt\n",
      "from laf.fabric import LafFabric\n",
      "fabric = LafFabric()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s This is LAF-Fabric 4.4.3\n",
        "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
        "Feature doc: http://shebanq-doc.readthedocs.org/en/latest/texts/welcome.html\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fabric.load('etcbc4', '--', 'lexemes', {\n",
      "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
      "    \"features\": (\"otype g_word language sp lex book chapter\", \"\"),\n",
      "})\n",
      "exec(fabric.localnames.format(var='fabric'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s LOADING API: please wait ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s INFO: USING DATA COMPILED AT: 2014-07-23T09-31-37\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.23s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX -- FOR TASK lexemes AT 2014-09-30T14-02-03\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Compute\n",
      "## Lexemes\n",
      "Compute the set of lexemes contained in every book."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "msg(\"Fetching lexemes\")\n",
      "cur_book = None\n",
      "book_sizes = collections.Counter()\n",
      "book_sizes_aram = collections.Counter()\n",
      "lexeme_books = collections.defaultdict(lambda: set())\n",
      "for n in NN():\n",
      "    otype = F.otype.v(n)\n",
      "    if otype == 'book':\n",
      "        cur_book = F.book.v(n)\n",
      "        continue\n",
      "    if otype != 'word': continue\n",
      "    lex = F.lex.v(n)\n",
      "    lan = F.language.v(n)\n",
      "    book_sizes[cur_book] += 1\n",
      "    if lan == 'Aramaic':\n",
      "        book_sizes_aram[cur_book] += 1\n",
      "    lexeme_books[lex].add(cur_book)\n",
      "\n",
      "msg(\"{} lexemes\".format(len(lexeme_books)))\n",
      "for (book, size) in sorted(book_sizes.items(), key=lambda x:(-x[1], x[0])):\n",
      "    print(\"{:<15} has {:>5} words of which {:>} Aramaic\".format(book, size, book_sizes_aram[book]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "11m 18s Fetching lexemes\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "11m 21s 8766 lexemes\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Jeremia         has 29735 words of which 19 Aramaic\n",
        "Genesis         has 28756 words of which 2 Aramaic\n",
        "Ezechiel        has 26180 words of which 0 Aramaic\n",
        "Psalmi          has 25371 words of which 2 Aramaic\n",
        "Exodus          has 23748 words of which 0 Aramaic\n",
        "Numeri          has 23186 words of which 0 Aramaic\n",
        "Jesaia          has 22934 words of which 0 Aramaic\n",
        "Deuteronomium   has 20127 words of which 0 Aramaic\n",
        "Chronica_II     has 19760 words of which 0 Aramaic\n",
        "Samuel_I        has 18926 words of which 0 Aramaic\n",
        "Reges_I         has 18684 words of which 0 Aramaic\n",
        "Reges_II        has 17305 words of which 0 Aramaic\n",
        "Leviticus       has 17099 words of which 0 Aramaic\n",
        "Samuel_II       has 15612 words of which 0 Aramaic\n",
        "Chronica_I      has 15561 words of which 0 Aramaic\n",
        "Josua           has 14523 words of which 0 Aramaic\n",
        "Judices         has 14084 words of which 0 Aramaic\n",
        "Iob             has 10912 words of which 0 Aramaic\n",
        "Proverbia       has  8859 words of which 0 Aramaic\n",
        "Daniel          has  8071 words of which 4560 Aramaic\n",
        "Nehemia         has  7842 words of which 0 Aramaic\n",
        "Esra            has  5268 words of which 1523 Aramaic\n",
        "Esther          has  4621 words of which 0 Aramaic\n",
        "Sacharia        has  4469 words of which 0 Aramaic\n",
        "Ecclesiastes    has  4233 words of which 0 Aramaic\n",
        "Hosea           has  3146 words of which 0 Aramaic\n",
        "Amos            has  2780 words of which 0 Aramaic\n",
        "Threni          has  1945 words of which 0 Aramaic\n",
        "Micha           has  1895 words of which 0 Aramaic\n",
        "Ruth            has  1802 words of which 0 Aramaic\n",
        "Canticum        has  1682 words of which 0 Aramaic\n",
        "Joel            has  1318 words of which 0 Aramaic\n",
        "Maleachi        has  1187 words of which 0 Aramaic\n",
        "Zephania        has  1037 words of which 0 Aramaic\n",
        "Jona            has   985 words of which 0 Aramaic\n",
        "Habakuk         has   897 words of which 0 Aramaic\n",
        "Haggai          has   877 words of which 0 Aramaic\n",
        "Nahum           has   746 words of which 0 Aramaic\n",
        "Obadia          has   392 words of which 0 Aramaic\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Language use\n",
      "Visualize the Aramaic language use versus the Hebrew language use."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rep = {'heb': '_', 'arm': '-'}\n",
      "repi = dict((y,x) for (x,y) in rep.items())\n",
      "repo = dict((y, 'heb' if x == 'arm' else 'arm') for (x,y) in rep.items())\n",
      "condense = {'heb': 100, 'arm': 20}\n",
      "\n",
      "def distil(words):\n",
      "    n = {'heb': 0, 'arm': 0}\n",
      "    \n",
      "    def inc(this_lan, that_lan, passive=False):\n",
      "        if n[that_lan]:\n",
      "            n[that_lan] = 0\n",
      "            yield rep[that_lan]\n",
      "        if not passive:\n",
      "            n[this_lan] += 1\n",
      "            if n[this_lan] == condense[this_lan]:\n",
      "                n[this_lan] = 0\n",
      "                yield rep[this_lan]\n",
      "    for w in words:\n",
      "        for y in inc(repi[w], repo[w]): yield y\n",
      "    for y in inc('arm', 'heb', passive=True): yield y\n",
      "    for y in inc('heb', 'arm', passive=True): yield y\n",
      "        \n",
      "msg('''\n",
      "Showing the Aramaic:\n",
      "'{}' is {} Hebrew  word{} (or less but at least 1),\n",
      "'{}' is {} Aramaic word{} (or less but at least 1)'''.format(\n",
      "    rep['heb'], condense['heb'], '' if condense['heb'] == 1 else 's', \n",
      "    rep['arm'], condense['arm'], '' if condense['arm'] == 1 else 's',\n",
      "))\n",
      "skipping = True\n",
      "words = []\n",
      "for n in NN():\n",
      "    otype = F.otype.v(n)\n",
      "    if otype == 'book':\n",
      "        if words:\n",
      "            print(''.join(distil(words)))\n",
      "            words = []\n",
      "        cur_book = F.book.v(n)\n",
      "        skipping = book_sizes_aram[cur_book] == 0\n",
      "        if not skipping:\n",
      "            print(cur_book)\n",
      "        continue\n",
      "    if skipping or otype != 'word': continue\n",
      "    words.append(rep['arm'] if F.language.v(n) == 'Aramaic' else rep['heb'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "55m 06s \n",
        "Showing the Aramaic:\n",
        "'_' is 100 Hebrew  words (or less but at least 1),\n",
        "'-' is 20 Aramaic words (or less but at least 1)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Genesis\n",
        "_____________________________________________________________________________________________________________________________________________________________________________-___________________________________________________________________________________________________________________"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Jeremia"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "___________________________________________________-_______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Psalmi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "____________________________________________________________________________________________________________________________________________________________________________________________________________-___________________________________________________"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Daniel"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "______------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------______________________________"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Esra\n",
        "________________------------------------------------------------------------___-----------------____________________\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Scopes\n",
      "Compile the lexeme scopes of each lexeme."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lexeme_scope = {}\n",
      "for lex in lexeme_books:\n",
      "    lexeme_scope[lex] = sum(book_sizes[book] for book in lexeme_books[lex])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scope_file = outfile('lexeme_scopes.csv')\n",
      "for ls in sorted(lexeme_scope.items(), key=lambda x: (-x[1], x[0])):\n",
      "    scope_file.write('\"{}\";{}\\n'.format(*ls))\n",
      "scope_file.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Profiles\n",
      "Compile the profiles of lexical scopes for each book"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "book_profile = collections.defaultdict(lambda: collections.Counter)\n",
      "for lex in lexeme_books:\n",
      "    for book in lexeme_books[lex]:\n",
      "        book_profile[book][lexeme_scope[lex]] += 1\n",
      "\n",
      "book_file = outfile('book_profiles.txt')\n",
      "for book in sorted(book_profile):\n",
      "    book_file.write(\"{}\\n\".format(book))\n",
      "    for scope in sorted(book_profile[book]):\n",
      "        book_file.write(\"{:>7} x {:>5}\\n\".format(scope, book_profile[book][scope]))\n",
      "book_file.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Make buckets of scopes, in a logaritmic scale, and bucketize the book profiles.\n",
      "\n",
      "We choose base 2 for the buckets, so we have buckets for scope 1, 2, 3-4, 5-8, 9-16 and so on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bucketize(data):\n",
      "    bucketed = collections.Counter\n",
      "    for (scope, n) in data.items():\n",
      "        the_bucket = int.bit_length(scope)\n",
      "        bucketed[the_bucket] += n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}