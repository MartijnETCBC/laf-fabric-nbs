{
 "metadata": {
  "name": "",
  "signature": "sha256:d80a6ef72f9158eb4a845669e13d0bac32ce1596e6a4df010383d183c94810c1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"left\" src=\"images/laf-fabric-xsmall.png\"/></a>\n",
      "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"left\" src=\"images/VU-ETCBC-xsmall.png\"/></a>\n",
      "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"left\"src=\"images/etcbc4easy-small.png\"/></a>\n",
      "<a href=\"http://tla.mpi.nl\" target=\"_blank\"><img align=\"right\" src=\"images/TLA-xsmall.png\"/></a>\n",
      "<a href=\"http://www.dans.knaw.nl\" target=\"_blank\"><img align=\"right\"src=\"images/DANS-xsmall.png\"/></a>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Lexeme spaces"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Research Question"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Following an idea of Martijn Naaijer:\n",
      "\n",
      "In order to research characteristics that separate early biblical Hebrew from late biblical Hebrew and even the intermediate stages, we need to count lexemes in a sophisticated way.\n",
      "\n",
      "The problem is that there are books with many different lexemes and books with fewer lexemes. If we compare them in a straightforward manner, it is not obvious whether we can ascribe the differences to language variation over time or to genre or just random fluctuation, or even the mere size of the book.\n",
      "\n",
      "We need to get more grip on the potential lexeme choice for each book."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Scope of a lexeme\n",
      "\n",
      "We approximate the *potential lexeme choice* by investigating *lexeme spaces*.\n",
      "\n",
      "For each lexeme we count as follows:\n",
      "\n",
      "The *lexeme scope* of a lexeme is the sum of the lengths (in words) of the books in which lexeme occur.\n",
      "\n",
      "## Lexeme profile of a book\n",
      "\n",
      "Now for each book in the bible, plot its lexemes as a cloud of points \n",
      "where the $x$-axis corresponds to the lexeme scope and the $y$-axis to the number of lexemes in that book for each scope.\n",
      "\n",
      "For convenience, we divide the lexeme scopes on the $x$-axis in intervals, called *buckets*.\n",
      "and then the $y$-values are the number of lexemes in the book whose scope is within the buckets in question.\n",
      "We also group the $y$ values together, we call them the *n-ranges*.\n",
      "\n",
      "So, on the $x$-axis we have buckets of lexeme scopes, and on the $y$-axis we have *n-ranges*,\n",
      "such that if a point links a bucket $b$ to an n-range $n$, the number of lexemes in $b$ is within $n$.\n",
      "\n",
      "We generate several book profiles, based on various subsets of lexemes and with different interval settings.\n",
      "\n",
      "## Subsets of lexemes\n",
      "\n",
      "We work with the following subsets of lexemes:\n",
      "\n",
      "* ``all``: all lexemes\n",
      "* ``c``: all lexemes except proper nouns\n",
      "* ``nva``: only nouns, verbs and adjectives, no proper nouns\n",
      "\n",
      "We work with two kinds of intervals: *linear* and *logarithmic*.\n",
      "\n",
      "The linear intervals are specified by two numbers: the *bucket size* ``B`` and the *n-range size* ``N``.\n",
      "A nice graph is obtained by taking ``B`` $= 10000$ and ``N`` $=10$.\n",
      "So we group lexeme scopes in intervals of 10000 and we group the numbers of lexemes in groups of 10.\n",
      "\n",
      "Logarithmic intervals start small and increase exponentially: \n",
      "take the number of bits used to represent a number instead of that number itself.\n",
      "So the logaritmic scale goes as follows:\n",
      "\n",
      "    0 1 2-3 4-7 8-15 16-31 32-63 64-127 128-255 256-511 512-1023 etc\n",
      "   \n",
      "We do this both horizontally and vertically.\n",
      "\n",
      "# Tasks\n",
      "\n",
      "We generate sets of book profiles for all the books, directed by the ``TASKS`` variable below.\n",
      "It specifies the interval method (*linear* or *logarithmic*) and if linear, it specifies the interval size for buckets and for n-ranges.\n",
      "\n",
      "All tasks are run separately for all possible lexeme subsets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TASKS = (\n",
      "    ('log',),\n",
      "    ('linear', 10000, 10),\n",
      "    ('linear', 5000, 5),\n",
      "    ('linear', 2500, 1),\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Fire up"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import collections\n",
      "import matplotlib.pyplot as plt\n",
      "from laf.fabric import LafFabric\n",
      "fabric = LafFabric()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s This is LAF-Fabric 4.4.3\n",
        "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
        "Feature doc: http://shebanq-doc.readthedocs.org/en/latest/texts/welcome.html\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fabric.load('etcbc4', '--', 'lexemes', {\n",
      "    'xmlids': {'node': False, 'edge': False},\n",
      "    'features': ('''otype g_word language sp lex book chapter''', ''),\n",
      "})\n",
      "exec(fabric.localnames.format(var='fabric'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s LOADING API: please wait ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s INFO: USING DATA COMPILED AT: 2014-07-23T09-31-37\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.09s LOGFILE=/Users/dirk/laf-fabric-output/etcbc4/lexemes/__log__lexemes.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.09s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX -- FOR TASK lexemes AT 2014-10-02T14-42-26\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Compute\n",
      "## Lexemes\n",
      "Compute the set of lexemes contained in every book."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "msg('Fetching lexemes')\n",
      "sources = {'all', 'c', 'nva'}\n",
      "cur_book = None\n",
      "book_sizes = collections.Counter()\n",
      "book_sizes_aram = collections.Counter()\n",
      "lexeme_books = collections.defaultdict(lambda: collections.defaultdict(lambda: set()))\n",
      "for n in NN():\n",
      "    otype = F.otype.v(n)\n",
      "    if otype == 'book':\n",
      "        cur_book = F.book.v(n)\n",
      "        continue\n",
      "    if otype != 'word': continue\n",
      "    lex = F.lex.v(n)\n",
      "    lan = F.language.v(n)\n",
      "    sp = F.sp.v(n)\n",
      "    book_sizes[cur_book] += 1\n",
      "    if lan == 'Aramaic':\n",
      "        book_sizes_aram[cur_book] += 1\n",
      "    if True: lexeme_books['all'][lex].add(cur_book)\n",
      "    if sp != 'nmpr': lexeme_books['c'][lex].add(cur_book)\n",
      "    if sp == 'verb' or sp == 'subs' or sp == 'adjv': lexeme_books['nva'][lex].add(cur_book)\n",
      "\n",
      "msg('{} lexemes'.format(len(lexeme_books)))\n",
      "for (book, size) in sorted(book_sizes.items(), key=lambda x:(-x[1], x[0])):\n",
      "    print('{:<15} has {:>5} words of which {:>} Aramaic'.format(book, size, book_sizes_aram[book]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.50s Fetching lexemes\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  7.09s 3 lexemes\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Jeremia         has 29735 words of which 19 Aramaic\n",
        "Genesis         has 28756 words of which 2 Aramaic\n",
        "Ezechiel        has 26180 words of which 0 Aramaic\n",
        "Psalmi          has 25371 words of which 2 Aramaic\n",
        "Exodus          has 23748 words of which 0 Aramaic\n",
        "Numeri          has 23186 words of which 0 Aramaic\n",
        "Jesaia          has 22934 words of which 0 Aramaic\n",
        "Deuteronomium   has 20127 words of which 0 Aramaic\n",
        "Chronica_II     has 19760 words of which 0 Aramaic\n",
        "Samuel_I        has 18926 words of which 0 Aramaic\n",
        "Reges_I         has 18684 words of which 0 Aramaic\n",
        "Reges_II        has 17305 words of which 0 Aramaic\n",
        "Leviticus       has 17099 words of which 0 Aramaic\n",
        "Samuel_II       has 15612 words of which 0 Aramaic\n",
        "Chronica_I      has 15561 words of which 0 Aramaic\n",
        "Josua           has 14523 words of which 0 Aramaic\n",
        "Judices         has 14084 words of which 0 Aramaic\n",
        "Iob             has 10912 words of which 0 Aramaic\n",
        "Proverbia       has  8859 words of which 0 Aramaic\n",
        "Daniel          has  8071 words of which 4560 Aramaic\n",
        "Nehemia         has  7842 words of which 0 Aramaic\n",
        "Esra            has  5268 words of which 1523 Aramaic\n",
        "Esther          has  4621 words of which 0 Aramaic\n",
        "Sacharia        has  4469 words of which 0 Aramaic\n",
        "Ecclesiastes    has  4233 words of which 0 Aramaic\n",
        "Hosea           has  3146 words of which 0 Aramaic\n",
        "Amos            has  2780 words of which 0 Aramaic\n",
        "Threni          has  1945 words of which 0 Aramaic\n",
        "Micha           has  1895 words of which 0 Aramaic\n",
        "Ruth            has  1802 words of which 0 Aramaic\n",
        "Canticum        has  1682 words of which 0 Aramaic\n",
        "Joel            has  1318 words of which 0 Aramaic\n",
        "Maleachi        has  1187 words of which 0 Aramaic\n",
        "Zephania        has  1037 words of which 0 Aramaic\n",
        "Jona            has   985 words of which 0 Aramaic\n",
        "Habakuk         has   897 words of which 0 Aramaic\n",
        "Haggai          has   877 words of which 0 Aramaic\n",
        "Nahum           has   746 words of which 0 Aramaic\n",
        "Obadia          has   392 words of which 0 Aramaic\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Language use\n",
      "As an aside we\n",
      "visualize the Aramaic language use versus the Hebrew language use."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rep = {'heb': '_', 'arm': '-'}\n",
      "repi = dict((y,x) for (x,y) in rep.items())\n",
      "repo = dict((y, 'heb' if x == 'arm' else 'arm') for (x,y) in rep.items())\n",
      "condense = {'heb': 100, 'arm': 20}\n",
      "\n",
      "def distil(words):\n",
      "    n = {'heb': 0, 'arm': 0}\n",
      "    \n",
      "    def inc(this_lan, that_lan, passive=False):\n",
      "        if n[that_lan]:\n",
      "            n[that_lan] = 0\n",
      "            yield rep[that_lan]\n",
      "        if not passive:\n",
      "            n[this_lan] += 1\n",
      "            if n[this_lan] == condense[this_lan]:\n",
      "                n[this_lan] = 0\n",
      "                yield rep[this_lan]\n",
      "    for w in words:\n",
      "        for y in inc(repi[w], repo[w]): yield y\n",
      "    for y in inc('arm', 'heb', passive=True): yield y\n",
      "    for y in inc('heb', 'arm', passive=True): yield y\n",
      "        \n",
      "msg('''\n",
      "Showing the Aramaic:\n",
      "'{}' is {} Hebrew  word{} (or less but at least 1),\n",
      "'{}' is {} Aramaic word{} (or less but at least 1)'''.format(\n",
      "    rep['heb'], condense['heb'], '' if condense['heb'] == 1 else 's', \n",
      "    rep['arm'], condense['arm'], '' if condense['arm'] == 1 else 's',\n",
      "))\n",
      "skipping = True\n",
      "words = []\n",
      "for n in NN():\n",
      "    otype = F.otype.v(n)\n",
      "    if otype == 'book':\n",
      "        if words:\n",
      "            print(''.join(distil(words)))\n",
      "            words = []\n",
      "        cur_book = F.book.v(n)\n",
      "        skipping = book_sizes_aram[cur_book] == 0\n",
      "        if not skipping:\n",
      "            print(cur_book)\n",
      "        continue\n",
      "    if skipping or otype != 'word': continue\n",
      "    words.append(rep['arm'] if F.language.v(n) == 'Aramaic' else rep['heb'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    11s \n",
        "Showing the Aramaic:\n",
        "'_' is 100 Hebrew  words (or less but at least 1),\n",
        "'-' is 20 Aramaic words (or less but at least 1)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Genesis\n",
        "_____________________________________________________________________________________________________________________________________________________________________________-___________________________________________________________________________________________________________________"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Jeremia"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "___________________________________________________-_______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Psalmi"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "____________________________________________________________________________________________________________________________________________________________________________________________________________-___________________________________________________"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Daniel"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "______------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------______________________________"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Esra\n",
        "________________------------------------------------------------------------___-----------------____________________\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Scopes and profiles\n",
      "Compile the scopes of all lexemes and then the profiles of lexical scopes for each book.\n",
      "\n",
      "Make buckets of scopes, in a logaritmic scale, and bucketize the book profiles."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "hdash = '\u2500'\n",
      "vdash = '\u2502'\n",
      "\n",
      "def make_profiles(source):\n",
      "    book_profile = collections.defaultdict(lambda: collections.Counter())\n",
      "    lexeme_bookss = lexeme_books[source]\n",
      "\n",
      "    lexeme_scope = {}\n",
      "    for lex in lexeme_bookss:\n",
      "        lexeme_scope[lex] = sum(book_sizes[book] for book in lexeme_bookss[lex])\n",
      "\n",
      "    scope_file = outfile('scopes_{}.csv'.format(source))\n",
      "    for ls in sorted(lexeme_scope.items(), key=lambda x: (-x[1], x[0])):\n",
      "        scope_file.write('\"{}\";{}\\n'.format(*ls))\n",
      "    scope_file.close()\n",
      "\n",
      "    for lex in lexeme_bookss:\n",
      "        for book in lexeme_bookss[lex]:\n",
      "            book_profile[book][lexeme_scope[lex]] += 1\n",
      "\n",
      "    book_file = outfile('profiles_{}.csv'.format(source))\n",
      "    for book in sorted(book_profile):\n",
      "        book_file.write('\"{}\"\\n'.format(book))\n",
      "        for scope in sorted(book_profile[book]):\n",
      "            book_file.write('{};{}\\n'.format(scope, book_profile[book][scope]))\n",
      "    book_file.close()\n",
      "\n",
      "    return book_profile\n",
      "\n",
      "def bucketize(data, method, params=()):\n",
      "    if method == 'log':\n",
      "        buckets_proto = collections.Counter()\n",
      "        buckets = collections.Counter()\n",
      "        for (scope, n) in data.items(): buckets_proto[int.bit_length(scope)] += n\n",
      "        buckets = dict((b, int.bit_length(n)) for (b, n) in buckets_proto.items())\n",
      "        return (buckets, max(buckets.keys()), max(buckets.values()))\n",
      "    if method == 'linear':\n",
      "        (B_LINEAR, N_LINEAR) = params\n",
      "        buckets_proto = collections.Counter()\n",
      "        buckets = collections.Counter()\n",
      "        for (scope, n) in data.items(): buckets_proto[int(scope // B_LINEAR)] += n\n",
      "        buckets = dict((b, n // N_LINEAR) for (b, n) in buckets_proto.items())\n",
      "        return (buckets, max(buckets.keys()), max(buckets.values()))\n",
      "\n",
      "def show_buckets(buckets, book, maxb, maxn):\n",
      "    bucket_rows = collections.defaultdict(lambda: set())\n",
      "    for (b, n) in buckets.items(): bucket_rows[n].add(b)\n",
      "    lines = []\n",
      "    lines.append('{:<4}\u250f{}{}{}\u2513'.format(' ',hdash * 4, book, hdash * (maxb + 1 - len(book) - 4)))\n",
      "    max_row_show = maxn\n",
      "    max_row_show = max(bucket_rows.keys())\n",
      "    for row in range(max_row_show + 1, 0, -1):\n",
      "        reps = ('\u2588' if b in bucket_rows[row] else ' ' for b in range(maxb + 1))\n",
      "        lines.append('{:<4}\u2503{}\u2503'.format(row, ''.join(reps)))\n",
      "    lines.append('{:<4}\u2517{}\u251b'.format(' ', hdash * (maxb + 1)))\n",
      "    lines.append('{:<4} 0{}{}'.format(' ', ' ' * (maxb - 1), maxb))\n",
      "    lines.append('\\n')\n",
      "    return '\\n'.join(lines)\n",
      "\n",
      "\n",
      "def show_books(source, method, params=()):\n",
      "    book_buckets = {}\n",
      "    maxb = -1\n",
      "    maxn = -1\n",
      "    paramstr = '-'.join(str(x) for x in params)\n",
      "\n",
      "    book_profile = make_profiles(source)\n",
      "    for book in sorted(book_profile):\n",
      "        (buckets, this_maxb, this_maxn) = bucketize(book_profile[book], method, params)    \n",
      "        if this_maxb > maxb: maxb = this_maxb\n",
      "        if this_maxn > maxn: maxn = this_maxn\n",
      "        book_buckets[book] = buckets\n",
      "\n",
      "    book_file_bucket = outfile('data_{}_{}{}.csv'.format(source, method, paramstr))\n",
      "    for book in sorted(book_buckets):\n",
      "        buckets = book_buckets[book]\n",
      "        book_file_bucket.write('\"{}\";\\n'.format(book))\n",
      "        #for bn in sorted(book_buckets[book].items()):\n",
      "        for b in range(max(buckets.keys()) + 1):\n",
      "            book_file_bucket.write('{};{}\\n'.format(b, buckets.get(b, 0)))\n",
      "    book_file_bucket.close()\n",
      "\n",
      "    book_file_graph = outfile('graph_{}_{}{}.txt'.format(source, method, paramstr))\n",
      "    message = '''\n",
      "BUCKET METHOD = {}\n",
      "HORIZONTAL    = the lexeme scope\n",
      "VERTICAL      = the number of lexemes in that scope\n",
      "'''.format(method)\n",
      "    book_file_graph.write(message + '\\n')\n",
      "    for book in sorted(book_buckets):\n",
      "        graph = show_buckets(book_buckets[book], book, maxb, maxn)\n",
      "        book_file_graph.write(graph)\n",
      "    book_file_graph.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for task in TASKS:\n",
      "    for source in sources:\n",
      "        method = task[0]\n",
      "        params = task[1:]\n",
      "        print('Profiles from {:<5} with method {:<6} {}'.format(source, method, params))\n",
      "        show_books(source, method, params=params)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Profiles from c     with method log    ()\n",
        "Profiles from nva   with method log    ()"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Profiles from all   with method log    ()"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Profiles from c     with method linear (10000, 10)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Profiles from nva   with method linear (10000, 10)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Profiles from all   with method linear (10000, 10)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Profiles from c     with method linear (5000, 5)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Profiles from nva   with method linear (5000, 5)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Profiles from all   with method linear (5000, 5)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Profiles from c     with method linear (2500, 1)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Profiles from nva   with method linear (2500, 1)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Profiles from all   with method linear (2500, 1)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}