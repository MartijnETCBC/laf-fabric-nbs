{
 "metadata": {
  "name": "",
  "signature": "sha256:49eed5713f0a2e40b77afbb4e776e5898fa07b8391cab988cfba90b8daa71b5b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"left\" src=\"images/laf-fabric-xsmall.png\"/></a>\n",
      "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"left\" src=\"images/VU-ETCBC-xsmall.png\"/></a>\n",
      "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"left\"src=\"images/etcbc4easy-small.png\"/></a>\n",
      "<a href=\"http://tla.mpi.nl\" target=\"_blank\"><img align=\"right\" src=\"images/TLA-xsmall.png\"/></a>\n",
      "<a href=\"http://www.dans.knaw.nl\" target=\"_blank\"><img align=\"right\"src=\"images/DANS-xsmall.png\"/></a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Reading the ETCBC lexicon files\n",
      "\n",
      "We examine the ETCBC lexicon files, we check their internal consistency and their correspondence\n",
      "with the ETCBC4 database, \n",
      "and finally we will transform this data to\n",
      "[LMF (Lexical Markup Framework)](http://www.lexicalmarkupframework.org) [ISO-24613:2008](http://www.iso.org/iso/catalogue_detail.htm?csnumber=37327) for usage next to the LAF version of the Hebrew Text Database.\n",
      "An introduction to LMF can be found on [Wikipedia](http://en.wikipedia.org/wiki/Lexical_Markup_Framework).\n",
      "[Generate schemas](https://tla.mpi.nl/relish/lmf/)\n",
      "\n",
      "# Feature explanation\n",
      "\n",
      "Here is a list of features encountered in the lexicon.\n",
      "Some of them correspond to features encountered in the text.\n",
      "\n",
      "## entry \n",
      "corresponds to the ``lex`` feature of word occurrences.\n",
      "\n",
      "## vocalized lexeme\n",
      "``vc``  looks like the ``g_lex`` feature of word occurrence, but they are not the same. The ``vc`` is an idealized (aka *paradigmatic*), pointed representation of the lexeme, which may or may not occur in the text. The ``g_lex`` is the realized lexeme in a concrete occurrence, of which it may be a part.\n",
      "\n",
      "## root\n",
      "``rt`` does not correspond to a feature of word occurrences.\n",
      "It contains the abstract root of a lexeme, in the sense of the form from which the lexeme has been *derived*.\n",
      "\n",
      "## part of speech\n",
      "``sp`` corresponds exactly to ``sp`` on word occurrences.\n",
      "\n",
      "## subpart of speech (lexical set)\n",
      "``ls`` corresponds to exactly to ``ls`` on word occurrences.\n",
      "\n",
      "## gender\n",
      "``gn`` corresponds to ``gn`` on word occurrences, except for words whose gender is not lexically determined such as verbs and adjectives. The policy is to include this feature in the lexicon only when the gender is not obvious from the set of its textual occurrences.\n",
      "\n",
      "## number\n",
      "``nu`` corresponds to ``nu`` on word occurrences, but in general number is not lexically determined.\n",
      "\n",
      "## person\n",
      "``ps`` corresponds to ``ps`` on word occurrences, but in general person is not lexically detemined.\n",
      "It occurs on a small number of ``pspr`` s in the lexicon.\n",
      "\n",
      "## state\n",
      "``st`` corresponds to ``st`` on word occurrences. It is only marked for a few ``nmpr`` s in the lexicon.\n",
      "\n",
      "## reference type\n",
      "``sm``  does not correspond to a textual feature. It is the reference type of proper nouns and personal pronouns (rarely), and consists of a comma separated list of the following values:\n",
      "\n",
      "* ``gens`` people or tribe name\n",
      "* ``mens`` month name\n",
      "* ``pers`` person name\n",
      "* ``ppde`` possible demonstrative pronoun, occurs only for personal pronouns\n",
      "* ``topo`` place name\n",
      "\n",
      "## comment or corrections\n",
      "``co`` does not correspond to any feature on word occurrences. Comments are used to express alternatives for the information entered. Comments are very rare in the lexicon.\n",
      "\n",
      "## kb\n",
      "``kb`` unclear. In *arc* there is one occurrence containing a transliterated words, in *hbo* there are a few occurrences containing a different value for ``sp``.\n",
      "\n",
      "## fc\n",
      "``fc`` unclear."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Among the ETCBC data are lexicon files for the biblical languages.\n",
      "We want to put that data in LMF files, and put them to use in LAF-Fabric and SHEBANQ.\n",
      "\n",
      "We have lexicon files for the following languages (given in [ISO 639-3 codes and names](http://www-01.sil.org/iso639-3/default.asp))\n",
      "\n",
      "* ``hbo`` *Ancient Hebrew*\n",
      "* ``arc`` *Official Aramaic; Imperial Aramaic*\n",
      "* ``syc`` *Classical Syriac*"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Firing up the engines"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import collections\n",
      "\n",
      "import laf\n",
      "from laf.fabric import LafFabric\n",
      "from etcbc.preprocess import prepare\n",
      "\n",
      "fabric = LafFabric()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s This is LAF-Fabric 4.4.4\n",
        "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
        "Feature doc: http://shebanq-doc.readthedocs.org/en/latest/texts/welcome.html\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "API = fabric.load('etcbc4', '--', 'lexicon', {\n",
      "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
      "    \"features\": ('''\n",
      "        oid otype monads\n",
      "        book chapter verse\n",
      "        g_word g_word_utf8 trailer_utf8 language lex lex_utf8 g_lex g_lex_utf8\n",
      "        sp ls gn nu ps st\n",
      "    ''','''\n",
      "    '''),\n",
      "    \"prepare\": prepare,\n",
      "}, verbose='DETAIL')\n",
      "exec(fabric.localnames.format(var='fabric'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s LOADING API: please wait ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING m: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s INFO: USING DATA COMPILED AT: 2014-07-23T09-31-37\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING a: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.01s DETAIL: load main: G.node_anchor_min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.07s DETAIL: load main: G.node_anchor_max\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.13s DETAIL: load main: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.18s DETAIL: load main: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.57s DETAIL: load main: G.edges_from\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.63s DETAIL: load main: G.edges_to\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.70s DETAIL: load main: F.etcbc4_db_monads [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.42s DETAIL: load main: F.etcbc4_db_oid [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.11s DETAIL: load main: F.etcbc4_db_otype [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.77s DETAIL: load main: F.etcbc4_ft_g_lex [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.97s DETAIL: load main: F.etcbc4_ft_g_lex_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.25s DETAIL: load main: F.etcbc4_ft_g_word [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.51s DETAIL: load main: F.etcbc4_ft_g_word_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.82s DETAIL: load main: F.etcbc4_ft_gn [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.96s DETAIL: load main: F.etcbc4_ft_language [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.16s DETAIL: load main: F.etcbc4_ft_lex [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.34s DETAIL: load main: F.etcbc4_ft_lex_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.62s DETAIL: load main: F.etcbc4_ft_ls [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.82s DETAIL: load main: F.etcbc4_ft_nu [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.00s DETAIL: load main: F.etcbc4_ft_ps [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.19s DETAIL: load main: F.etcbc4_ft_sp [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.38s DETAIL: load main: F.etcbc4_ft_st [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.53s DETAIL: load main: F.etcbc4_ft_trailer_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.67s DETAIL: load main: F.etcbc4_sft_book [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.68s DETAIL: load main: F.etcbc4_sft_chapter [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.70s DETAIL: load main: F.etcbc4_sft_verse [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.71s LOGFILE=/Users/dirk/laf-fabric-output/etcbc4/lexicon/__log__lexicon.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.71s DETAIL: prep prep: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.77s DETAIL: prep prep: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  6.24s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX -- FOR TASK lexicon AT 2014-10-09T12-06-26\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Read the lexica\n",
      "\n",
      "We read the lexica for Hebrew and Aramaic.\n",
      "In the sequel we will consistently and exclusively refer to these languages by their ISO 639-3 abbreviations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "langs = {'hbo', 'arc', 'syc'}\n",
      "lex_base = dict((lan, '{}/{}/{}.{}'.format(API['data_dir'], 'lexicon', lan, 'etcbc')) for lan in langs)\n",
      "lex_base\n",
      "lang_map = {\n",
      "    'Hebrew': 'hbo',\n",
      "    'Aramaic': 'arc',\n",
      "    'Syriac': 'syc',\n",
      "}\n",
      "\n",
      "def read_lex(lan):\n",
      "    lex_infile = open(lex_base[lan], encoding='utf-8')\n",
      "    lex_outfile = outfile('{}.txt'.format(lan))\n",
      "    lex_errfile = outfile('{}.err.txt'.format(lan))\n",
      "\n",
      "    lex_items = {}\n",
      "    ln = 0\n",
      "    e = 0\n",
      "    for line in lex_infile:\n",
      "        ln += 1\n",
      "        line = line.rstrip()\n",
      "        line = line.split('#')[0]\n",
      "        if line == '': continue\n",
      "        (entry, featurestr) = line.split(sep=None, maxsplit=1)\n",
      "        entry = entry.strip('\"')\n",
      "        if entry in lex_items:\n",
      "            lex_errfile.write('duplicate lexical entry {} in line {}.\\n'.format(entry, ln))\n",
      "            e += 1\n",
      "            continue\n",
      "        featurestr = featurestr.strip(':')\n",
      "        featurestr = featurestr.replace('\\\\:', chr(254))\n",
      "        featurelst = featurestr.split(':')\n",
      "        features = {}\n",
      "        for feature in featurelst:\n",
      "            comps = feature.split('=', maxsplit=1)\n",
      "            if len(comps) == 1:\n",
      "                if feature.strip().isnumeric():\n",
      "                    comps = ('_n', feature.strip())\n",
      "                else:\n",
      "                    lex_errfile.write('feature without value for lexical entry {} in line {}: {}\\n'.format(entry, ln, feature))\n",
      "                    e += 1\n",
      "                    continue\n",
      "            (key, value) = comps\n",
      "            value = value.replace(chr(254), ':')\n",
      "            if key in features:\n",
      "                lex_errfile.write('duplicate feature for lexical entry {} in line {}: {}={}\\n'.format(entry, ln, key, value))\n",
      "                e += 1\n",
      "                continue\n",
      "            features[key] = value\n",
      "        if 'sp' in features and features['sp'] == 'verb':\n",
      "            if 'gl' in features:\n",
      "                gloss = features['gl']\n",
      "                if gloss.startswith('to '):\n",
      "                    features['gl'] = gloss[3:]\n",
      "        lex_items[entry] = features\n",
      "        lex_outfile.write('{}\\t{}\\n'.format(entry, features))\n",
      "        \n",
      "    lex_infile.close()\n",
      "    lex_outfile.close()\n",
      "    lex_errfile.close()\n",
      "    msgstr = \"Lexicon {}: there w\".format(lan) + ('ere {} errors'.format(e) if e != 1 else 'as 1 error')\n",
      "    print(msgstr)\n",
      "    return lex_items\n",
      "\n",
      "lex_entries = dict((lan, read_lex(lan)) for lan in sorted(langs))\n",
      "for lan in sorted(lex_entries):\n",
      "    print('Lexicon {} has {:>5} entries'.format(lan, len(lex_entries[lan])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Lexicon arc: there were 0 errors\n",
        "Lexicon hbo: there were 0 errors"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Lexicon syc: there were 0 errors"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Lexicon arc has   707 entries\n",
        "Lexicon hbo has  8529 entries\n",
        "Lexicon syc has  2383 entries\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Gather the lexemes from the etcbc4 database\n",
      "\n",
      "We inspect all word occurrences of the etcbc4 database, inspect their language and lexeme values, and construct sets of lexemes that belong to each of the two languages, ``hbo`` and ``aramaic``."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lex_text = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(lambda: set())))\n",
      "do_value_compare = {'sp', 'ls', 'gn', 'ps', 'nu', 'st'}\n",
      "text_value_set = collections.defaultdict(lambda: set())\n",
      "\n",
      "text_langs = set()\n",
      "for n in NN():\n",
      "    otype = F.otype.v(n)\n",
      "    if otype != 'word': continue\n",
      "    lan = lang_map[F.language.v(n)]\n",
      "    text_langs.add(lan)\n",
      "    lex = F.lex.v(n)\n",
      "    lex_text[lan][lex]['sp'].add(F.sp.v(n))\n",
      "    lex_text[lan][lex]['ls'].add(F.ls.v(n))\n",
      "    lex_text[lan][lex]['gn'].add(F.gn.v(n))\n",
      "    lex_text[lan][lex]['nu'].add(F.nu.v(n))\n",
      "    lex_text[lan][lex]['ps'].add(F.ps.v(n))\n",
      "    lex_text[lan][lex]['vc'].add(F.g_lex.v(n))\n",
      "    for p in do_value_compare:\n",
      "        text_value_set[p].add(F.item[p].v(n))        \n",
      "\n",
      "tf = outfile('text_lexemes.txt')\n",
      "for lan in sorted(lex_text):\n",
      "    for lex in sorted(lex_text[lan]):\n",
      "        tf.write('{} \"{}\"\\n'.format(lan, lex))\n",
      "tf.close()\n",
      "for lan in sorted(lex_text):\n",
      "    print('Language {} has {:>5} lexemes in the etcbc4 text'.format(lan, len(lex_text[lan])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Language arc has   707 lexemes in the etcbc4 text\n",
        "Language hbo has  8518 lexemes in the etcbc4 text\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Checks\n",
      "\n",
      "## Intersection between Hebrew and Aramaic\n",
      "\n",
      "Are there entries that are Hebrew and Aramaic?\n",
      "\n",
      "We check \n",
      "* whether the etcbc4 database has marked some lexemes sometimes as Hebrew and sometimes as Aramaic;\n",
      "* whether the lexica for ``hbo`` and ``arc`` share lexeme entries.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "arc_entries = set(lex_entries['arc'])\n",
      "hbo_entries = set(lex_entries['hbo'])\n",
      "\n",
      "arc_text = set(lex_text['arc'])\n",
      "hbo_text = set(lex_text['hbo'])\n",
      "\n",
      "hbo_and_arc_lexemes = arc_text & hbo_text\n",
      "hbo_and_arc_entries = arc_entries & hbo_entries\n",
      "lexemes_entries = hbo_and_arc_lexemes & hbo_and_arc_entries\n",
      "print('The intersection of hbo and arc in the etcbc4 text contains {} lexemes'.format(len(hbo_and_arc_lexemes)))\n",
      "print('The intersection of hbo and arc in the lexicon     contains {} lexemes'.format(len(hbo_and_arc_entries)))\n",
      "print('The hbo-arc intersection of the lexemes and of the entries share {} lexemes'.format(len(lexemes_entries)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The intersection of hbo and arc in the etcbc4 text contains 459 lexemes\n",
        "The intersection of hbo and arc in the lexicon     contains 460 lexemes\n",
        "The hbo-arc intersection of the lexemes and of the entries share 459 lexemes\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, there are lexemes in the etcbc4 text that are both marked as ``hbo`` and as ``arc``. \n",
      "The same holds for entries in the lexica, and both agree.\n",
      "\n",
      "Let us now check whether all lexemes in the text occur in the lexicon and vice versa."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "arc_text_diff = arc_text - arc_entries\n",
      "arc_entries_diff = arc_entries - arc_text\n",
      "\n",
      "hbo_text_diff = hbo_text - hbo_entries\n",
      "hbo_entries_diff = hbo_entries - hbo_text\n",
      "\n",
      "for (myset, mymsg) in (\n",
      "    (arc_text_diff, 'arc: lexemes in text but not in lexicon'),\n",
      "    (arc_entries_diff, 'arc: lexemes in lexicon but not in text'),\n",
      "    (hbo_text_diff, 'hbo: lexemes in text but not in lexicon'),\n",
      "    (hbo_entries_diff, 'hbo: lexemes in lexicon but not in text'),\n",
      "):\n",
      "    print('{}: {}x\\n\"{}\"'.format(mymsg, len(myset), '\", \"'.join(sorted(myset))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "arc: lexemes in text but not in lexicon: 0x\n",
        "\"\"\n",
        "arc: lexemes in lexicon but not in text: 0x\n",
        "\"\"\n",
        "hbo: lexemes in text but not in lexicon: 0x\n",
        "\"\"\n",
        "hbo: lexemes in lexicon but not in text: 11x\n",
        "\"<JR_HHRS/\", \">BVJX/\", \">XCDRPN/\", \">XCTRN/\", \"BL<DJ/\", \"BLTJ/\", \"BN_JMJNJ/\", \"MHRH\", \"MQYT/\", \"MRPH/\", \"TPWYH/\"\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Feature richness\n",
      "\n",
      "Which features do the entries have, and what percentage of the entries has those features?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_count = collections.defaultdict(lambda: collections.Counter())\n",
      "inspect_prop = collections.defaultdict(\n",
      "    lambda: collections.defaultdict(lambda: collections.defaultdict(lambda: collections.Counter()))\n",
      ")\n",
      "lex_value_set = collections.defaultdict(lambda: set())\n",
      "\n",
      "close_inspect = {'sp', 'sm', 'ls', 'gn', 'ps', 'nu', 'st'} # , 'co', 'kb', 'fc'\n",
      "\n",
      "for lan in lex_entries:\n",
      "    entries = lex_entries[lan]\n",
      "    for entry in entries:\n",
      "        features = entries[entry]\n",
      "        for feature in features:\n",
      "            feature_count[lan][feature] += 1\n",
      "        for p in close_inspect:\n",
      "            if p in features:\n",
      "                inspect_prop[lan][p][features[p]][features['sp']] += 1\n",
      "        if lan != 'syc':\n",
      "            for p in do_value_compare:\n",
      "                if p in features:\n",
      "                    lex_value_set[p].add(features[p])\n",
      "\n",
      "for lan in feature_count:\n",
      "    nentries = len(lex_entries[lan])\n",
      "    for feature in feature_count[lan]:\n",
      "        fv = feature_count[lan][feature]\n",
      "        feature_count[lan][feature] = fv * 100 / nentries\n",
      "\n",
      "print(\"Feature occurrences in the lexicon\")\n",
      "for lan in sorted(feature_count):\n",
      "    feature_spec = '\\n'.join('\\t{:<8}: {:>6.2f}%'.format(f, v) for (f,v) in sorted(feature_count[lan].items(), key=lambda x: (-x[1], x[0])))\n",
      "    print(\"{}\\n{}\\t\".format(lan, feature_spec))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Feature occurrences in the lexicon\n",
        "arc\n",
        "\tgl      : 100.00%\n",
        "\tsp      : 100.00%\n",
        "\tvc      : 100.00%\n",
        "\tgn      :  36.35%\n",
        "\tls      :   6.79%\n",
        "\tsm      :   5.80%\n",
        "\tnu      :   2.69%\n",
        "\tps      :   1.41%\n",
        "\trt      :   0.71%\n",
        "\tco      :   0.14%\n",
        "\tkb      :   0.14%\t\n",
        "hbo\n",
        "\tsp      :  99.99%\n",
        "\tvc      :  86.99%\n",
        "\tgl      :  71.11%\n",
        "\tgn      :  44.50%\n",
        "\tsm      :  30.26%\n",
        "\tfc      :  26.96%\n",
        "\tls      :   3.79%\n",
        "\trt      :   2.66%\n",
        "\tnu      :   0.27%\n",
        "\tst      :   0.23%\n",
        "\tps      :   0.16%\n",
        "\tkb      :   0.04%\n",
        "\tvw      :   0.02%\n",
        "\tco      :   0.01%\t\n",
        "syc\n",
        "\t_n      : 100.00%\n",
        "\tsp      :  99.92%\n",
        "\tgl      :  99.66%\n",
        "\tgn      :  52.54%\n",
        "\tde      :  32.56%\n",
        "\tls      :  21.65%\n",
        "\tst      :  17.16%\n",
        "\tnu      :   0.88%\n",
        "\tps      :   0.50%\n",
        "\tf       :   0.08%\n",
        "\tfl      :   0.04%\t\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Detail feature values and occurrences\")\n",
      "for lan in sorted(inspect_prop):\n",
      "    print(\"{}\\n\".format(lan))\n",
      "    for p in sorted(inspect_prop[lan]):\n",
      "        print(\"\\t{}\\n\".format(p))\n",
      "        for value in sorted(inspect_prop[lan][p]):\n",
      "            inspect_spec = '\\n'.join('\\t\\t\\t{:<6}: {:>3}x'.format(f, v) for (f,v) in sorted(\n",
      "                inspect_prop[lan][p][value].items(), key=lambda x: (-x[1], x[0])\n",
      "            ))\n",
      "            print(\"\\t\\t{}\\n{}\\t\".format(value, inspect_spec))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Detail feature values and occurrences\n",
        "arc\n",
        "\n",
        "\tgn\n",
        "\n",
        "\t\tf\n",
        "\t\t\tsubs  :  43x\n",
        "\t\t\tprde  :   2x\n",
        "\t\t\tprps  :   2x\n",
        "\t\t\tadjv  :   1x\t\n",
        "\t\tm\n",
        "\t\t\tsubs  : 171x\n",
        "\t\t\tnmpr  :  29x\n",
        "\t\t\tprps  :   6x\n",
        "\t\t\tprde  :   2x\n",
        "\t\t\tadjv  :   1x\t\n",
        "\tls\n",
        "\n",
        "\t\tcard\n",
        "\t\t\tsubs  :  12x\t\n",
        "\t\tcjad\n",
        "\t\t\tadvb  :   1x\t\n",
        "\t\tfocp\n",
        "\t\t\tadvb  :   1x\n",
        "\t\t\tinrg  :   1x\t\n",
        "\t\tgntl\n",
        "\t\t\tadjv  :  10x\t\n",
        "\t\tmult\n",
        "\t\t\tsubs  :   1x\t\n",
        "\t\tnmcp\n",
        "\t\t\tsubs  :   1x\t\n",
        "\t\tnmdi\n",
        "\t\t\tsubs  :   3x\t\n",
        "\t\tordn\n",
        "\t\t\tadjv  :   4x\t\n",
        "\t\tpadv\n",
        "\t\t\tsubs  :   2x\t\n",
        "\t\tppre\n",
        "\t\t\tsubs  :   9x\t\n",
        "\t\tquot\n",
        "\t\t\tverb  :   2x\t\n",
        "\t\tvbcp\n",
        "\t\t\tverb  :   1x\t\n",
        "\tnu\n",
        "\n",
        "\t\tp\n",
        "\t\t\tprps  :   6x\n",
        "\t\t\tprde  :   5x\t\n",
        "\t\ts\n",
        "\t\t\tprde  :   4x\n",
        "\t\t\tprps  :   4x\t\n",
        "\tps\n",
        "\n",
        "\t\t1\n",
        "\t\t\tprps  :   2x\t\n",
        "\t\t2\n",
        "\t\t\tprps  :   2x\t\n",
        "\t\t3\n",
        "\t\t\tprps  :   6x\t\n",
        "\tsm\n",
        "\n",
        "\t\tgens\n",
        "\t\t\tnmpr  :   1x\t\n",
        "\t\tmens\n",
        "\t\t\tnmpr  :   1x\t\n",
        "\t\tpers\n",
        "\t\t\tnmpr  :  29x\t\n",
        "\t\tppde\n",
        "\t\t\tprps  :   2x\t\n",
        "\t\ttopo\n",
        "\t\t\tnmpr  :   8x\t\n",
        "\tsp\n",
        "\n",
        "\t\tadjv\n",
        "\t\t\tadjv  :  51x\t\n",
        "\t\tadvb\n",
        "\t\t\tadvb  :  19x\t\n",
        "\t\tconj\n",
        "\t\t\tconj  :   3x\t\n",
        "\t\tinrg\n",
        "\t\t\tinrg  :   1x\t\n",
        "\t\tintj\n",
        "\t\t\tintj  :   4x\t\n",
        "\t\tnega\n",
        "\t\t\tnega  :   2x\t\n",
        "\t\tnmpr\n",
        "\t\t\tnmpr  :  39x\t\n",
        "\t\tprde\n",
        "\t\t\tprde  :   9x\t\n",
        "\t\tprep\n",
        "\t\t\tprep  :  10x\t\n",
        "\t\tprin\n",
        "\t\t\tprin  :   2x\t\n",
        "\t\tprps\n",
        "\t\t\tprps  :  10x\t\n",
        "\t\tsubs\n",
        "\t\t\tsubs  : 370x\t\n",
        "\t\tverb\n",
        "\t\t\tverb  : 187x\t\n",
        "hbo\n",
        "\n",
        "\tgn\n",
        "\n",
        "\t\tf\n",
        "\t\t\tsubs  : 253x\n",
        "\t\t\tnmpr  : 108x\n",
        "\t\t\tprps  :   5x\n",
        "\t\t\tprde  :   3x\t\n",
        "\t\tm\n",
        "\t\t\tsubs  : 1883x\n",
        "\t\t\tnmpr  : 1531x\n",
        "\t\t\tprps  :   5x\n",
        "\t\t\tprde  :   1x\t\n",
        "\t\tm,f\n",
        "\t\t\tnmpr  :   6x\t\n",
        "\tls\n",
        "\n",
        "\t\tafad\n",
        "\t\t\tadvb  :   1x\t\n",
        "\t\tcard\n",
        "\t\t\tsubs  :  16x\t\n",
        "\t\tcjad\n",
        "\t\t\tadvb  :   2x\t\n",
        "\t\tfocp\n",
        "\t\t\tadvb  :   4x\n",
        "\t\t\tinrg  :   1x\t\n",
        "\t\tgntl\n",
        "\t\t\tadjv  : 212x\t\n",
        "\t\tmult\n",
        "\t\t\tsubs  :   2x\t\n",
        "\t\tnmcp\n",
        "\t\t\tsubs  :   6x\t\n",
        "\t\tnmdi\n",
        "\t\t\tsubs  :   6x\t\n",
        "\t\tordn\n",
        "\t\t\tadjv  :  10x\t\n",
        "\t\tpadv\n",
        "\t\t\tsubs  :  39x\t\n",
        "\t\tppre\n",
        "\t\t\tsubs  :  21x\t\n",
        "\t\tquot\n",
        "\t\t\tverb  :   2x\t\n",
        "\t\tvbcp\n",
        "\t\t\tverb  :   1x\t\n",
        "\tnu\n",
        "\n",
        "\t\tp\n",
        "\t\t\tprps  :   8x\n",
        "\t\t\tprde  :   2x\t\n",
        "\t\ts\n",
        "\t\t\tprde  :   7x\n",
        "\t\t\tprps  :   6x\t\n",
        "\tps\n",
        "\n",
        "\t\t1\n",
        "\t\t\tprps  :   4x\t\n",
        "\t\t2\n",
        "\t\t\tprps  :   5x\t\n",
        "\t\t3\n",
        "\t\t\tprps  :   5x\t\n",
        "\tsm\n",
        "\n",
        "\t\tgens\n",
        "\t\t\tnmpr  :  52x\t\n",
        "\t\tgens,topo\n",
        "\t\t\tnmpr  :   2x\t\n",
        "\t\tmens\n",
        "\t\t\tnmpr  :   9x\t\n",
        "\t\tpers\n",
        "\t\t\tnmpr  : 1641x\t\n",
        "\t\tpers,gens\n",
        "\t\t\tnmpr  :  13x\t\n",
        "\t\tpers,gens,topo\n",
        "\t\t\tnmpr  :  19x\t\n",
        "\t\tppde\n",
        "\t\t\tprps  :   5x\t\n",
        "\t\ttopo\n",
        "\t\t\tnmpr  : 840x\t\n",
        "\tsp\n",
        "\n",
        "\t\tadjv\n",
        "\t\t\tadjv  : 562x\t\n",
        "\t\tadvb\n",
        "\t\t\tadvb  :  34x\t\n",
        "\t\tart\n",
        "\t\t\tart   :   1x\t\n",
        "\t\tconj\n",
        "\t\t\tconj  :  11x\t\n",
        "\t\tinrg\n",
        "\t\t\tinrg  :  17x\t\n",
        "\t\tintj\n",
        "\t\t\tintj  :  24x\t\n",
        "\t\tnega\n",
        "\t\t\tnega  :   3x\t\n",
        "\t\tnmpr\n",
        "\t\t\tnmpr  : 2582x\t\n",
        "\t\tprde\n",
        "\t\t\tprde  :   9x\t\n",
        "\t\tprep\n",
        "\t\t\tprep  :  15x\t\n",
        "\t\tprin\n",
        "\t\t\tprin  :   3x\t\n",
        "\t\tprps\n",
        "\t\t\tprps  :  14x\t\n",
        "\t\tsubs\n",
        "\t\t\tsubs  : 3701x\t\n",
        "\t\tverb\n",
        "\t\t\tverb  : 1552x\t\n",
        "\tst\n",
        "\n",
        "\t\ta\n",
        "\t\t\tnmpr  :  20x\t\n",
        "syc\n",
        "\n",
        "\tgn\n",
        "\n",
        "\t\tf\n",
        "\t\t\tsubs  : 401x\n",
        "\t\t\tpron  :   7x\n",
        "\t\t\tinrg  :   1x\t\n",
        "\t\tm\n",
        "\t\t\tsubs  : 834x\n",
        "\t\t\tpron  :   8x\n",
        "\t\t\tinrg  :   1x\t\n",
        "\tls\n",
        "\n",
        "\t\tcard\n",
        "\t\t\tsubs  :  34x\t\n",
        "\t\tdemo\n",
        "\t\t\tpron  :   8x\t\n",
        "\t\tgntl\n",
        "\t\t\tadjv  :  27x\t\n",
        "\t\tnmex\n",
        "\t\t\tsubs  :   1x\t\n",
        "\t\tordn\n",
        "\t\t\tadjv  :   6x\t\n",
        "\t\tpadv\n",
        "\t\t\tsubs  :   1x\t\n",
        "\t\tpcon\n",
        "\t\t\tprep  :   4x\t\n",
        "\t\tpers\n",
        "\t\t\tpron  :  11x\t\n",
        "\t\tpinr\n",
        "\t\t\tadvb  :   1x\t\n",
        "\t\tpint\n",
        "\t\t\tadvb  :   2x\n",
        "\t\t\tconj  :   1x\t\n",
        "\t\tppre\n",
        "\t\t\tsubs  :   6x\t\n",
        "\t\tprin\n",
        "\t\t\tpron  :   3x\t\n",
        "\t\tprop\n",
        "\t\t\tsubs  : 409x\t\n",
        "\t\tquot\n",
        "\t\t\tverb  :   1x\t\n",
        "\t\tvbex\n",
        "\t\t\tverb  :   1x\t\n",
        "\tnu\n",
        "\n",
        "\t\tpl\n",
        "\t\t\tpron  :   9x\n",
        "\t\t\tsubs  :   2x\n",
        "\t\t\tinrg  :   1x\t\n",
        "\t\tsg\n",
        "\t\t\tpron  :   9x\t\n",
        "\tps\n",
        "\n",
        "\t\tfirst\n",
        "\t\t\tpron  :   2x\t\n",
        "\t\tsecond\n",
        "\t\t\tpron  :   3x\t\n",
        "\t\tthird\n",
        "\t\t\tpron  :   7x\t\n",
        "\tsp\n",
        "\n",
        "\t\tadjv\n",
        "\t\t\tadjv  : 145x\t\n",
        "\t\tadvb\n",
        "\t\t\tadvb  :  48x\t\n",
        "\t\tconj\n",
        "\t\t\tconj  :  14x\t\n",
        "\t\tinrg\n",
        "\t\t\tinrg  :   4x\t\n",
        "\t\tintj\n",
        "\t\t\tintj  :   8x\t\n",
        "\t\tnega\n",
        "\t\t\tnega  :   1x\t\n",
        "\t\tprep\n",
        "\t\t\tprep  :  21x\t\n",
        "\t\tpron\n",
        "\t\t\tpron  :  22x\t\n",
        "\t\tsubs\n",
        "\t\t\tsubs  : 1548x\t\n",
        "\t\tsubs \n",
        "\t\t\tsubs  :   1x\t\n",
        "\t\tverb\n",
        "\t\t\tverb  : 569x\t\n",
        "\tst\n",
        "\n",
        "\t\tabs\n",
        "\t\t\tsubs  : 409x\t\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Consistency of text features\n",
      "\n",
      "Multiple occurrences in the text point to the same lexeme. \n",
      "Some properties of those occurrences are in fact properties of the lexeme, e.g. the part of speech.\n",
      "\n",
      "The question arises: is the part of speech property assigned to the word occurrences in such a way, that all occurrences of the same lexeme have the same part of speech?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "consistent_props = {'sp', 'ls', 'gn', 'vc'}\n",
      "variable_gender = {'verb', 'adjv'}\n",
      "\n",
      "exceptions = collections.defaultdict(lambda: collections.defaultdict(lambda: set()))\n",
      "exceptions_gn = collections.defaultdict(lambda: collections.Counter())\n",
      "incons = outfile('inconsistent.csv')\n",
      "for lan in sorted(lex_text):\n",
      "    lexemes = lex_text[lan]\n",
      "    for lexeme in sorted(lexemes):\n",
      "        properties = lexemes[lexeme]\n",
      "        for prop in consistent_props:\n",
      "            if prop in properties:\n",
      "                values = properties[prop]\n",
      "                psp = list(properties['sp'])[0]\n",
      "                if len(values) > 1:\n",
      "                    if prop == 'gn':\n",
      "                        if (len(set(properties['sp']) & variable_gender) != 0): continue\n",
      "                        exceptions_gn[lan][psp] += 1\n",
      "                    exceptions[lan][prop].add(lexeme)\n",
      "                    incons.write('\"{}\";\"{}\";\"{}\";\"{}\";{};\"{}\"\\n'.format(lan, prop, lexeme, psp, len(values), '\";\"'.join(values)))\n",
      "incons.close()\n",
      "for lan in sorted(text_langs):\n",
      "    print(\"{}\\n\".format(lan))\n",
      "    for prop in sorted(consistent_props):\n",
      "        extra = ''\n",
      "        if prop == 'gn':\n",
      "            for psp in exceptions_gn[lan]:\n",
      "                extra += '\\n{}{}: {}x'.format(' ' * 8, psp, exceptions_gn[lan][psp])\n",
      "        print(\"{}{:<8}: {:>4} inconsistent lexemes{}\".format(' ' * 4, prop, len(exceptions.get(lan, {}).get(prop, set())), extra))\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "arc\n",
        "\n",
        "    gn      :   10 inconsistent lexemes\n",
        "        subs: 10x\n",
        "    ls      :    0 inconsistent lexemes\n",
        "    sp      :    0 inconsistent lexemes\n",
        "    vc      :  262 inconsistent lexemes\n",
        "hbo\n",
        "\n",
        "    gn      :   61 inconsistent lexemes\n",
        "        subs: 50x\n",
        "        nmpr: 11x\n",
        "    ls      :    0 inconsistent lexemes\n",
        "    sp      :    0 inconsistent lexemes\n",
        "    vc      : 3862 inconsistent lexemes\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Consistency of feature values between lexicon and text\n",
      "\n",
      "Are the values of features used in the text database consistent with the values used in the lexicon?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for p in do_value_compare:\n",
      "    print(p)\n",
      "    text_not_lex = sorted(text_value_set[p] - lex_value_set[p])\n",
      "    lex_not_text = sorted(lex_value_set[p] - text_value_set[p])\n",
      "    print('\\tin text and not in lex: {}: {}'.format(len(text_not_lex), text_not_lex))\n",
      "    print('\\tin lex and not in text: {}: {}'.format(len(lex_not_text), lex_not_text))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "gn\n",
        "\tin text and not in lex: 2: ['NA', 'unknown']\n",
        "\tin lex and not in text: 1: ['m,f']\n",
        "ps\n",
        "\tin text and not in lex: 5: ['NA', 'p1', 'p2', 'p3', 'unknown']\n",
        "\tin lex and not in text: 3: ['1', '2', '3']\n",
        "sp\n",
        "\tin text and not in lex: 0: []\n",
        "\tin lex and not in text: 0: []\n",
        "st\n",
        "\tin text and not in lex: 3: ['NA', 'c', 'e']\n",
        "\tin lex and not in text: 0: []\n",
        "ls\n",
        "\tin text and not in lex: 2: ['none', 'ques']\n",
        "\tin lex and not in text: 0: []\n",
        "nu\n",
        "\tin text and not in lex: 5: ['NA', 'du', 'pl', 'sg', 'unknown']\n",
        "\tin lex and not in text: 2: ['p', 's']\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Generate lexicon data\n",
      "\n",
      "We make sure that we translate lexical feature values into values used for the etcbc4.\n",
      "\n",
      "We need the following information per entry:\n",
      "\n",
      "* **id** a fresh id, to be used in applications, unique over **entryid** and **lan**\n",
      "* **lan** the language of the entry, in ISO 639-3 abbreviation\n",
      "* **entryid** the string used as entry in the lexicon and as value of the ``lex`` feature in the text\n",
      "* **entry** the unpointed transliteration (= **entryid** without disambiguation marks)\n",
      "* **entry_heb** the unpointed hebrew representation, obtained by untransliterating **entry**\n",
      "* **g_entry** the pointed transliteration, without disambiguation marks, obtained from ``vc``\n",
      "* **g_entry_heb** the pointed hebrew representation, obtained by untransliterating **g_entry**\n",
      "* **root** the root, obtained from ``rt``\n",
      "* **gloss** the gloss from ``gl``\n",
      "* **pos** the part of speech, obtained from ``sp``\n",
      "* **nametype** the type of named entity, obtained from ``sm``\n",
      "* **subpos** subtype of part of speech, obtained from ``ls`` (aka *lexical set*)\n",
      "* **gender** the gender, in so far it is lexical, obtained from ``gn``\n",
      "* **person** the person, in so far it is lexical, obtained from ``ps`` with value transformation\n",
      "* **number** the number, in so far it is lexical, obtained from ``nu`` with value transformation\n",
      "* **state** the state, in so far it is lexical, obtained from ``st``"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lex_fields = {\n",
      "    'vc': ('vocalized',\n",
      "           None,\n",
      "    ),\n",
      "    'rt': ('root',\n",
      "           None,\n",
      "    ),\n",
      "    'sp': ('pos',\n",
      "           {\n",
      "                'subs ': 'subs',\n",
      "                'pron' : '',\n",
      "            }\n",
      "    ),\n",
      "    'sm': ('nametype',\n",
      "           None,\n",
      "    ),\n",
      "    'ls': ('subpos',\n",
      "           None,\n",
      "    ),\n",
      "    'gn': ('gender',\n",
      "           None,\n",
      "    ),\n",
      "    'ps': ('person', \n",
      "           {\n",
      "                '1': 'p1',\n",
      "                '2': 'p2',\n",
      "                '3': 'p3',\n",
      "                'first': 'p1',\n",
      "                'second': 'p2',\n",
      "                'third': 'p3',\n",
      "            }\n",
      "\n",
      "    ),\n",
      "    'nu': ('number', \n",
      "           {\n",
      "                's': 'sg',\n",
      "                'd': 'du',\n",
      "                'p': 'pl',\n",
      "            }\n",
      "    ),\n",
      "    'st': ('state',\n",
      "           {\n",
      "                'abs': 'a',\n",
      "            }\n",
      "    ),\n",
      "    'gl': ('gloss',\n",
      "           None,\n",
      "    ),\n",
      "}\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}