{
 "metadata": {
  "name": "",
  "signature": "sha256:01477448fa6ae8c26734377ac5ff3d4e7ee8028a976c52a40f271d3c895ba94c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"left\" src=\"images/laf-fabric-xsmall.png\"/></a>\n",
      "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"left\" src=\"images/VU-ETCBC-xsmall.png\"/></a>\n",
      "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"left\"src=\"images/etcbc4easy-small.png\"/></a>\n",
      "<a href=\"http://tla.mpi.nl\" target=\"_blank\"><img align=\"right\" src=\"images/TLA-xsmall.png\"/></a>\n",
      "<a href=\"http://www.dans.knaw.nl\" target=\"_blank\"><img align=\"right\"src=\"images/DANS-xsmall.png\"/></a>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Reading the ETCBC lexicon files"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Among the ETCBC data are lexicon files for the biblical languages.\n",
      "We want to put that data in LMF files, and put them to use in LAF-Fabric and SHEBANQ.\n",
      "\n",
      "We have lexicon files for the following languages (given in [ISO 639-3 codes and names](http://www-01.sil.org/iso639-3/default.asp))\n",
      "\n",
      "* ``hbo`` *Ancient Hebrew*\n",
      "* ``arc`` *Official Aramaic; Imperial Aramaic*\n",
      "* ``syc`` *Classical Syriac*"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Firing up the engines"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import collections\n",
      "\n",
      "import laf\n",
      "from laf.fabric import LafFabric\n",
      "from etcbc.preprocess import prepare\n",
      "\n",
      "fabric = LafFabric()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s This is LAF-Fabric 4.4.3\n",
        "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
        "Feature doc: http://shebanq-doc.readthedocs.org/en/latest/texts/welcome.html\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "API = fabric.load('etcbc4', '--', 'lexicon', {\n",
      "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
      "    \"features\": ('''\n",
      "        oid otype monads\n",
      "        book chapter verse\n",
      "        g_word g_word_utf8 trailer_utf8 language lex lex_utf8 g_lex g_lex_utf8\n",
      "        sp vs vt\n",
      "    ''','''\n",
      "    '''),\n",
      "    \"prepare\": prepare,\n",
      "}, verbose='DETAIL')\n",
      "exec(fabric.localnames.format(var='fabric'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s LOADING API: please wait ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING m: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s INFO: USING DATA COMPILED AT: 2014-07-23T09-31-37\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING a: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.01s DETAIL: keep main: G.node_anchor_min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.01s DETAIL: keep main: G.node_anchor_max\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.01s DETAIL: keep main: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.01s DETAIL: keep main: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.01s DETAIL: keep main: G.edges_from\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.01s DETAIL: keep main: G.edges_to\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.01s DETAIL: keep main: F.etcbc4_db_monads [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.01s DETAIL: keep main: F.etcbc4_db_oid [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.01s DETAIL: keep main: F.etcbc4_db_otype [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.01s DETAIL: keep main: F.etcbc4_ft_g_lex [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.01s DETAIL: keep main: F.etcbc4_ft_g_lex_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.02s DETAIL: keep main: F.etcbc4_ft_g_word [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.02s DETAIL: keep main: F.etcbc4_ft_g_word_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.02s DETAIL: keep main: F.etcbc4_ft_language [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.02s DETAIL: keep main: F.etcbc4_ft_lex [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.02s DETAIL: keep main: F.etcbc4_ft_lex_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.02s DETAIL: keep main: F.etcbc4_ft_sp [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.02s DETAIL: keep main: F.etcbc4_ft_trailer_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.02s DETAIL: keep main: F.etcbc4_ft_vs [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.02s DETAIL: keep main: F.etcbc4_ft_vt [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.02s DETAIL: keep main: F.etcbc4_sft_book [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.02s DETAIL: keep main: F.etcbc4_sft_chapter [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.02s DETAIL: keep main: F.etcbc4_sft_verse [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.02s DETAIL: prep prep: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.09s DETAIL: prep prep: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.59s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX -- FOR TASK lexicon AT 2014-10-08T08-35-53\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Read the lexica\n",
      "\n",
      "We read the lexica for Hebrew and Aramaic.\n",
      "In the sequel we will consistently and exclusively refer to these languages by their ISO 639-3 abbreviations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "langs = {'hbo', 'arc', 'syc'}\n",
      "lex_base = dict((lan, '{}/{}/{}.{}'.format(API['data_dir'], 'lexicon', lan, 'etcbc')) for lan in langs)\n",
      "lex_base\n",
      "lang_map = {\n",
      "    'Hebrew': 'hbo',\n",
      "    'Aramaic': 'arc',\n",
      "    'Syriac': 'syc',\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_lex(lan):\n",
      "    lex_infile = open(lex_base[lan], encoding='utf-8')\n",
      "    lex_outfile = outfile('{}.txt'.format(lan))\n",
      "    lex_errfile = outfile('{}.err.txt'.format(lan))\n",
      "\n",
      "    lex_items = {}\n",
      "    ln = 0\n",
      "    e = 0\n",
      "    for line in lex_infile:\n",
      "        ln += 1\n",
      "        line = line.rstrip()\n",
      "        line = line.split('#')[0]\n",
      "        if line == '': continue\n",
      "        (entry, featurestr) = line.split(sep=None, maxsplit=1)\n",
      "        entry = entry.strip('\"')\n",
      "        if entry in lex_items:\n",
      "            lex_errfile.write('duplicate lexical entry {} in line {}.\\n'.format(entry, ln))\n",
      "            e += 1\n",
      "            continue\n",
      "        featurestr = featurestr.strip(':')\n",
      "        featurestr = featurestr.replace('\\\\:', chr(255))\n",
      "        featurelst = featurestr.split(':')\n",
      "        features = {}\n",
      "        for feature in featurelst:\n",
      "            comps = feature.split('=', maxsplit=1)\n",
      "            if len(comps) == 1:\n",
      "                if feature.strip().isnumeric():\n",
      "                    comps = ('_n', feature.strip())\n",
      "                else:\n",
      "                    lex_errfile.write('feature without value for lexical entry {} in line {}: {}\\n'.format(entry, ln, feature))\n",
      "                    e += 1\n",
      "                    continue\n",
      "            (key, value) = comps\n",
      "            value = value.replace(chr(255), ':')\n",
      "            if key in features:\n",
      "                lex_errfile.write('duplicate feature for lexical entry {} in line {}: {}={}\\n'.format(entry, ln, key, value))\n",
      "                e += 1\n",
      "                continue\n",
      "            features[key] = value\n",
      "        if 'sp' in features and features['sp'] == 'verb':\n",
      "            if 'gl' in features:\n",
      "                gloss = features['gl']\n",
      "                if gloss.startswith('to '):\n",
      "                    features['gl'] = gloss[3:]\n",
      "        lex_items[entry] = features\n",
      "        lex_outfile.write('{}\\t{}\\n'.format(entry, features))\n",
      "        \n",
      "    lex_infile.close()\n",
      "    lex_outfile.close()\n",
      "    lex_errfile.close()\n",
      "    msgstr = \"Lexicon {}: there w\".format(lan) + ('ere {} errors'.format(e) if e != 1 else 'as 1 error')\n",
      "    print(msgstr)\n",
      "    return lex_items"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lex_entries = dict((lan, read_lex(lan)) for lan in sorted(langs))\n",
      "for lan in sorted(lex_entries):\n",
      "    print('Lexicon {} has {:>5} entries'.format(lan, len(lex_entries[lan])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Lexicon arc: there were 0 errors\n",
        "Lexicon hbo: there were 0 errors"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Lexicon syc: there were 0 errors"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Lexicon arc has   707 entries\n",
        "Lexicon hbo has  8520 entries\n",
        "Lexicon syc has  2383 entries\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Gather the lexemes from the etcbc4 database\n",
      "\n",
      "We inspect all word occurrences of the etcbc4 database, inspect their language and lexeme values, and construct sets of lexemes that belong to each of the two languages, ``hbo`` and ``aramaic``."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lex_text = collections.defaultdict(lambda: set())\n",
      "for n in NN():\n",
      "    otype = F.otype.v(n)\n",
      "    if otype != 'word': continue\n",
      "    lan = lang_map[F.language.v(n)]\n",
      "    lex = F.lex.v(n)\n",
      "    lex_text[lan].add(lex)\n",
      "for lan in sorted(lex_text):\n",
      "    print('Language {} has {:>5} lexemes in the etcbc4 text'.format(lan, len(lex_text[lan])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Language arc has   707 lexemes in the etcbc4 text\n",
        "Language hbo has  8518 lexemes in the etcbc4 text\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Checks\n",
      "\n",
      "## Intersection between Hebrew and Aramaic\n",
      "\n",
      "Are there entries that are Hebrew and Aramaic?\n",
      "\n",
      "We check \n",
      "* whether the etcbc4 database has marked some lexemes sometimes as Hebrew and sometimes as Aramaic;\n",
      "* whether the lexica for ``hbo`` and ``arc`` share lexeme entries.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "arc_entries = set(lex_entries['arc'])\n",
      "hbo_entries = set(lex_entries['hbo'])\n",
      "\n",
      "arc_text = lex_text['arc']\n",
      "hbo_text = lex_text['hbo']\n",
      "\n",
      "hbo_and_arc_lexemes = arc_text & hbo_text\n",
      "hbo_and_arc_entries = arc_entries & hbo_entries\n",
      "lexemes_entries = hbo_and_arc_lexemes & hbo_and_arc_entries\n",
      "print('The intersection of hbo and arc in the etcbc4 text contains {} lexemes'.format(len(hbo_and_arc_lexemes)))\n",
      "print('The intersection of hbo and arc in the lexicon     contains {} lexemes'.format(len(hbo_and_arc_entries)))\n",
      "print('The hbo-arc intersection of the lexemes and of the entries share {} lexemes'.format(len(lexemes_entries)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The intersection of hbo and arc in the etcbc4 text contains 459 lexemes\n",
        "The intersection of hbo and arc in the lexicon     contains 459 lexemes\n",
        "The hbo-arc intersection of the lexemes and of the entries share 459 lexemes\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, there are lexemes in the etcbc4 text that are both marked as ``hbo`` and as ``arc``. \n",
      "The same holds for entries in the lexica, and both agree.\n",
      "\n",
      "Let us now check whether all lexemes in the text occur in the lexicon and vice versa."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "arc_text_diff = arc_text - arc_entries\n",
      "arc_entries_diff = arc_entries - arc_text\n",
      "\n",
      "hbo_text_diff = hbo_text - hbo_entries\n",
      "hbo_entries_diff = hbo_entries - hbo_text\n",
      "\n",
      "for (myset, mymsg) in (\n",
      "    (arc_text_diff, 'arc: lexemes in text but not in lexicon'),\n",
      "    (arc_entries_diff, 'arc: lexemes in lexicon but not in text'),\n",
      "    (hbo_text_diff, 'hbo: lexemes in text but not in lexicon'),\n",
      "    (hbo_entries_diff, 'hbo: lexemes in lexicon but not in text'),\n",
      "):\n",
      "    print('{}: {}x\\n\"{}\"'.format(mymsg, len(myset), '\", \"'.join(sorted(myset))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "arc: lexemes in text but not in lexicon: 0x\n",
        "\"\"\n",
        "arc: lexemes in lexicon but not in text: 0x\n",
        "\"\"\n",
        "hbo: lexemes in text but not in lexicon: 15x\n",
        "\"<ZR==/\", \">BJ\", \">BVXJM/\", \">CR/\", \">HBJM=/\", \">LMNWT=/\", \"H>ZL/\", \"HLZ\", \"HLZH\", \"HLZW\", \"JPHPJ/\", \"KRTWT/\", \"M<DNJM/\", \"MWSR==/\", \"XRBH==/\"\n",
        "hbo: lexemes in lexicon but not in text: 17x\n",
        "\"<ZWR/\", \">BVJX/\", \">CWR=/\", \">CWR==/\", \">HB/\", \">J==\", \">ZL/\", \"JSD/\", \"JWNQT/\", \"KRWTH/\", \"LZ\", \"LZH\", \"LZW\", \"M<DN/\", \"MWSRH=/\", \"PJH/\", \"RXB======/\"\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}