{
 "metadata": {
  "name": "",
  "signature": "sha256:ab5f1d16b2cd5bbb79bf248cc0e95673216c75de6663ea6cd6b217d41cd9ef82"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"left\" src=\"images/laf-fabric-xsmall.png\"/></a>\n",
      "<a href=\"http://emdros.org\" target=\"_blank\"><img align=\"left\" src=\"files/images/Emdros-xsmall.png\"/></a>\n",
      "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"left\"src=\"images/etcbc4easy-small.png\"/></a>\n",
      "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"right\" src=\"images/VU-ETCBC-xsmall.png\"/></a>\n",
      "<a href=\"http://tla.mpi.nl\" target=\"_blank\"><img align=\"right\" src=\"images/TLA-xsmall.png\"/></a>\n",
      "<a href=\"http://www.dans.knaw.nl\" target=\"_blank\"><img align=\"right\"src=\"images/DANS-xsmall.png\"/></a>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "LAF2SHEBANQ"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook constructs a relational database, *passage*, meant to support browsing of texts and highlighting of words.\n",
      "It contains the texts themselves, verse by verse, and it contains book and chapter information.\n",
      "The *passage* database also contains a lexicon, which is linked to the word occurrences.\n",
      "\n",
      "See the MySQL create statements below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import collections\n",
      "\n",
      "from laf.fabric import LafFabric\n",
      "from etcbc.lib import Transcription\n",
      "\n",
      "fabric = LafFabric()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s This is LAF-Fabric 4.4.6\n",
        "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
        "Feature doc: http://shebanq-doc.readthedocs.org/en/latest/texts/welcome.html\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "source = 'etcbc4'\n",
      "API = fabric.load('etcbc4', 'lexicon', 'shebanq', {\n",
      "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
      "    \"features\": ('''\n",
      "        oid otype monads minmonad maxmonad\n",
      "        book chapter verse\n",
      "        g_cons g_cons_utf8 g_word g_word_utf8 trailer_utf8\n",
      "        language lex sp ls\n",
      "        vt vs gn nu ps st\n",
      "        g_entry g_entry_heb gloss\n",
      "        function typ rela txt det\n",
      "        number\n",
      "\n",
      "    ''',''),\n",
      "    \"primary\": False,\n",
      "}, verbose='DETAIL')\n",
      "exec(fabric.localnames.format(var='fabric'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s LOADING API: please wait ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING m: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s INFO: USING DATA COMPILED AT: 2014-07-23T09-31-37\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING a: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.01s INFO: USING DATA COMPILED AT: 2014-10-14T11-06-47\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.01s DETAIL: load main: G.node_anchor_min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.08s DETAIL: load main: G.node_anchor_max\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.15s DETAIL: load main: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.21s DETAIL: load main: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.63s DETAIL: load main: G.edges_from\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.69s DETAIL: load main: G.edges_to\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.77s DETAIL: load main: F.etcbc4_db_maxmonad [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.44s DETAIL: load main: F.etcbc4_db_minmonad [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.26s DETAIL: load main: F.etcbc4_db_monads [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.19s DETAIL: load main: F.etcbc4_db_oid [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.00s DETAIL: load main: F.etcbc4_db_otype [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.73s DETAIL: load main: F.etcbc4_ft_det [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.02s DETAIL: load main: F.etcbc4_ft_function [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.17s DETAIL: load main: F.etcbc4_ft_g_cons [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.41s DETAIL: load main: F.etcbc4_ft_g_cons_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.80s DETAIL: load main: F.etcbc4_ft_g_word [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  6.12s DETAIL: load main: F.etcbc4_ft_g_word_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  6.45s DETAIL: load main: F.etcbc4_ft_gn [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  6.64s DETAIL: load main: F.etcbc4_ft_language [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  6.95s DETAIL: load main: F.etcbc4_ft_lex [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  7.20s DETAIL: load main: F.etcbc4_ft_ls [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  7.40s DETAIL: load main: F.etcbc4_ft_nu [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  7.60s DETAIL: load main: F.etcbc4_ft_number [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  8.08s DETAIL: load main: F.etcbc4_ft_ps [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  8.30s DETAIL: load main: F.etcbc4_ft_rela [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  8.74s DETAIL: load main: F.etcbc4_ft_sp [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  8.97s DETAIL: load main: F.etcbc4_ft_st [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.18s DETAIL: load main: F.etcbc4_ft_trailer_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.36s DETAIL: load main: F.etcbc4_ft_txt [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.40s DETAIL: load main: F.etcbc4_ft_typ [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.82s DETAIL: load main: F.etcbc4_ft_vs [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load main: F.etcbc4_ft_vt [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load main: F.etcbc4_lex_g_entry [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load main: F.etcbc4_lex_g_entry_heb [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load main: F.etcbc4_lex_gloss [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load main: F.etcbc4_sft_book [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load main: F.etcbc4_sft_chapter [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load main: F.etcbc4_sft_verse [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_db_maxmonad [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_db_minmonad [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_db_monads [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_db_oid [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_db_otype [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_ft_det [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_ft_function [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_ft_g_cons [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_ft_g_cons_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_ft_g_word [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_ft_g_word_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_ft_gn [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_ft_language [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_ft_lex [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_ft_ls [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_ft_nu [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_ft_number [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_ft_ps [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_ft_rela [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_ft_sp [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_ft_st [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_ft_trailer_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_ft_txt [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_ft_typ [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_ft_vs [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_ft_vt [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    10s DETAIL: load annox: F.etcbc4_lex_g_entry [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    11s DETAIL: load annox: F.etcbc4_lex_g_entry_heb [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    11s DETAIL: load annox: F.etcbc4_lex_gloss [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    11s DETAIL: load annox: F.etcbc4_sft_book [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    11s DETAIL: load annox: F.etcbc4_sft_chapter [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    11s DETAIL: load annox: F.etcbc4_sft_verse [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    11s LOGFILE=/Users/dirk/Dropbox/laf-fabric-output/etcbc4/shebanq/__log__shebanq.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    11s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX lexicon FOR TASK shebanq AT 2014-11-03T09-33-11\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Data model\n",
      "\n",
      "The data model of the browsing database as as follows:\n",
      "\n",
      "There are tables ``book``, ``chapter``, ``verse``, ``word_verse``, ``lexicon``.\n",
      "\n",
      "The tables ``book``, ``chapter`` and ``verse`` contain fields ``first_m``, ``last_m``, \n",
      "denoting the first and last monad number of that book, chapter or verse.\n",
      "\n",
      "A ``book``-record contains an identifier and the name of the book.\n",
      "\n",
      "A ``chapter``-record contains an identifier, the number of the chapter, and a foreign key to the record in the ``book`` table to which the chapter belongs.\n",
      "\n",
      "A ``verse``-record contains an identifier, the number of the verse, and a foreign key to the record in the ``chapter`` table to which the verse belongs. More over, it contains the text of the whole verse in two formats:\n",
      "\n",
      "In field ``text``: the plain unicode text string of the complete verse.\n",
      "\n",
      "In field ``xml``: a sequence of ``<w>`` elements, one for each word in the verse, containing the plain unicode text string of that word as element content.\n",
      "The monad number of that word is stored in an attribute value. \n",
      "The monad number is a globally unique sequence number of a word occurrence in the Hebrew Bible, going from 1 to precisely 426,555.\n",
      "There is also a lexical identifier stored in an attribute value.\n",
      "The lexical identifier points to the lexical entry that corresponds with the word.\n",
      "\n",
      "    <w m=\"2\" l=\"3\">\u05e8\u05b5\u05d0\u05e9\u05b4\u05c1\u0596\u05d9\u05ea </w>\n",
      "\n",
      "As you see, the material between a word and the next word is appended to the first word. So, when you concatenate words, whitespace or other separators are needed.\n",
      "\n",
      "A ``word_verse``-record links a word to a verse. \n",
      "The monad number is in field ``anchor``, which is an integer, \n",
      "and the verse is specified in the field ``verse_id`` as foreign key.\n",
      "The field ``lexicon_id`` is a foreign key into the ``lexicon`` table.\n",
      "\n",
      "There is also a ``word`` table, meant to store all the information to generate a rich representation of the hebrew text,\n",
      "its syntactic structure, and some linguistic properties.\n",
      "See that notebook for a description and an example of the rich hebrew text representation.\n",
      "\n",
      "The rich data is added per word, but the data has a dependency on the verses the words are contained in.\n",
      "In general, information about sentences, clauses and phrases will be displayed on the first words of those objects,\n",
      "but if the object started in a previous verse, this information is repeated on the first word of that object in the\n",
      "current verse.\n",
      "This insures that the display of a verse is always self-contained.\n",
      "\n",
      "The ``word`` table has no field ``id``, its primary key is the field called ``word_number``. \n",
      "This fields contains the same monad number as is used in the field ``anchor`` of the table ``word_verse``.\n",
      "\n",
      "## Lexicon\n",
      "\n",
      "A ``lexicon`` record contains the various lexical fields, such as identifiers, entry representations,\n",
      "additional lexical properties, and a gloss.\n",
      "\n",
      "We make sure that we translate lexical feature values into values used for the etcbc4.\n",
      "We need the following information per entry:\n",
      "\n",
      "* **id** a fresh id, to be used in applications, unique over **entryid** and **lan**\n",
      "* **lan** the language of the entry, in ISO 639-3 abbreviation\n",
      "* **entryid** the string used as entry in the lexicon and as value of the ``lex`` feature in the text\n",
      "* **entry** the unpointed transliteration (= **entryid** without disambiguation marks)\n",
      "* **entry_heb** the unpointed hebrew representation, obtained by untransliterating **entry**\n",
      "* **g_entry** the pointed transliteration, without disambiguation marks, obtained from ``vc``\n",
      "* **g_entry_heb** the pointed hebrew representation, obtained by untransliterating **g_entry**\n",
      "* **root** the root, obtained from ``rt``\n",
      "* **pos** the part of speech, obtained from ``sp``\n",
      "* **nametype** the type of named entity, obtained from ``sm``\n",
      "* **subpos** subtype of part of speech, obtained from ``ls`` (aka *lexical set*)\n",
      "* **gloss** the gloss from ``gl``"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Field transformation\n",
      "\n",
      "The lexical fields require a bit of attention.\n",
      "The specification in ``lex_fields`` below specifies the lexicon fields in the intended order.\n",
      "It contains instructions how to construct the field values from the lexical information obtained from the lexicon files.\n",
      "\n",
      "    (source, method, name, transformation table, data type, data size, data options, params)\n",
      "\n",
      "## source \n",
      "May contain one of the following:\n",
      "\n",
      "* the name of a lexical feature as shown in the lexicon files, such as ``sp``, ``vc``.\n",
      "* None. \n",
      "  In this case, **method** is a code that triggers special actions, such as getting an id or something that is available to the   program that fills the lexicon table\n",
      "* the name of an other field as shown in the **name** part of the specification. \n",
      "  In this case, **method** is a function, defined else where, that takes the value of that other field as argument. \n",
      "  The function is typically a transliteration, or a stripping action.\n",
      "\n",
      "## method\n",
      "May contain one of the following:\n",
      "\n",
      "* a code (string), indicating:\n",
      "    * ``lex``: take the value of a feature (indicated in **source**) for this entry from the lexicon file\n",
      "    * ``entry``: take the value of the entry itself as found in the lexicon file\n",
      "    * ``id``: take the id for this entry as generated by the program\n",
      "    * ``lan``: take the language of this entry\n",
      "* a function taking one argument\n",
      "    * *strip_id*: strip the non-lexeme characters at the end of the entry (the ``/ [ =`` characters)\n",
      "    * *to_heb*: transform the transliteration into real unicode Hebrew\n",
      "    * feature lookup functions such as ``F.lex.v``\n",
      "\n",
      "## name\n",
      "The name of the field in the to be constructed table ``lexicon`` in the database ``passage``.\n",
      "\n",
      "## data type\n",
      "The sql data type, such as ``int`` or ``varchar``, without the size and options.\n",
      "\n",
      "## data size\n",
      "The sql data size, which shows up between ``()`` after the data type\n",
      "\n",
      "## data options\n",
      "Any remaining type specification, such as `` character set utf8``.\n",
      "\n",
      "## params\n",
      "Params consists currently of 1 boolean, indicating whether the field is defined on all words of the object, or only on its first word."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def strip_id(entryid):\n",
      "    return entryid.rstrip('/[=')\n",
      "\n",
      "def to_heb(translit):\n",
      "    return Transcription.to_hebrew(Transcription.suffix_and_finales(translit)[0])\n",
      "\n",
      "def heb(n):\n",
      "    return F.g_word_utf8.v(n) + F.trailer_utf8.v(n)\n",
      "\n",
      "def lang(n):\n",
      "    return 'hbo' if F.language.v(n) == 'Hebrew' else 'arc'\n",
      "\n",
      "def df(f):\n",
      "    def g(n): \n",
      "        val = f(n)\n",
      "        if val == None or val == \"None\" or val == \"none\" or val == \"NA\" or val == \"N/A\":\n",
      "            return '-'\n",
      "        return val\n",
      "    return g\n",
      "\n",
      "lex_fields = (\n",
      "    (None, 'id', 'id', None, 'int', 4, ' primary key'),\n",
      "    (None, 'lan', 'lan', None, 'char', 4, ''),\n",
      "    (None, 'entry', 'entryid', None, 'varchar', 32, ''),\n",
      "    ('entryid', strip_id, 'entry', None, 'varchar', 32, ''),\n",
      "    ('entry', to_heb, 'entry_heb', None, 'varchar', 32, ' character set utf8'),\n",
      "    ('vc', 'lex', 'g_entry', None, 'varchar', 32, ''),\n",
      "    ('g_entry', to_heb, 'g_entry_heb', None, 'varchar', 32, ' character set utf8'),\n",
      "    ('rt', 'lex', 'root', None, 'varchar', 32, ''),\n",
      "    ('sp', 'lex', 'pos', None, 'varchar', 32, ''),\n",
      "    ('sm', 'lex', 'nametype', None, 'varchar', 32, ''),\n",
      "    ('ls', 'lex', 'subpos', None, 'varchar', 32, ''),\n",
      "    ('gl', 'lex', 'gloss', None, 'varchar', 128, ' character set utf8'),\n",
      ")\n",
      "word_fields = (\n",
      "    (F.monads.v, 'number', 'word', 'int', 4, ' primary key', False),\n",
      "    (heb, 'heb', 'word', 'varchar', 32, '', False),\n",
      "    (F.g_entry_heb.v, 'vlex', 'word', 'varchar', 32, '', False),\n",
      "    (F.g_word.v, 'tran', 'word', 'varchar', 32, '', False),\n",
      "    (F.lex.v, 'lex', 'word', 'varchar', 32, '', False),\n",
      "    (F.gloss.v, 'gloss', 'word', 'varchar', 128, '', False),\n",
      "    (lang, 'lang', 'word', 'varchar', 32, '', False),\n",
      "    (df(F.sp.v), 'pos', 'word', 'varchar', 32, '', False),\n",
      "    (df(F.ls.v), 'subpos', 'word', 'varchar', 32, '', False),\n",
      "    (df(F.vt.v), 'tense', 'word', 'varchar', 32, '', False),\n",
      "    (df(F.vs.v), 'stem', 'word', 'varchar', 32, '', False),\n",
      "    (df(F.gn.v), 'gender', 'word', 'varchar', 32, '', False),\n",
      "    (df(F.nu.v), 'gnumber', 'word', 'varchar', 32, '', False),\n",
      "    (df(F.ps.v), 'person', 'word', 'varchar', 32, '', False),\n",
      "    (df(F.st.v), 'state', 'word', 'varchar', 32, '', False),\n",
      "    (None, 'border', 'subphrase', 'varchar', 32, '', False),\n",
      "    ('id', 'number', 'subphrase', 'varchar', 128, '', False),\n",
      "    (F.rela.v, 'rela', 'subphrase', 'varchar', 32, '', True),\n",
      "    (None, 'border', 'phrase', 'varchar', 32, '', False),\n",
      "    (F.number.v, 'number', 'phrase', 'int', 4, '', False),\n",
      "    (df(F.function.v), 'function', 'phrase', 'varchar', 32, '', True),\n",
      "    (df(F.typ.v), 'typ', 'phrase', 'varchar', 32, '', True),\n",
      "    (df(F.det.v), 'det', 'phrase', 'varchar', 32, '', True),\n",
      "    (None, 'border', 'clause', 'varchar', 32, '', False),\n",
      "    (F.number.v, 'number', 'clause', 'int', 4, '', False),\n",
      "    (df(F.typ.v), 'typ', 'clause', 'varchar', 32, '', True),\n",
      "    (df(F.txt.v), 'txt', 'clause', 'varchar', 32, '', True),\n",
      "    (None, 'border', 'sentence', 'varchar', 32, '', False),\n",
      "    (F.number.v, 'number', 'sentence', 'int', 4, '', False),\n",
      ")\n",
      "first_only = dict(('{}_{}'.format(f[2], f[1]), f[6]) for f in word_fields)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Sanity\n",
      "The texts and xml representations of verses are stored in ``varchar`` fields.\n",
      "We have to make sure that the values fit within the declared sizes of these fields.\n",
      "The code measures the maximum lengths of these fields, and it turns out that the text is maximally 434 chars and the xml 2186 chars."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "field_limits = {\n",
      "    'book': {\n",
      "        'name': 64,\n",
      "    },\n",
      "    'verse': {\n",
      "        'text': 1024,\n",
      "        'xml': 4096,\n",
      "    },\n",
      "    'lexicon': {},\n",
      "}\n",
      "for f in lex_fields:\n",
      "    if f[4].endswith('char'):\n",
      "        field_limits['lexicon'][f[2]] = f[5]\n",
      "\n",
      "config = {\n",
      "    'db': 'passage',\n",
      "}\n",
      "for tb in field_limits:\n",
      "    for fl in field_limits[tb]: config['{}_{}'.format(tb, fl)] = field_limits[tb][fl]\n",
      "\n",
      "text_create_sql = '''\n",
      "drop database if exists {db};\n",
      "\n",
      "create database {db} character set utf8;\n",
      "\n",
      "use {db};\n",
      "\n",
      "create table book(\n",
      "    id      int(4) primary key,\n",
      "    first_m int(4),\n",
      "    last_m int(4),\n",
      "    name varchar({book_name})\n",
      ");\n",
      "\n",
      "create table chapter(\n",
      "    id int(4) primary key,\n",
      "    first_m int(4),\n",
      "    last_m int(4),\n",
      "    book_id int(4),\n",
      "    chapter_num int(4),\n",
      "    foreign key (book_id) references book(id)\n",
      ");\n",
      "\n",
      "create table verse(\n",
      "    id int(4) primary key,\n",
      "    first_m int(4),\n",
      "    last_m int(4),\n",
      "    chapter_id int(4),\n",
      "    verse_num int(4),\n",
      "    text varchar({verse_text}) character set utf8,\n",
      "    xml varchar({verse_xml}) character set utf8,\n",
      "    foreign key (chapter_id) references chapter(id)\n",
      ");\n",
      "\n",
      "create table word(\n",
      "    {{wordfields}}\n",
      ");\n",
      "\n",
      "create table lexicon(\n",
      "    {{lexfields}}\n",
      ");\n",
      "\n",
      "create table word_verse(\n",
      "    anchor int(4) unique,\n",
      "    verse_id int(4),\n",
      "    lexicon_id int(4),\n",
      "    foreign key (anchor) references word(word_number),\n",
      "    foreign key (verse_id) references verse(id),\n",
      "    foreign key (lexicon_id) references lexicon(id)\n",
      ");\n",
      "\n",
      "'''.format(**config).format(\n",
      "        lexfields = ',\\n    '.join('{} {}({}){}'.format(\n",
      "            f[2], f[4], f[5], f[6],\n",
      "        ) for f in lex_fields),\n",
      "        wordfields = ', \\n    '.join('{}_{} {}({}){}'.format(\n",
      "            f[2], f[1], f[3], f[4], f[5],\n",
      "    ) for f in word_fields),\n",
      ")\n",
      "print(text_create_sql)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "drop database if exists passage;\n",
        "\n",
        "create database passage character set utf8;\n",
        "\n",
        "use passage;\n",
        "\n",
        "create table book(\n",
        "    id      int(4) primary key,\n",
        "    first_m int(4),\n",
        "    last_m int(4),\n",
        "    name varchar(64)\n",
        ");\n",
        "\n",
        "create table chapter(\n",
        "    id int(4) primary key,\n",
        "    first_m int(4),\n",
        "    last_m int(4),\n",
        "    book_id int(4),\n",
        "    chapter_num int(4),\n",
        "    foreign key (book_id) references book(id)\n",
        ");\n",
        "\n",
        "create table verse(\n",
        "    id int(4) primary key,\n",
        "    first_m int(4),\n",
        "    last_m int(4),\n",
        "    chapter_id int(4),\n",
        "    verse_num int(4),\n",
        "    text varchar(1024) character set utf8,\n",
        "    xml varchar(4096) character set utf8,\n",
        "    foreign key (chapter_id) references chapter(id)\n",
        ");\n",
        "\n",
        "create table word(\n",
        "    word_number int(4) primary key, \n",
        "    word_heb varchar(32), \n",
        "    word_vlex varchar(32), \n",
        "    word_tran varchar(32), \n",
        "    word_lex varchar(32), \n",
        "    word_gloss varchar(128), \n",
        "    word_lang varchar(32), \n",
        "    word_pos varchar(32), \n",
        "    word_subpos varchar(32), \n",
        "    word_tense varchar(32), \n",
        "    word_stem varchar(32), \n",
        "    word_gender varchar(32), \n",
        "    word_gnumber varchar(32), \n",
        "    word_person varchar(32), \n",
        "    word_state varchar(32), \n",
        "    subphrase_border varchar(32), \n",
        "    subphrase_number varchar(128), \n",
        "    subphrase_rela varchar(32), \n",
        "    phrase_border varchar(32), \n",
        "    phrase_number int(4), \n",
        "    phrase_function varchar(32), \n",
        "    phrase_typ varchar(32), \n",
        "    phrase_det varchar(32), \n",
        "    clause_border varchar(32), \n",
        "    clause_number int(4), \n",
        "    clause_typ varchar(32), \n",
        "    clause_txt varchar(32), \n",
        "    sentence_border varchar(32), \n",
        "    sentence_number int(4)\n",
        ");\n",
        "\n",
        "create table lexicon(\n",
        "    id int(4) primary key,\n",
        "    lan char(4),\n",
        "    entryid varchar(32),\n",
        "    entry varchar(32),\n",
        "    entry_heb varchar(32) character set utf8,\n",
        "    g_entry varchar(32),\n",
        "    g_entry_heb varchar(32) character set utf8,\n",
        "    root varchar(32),\n",
        "    pos varchar(32),\n",
        "    nametype varchar(32),\n",
        "    subpos varchar(32),\n",
        "    gloss varchar(128) character set utf8\n",
        ");\n",
        "\n",
        "create table word_verse(\n",
        "    anchor int(4) unique,\n",
        "    verse_id int(4),\n",
        "    lexicon_id int(4),\n",
        "    foreign key (anchor) references word(word_number),\n",
        "    foreign key (verse_id) references verse(id),\n",
        "    foreign key (lexicon_id) references lexicon(id)\n",
        ");\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Lexicon file reading"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "langs = {'hbo', 'arc'}\n",
      "lex_base = dict((lan, '{}/{}/{}.{}'.format(API['data_dir'], 'lexicon', lan, source)) for lan in langs)\n",
      "lang_map = {\n",
      "    'Hebrew': 'hbo',\n",
      "    'Aramaic': 'arc',\n",
      "}\n",
      "\n",
      "def read_lex(lan):\n",
      "    lex_infile = open(lex_base[lan], encoding='utf-8')\n",
      "\n",
      "    lex_items = {}\n",
      "    ln = 0\n",
      "    e = 0\n",
      "    for line in lex_infile:\n",
      "        ln += 1\n",
      "        line = line.split('#')[0]\n",
      "        line = line.rstrip()\n",
      "        if line == '': continue\n",
      "        (entry, featurestr) = line.split(sep=None, maxsplit=1)\n",
      "        entry = entry.strip('\"')\n",
      "        if entry in lex_items:\n",
      "            sys.stderr.write('duplicate lexical entry {} in line {}.\\n'.format(entry, ln))\n",
      "            e += 1\n",
      "            continue\n",
      "        if featurestr.startswith(':') and featurestr.endswith(':'):\n",
      "            featurestr = featurestr.strip(':')\n",
      "        featurestr = featurestr.replace('\\\\:', chr(254))\n",
      "        featurelst = featurestr.split(':')\n",
      "        features = {}\n",
      "        for feature in featurelst:\n",
      "            comps = feature.split('=', maxsplit=1)\n",
      "            if len(comps) == 1:\n",
      "                if feature.strip().isnumeric():\n",
      "                    comps = ('_n', feature.strip())\n",
      "                else:\n",
      "                    sys.stderr.write('feature without value for lexical entry {} in line {}: {}\\n'.format(entry, ln, feature))\n",
      "                    e += 1\n",
      "                    continue\n",
      "            (key, value) = comps\n",
      "            value = value.replace(chr(254), ':')\n",
      "            if key in features:\n",
      "                sys.stderr.write('duplicate feature for lexical entry {} in line {}: {}={}\\n'.format(entry, ln, key, value))\n",
      "                e += 1\n",
      "                continue\n",
      "            features[key] = value\n",
      "        if 'sp' in features and features['sp'] == 'verb':\n",
      "            if 'gl' in features:\n",
      "                gloss = features['gl']\n",
      "                if gloss.startswith('to '):\n",
      "                    features['gl'] = gloss[3:]\n",
      "        lex_items[entry] = features\n",
      "        \n",
      "    lex_infile.close()\n",
      "    msgstr = \"Lexicon {}: there w\".format(lan) + ('ere {} errors'.format(e) if e != 1 else 'as 1 error') + '\\n'\n",
      "    sys.stderr.write(msgstr)\n",
      "    return lex_items\n",
      "\n",
      "lex_entries = dict((lan, read_lex(lan)) for lan in sorted(langs))\n",
      "for lan in sorted(lex_entries):\n",
      "    print('Lexicon {} has {:>5} entries'.format(lan, len(lex_entries[lan])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Lexicon arc: there were 0 errors\n",
        "Lexicon hbo: there were 0 errors\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Lexicon arc has   707 entries\n",
        "Lexicon hbo has  8529 entries\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Table filling\n",
      "\n",
      "We compose all the records for all the tables.\n",
      "\n",
      "We also generate a file that can act as the basis of an extra annotation file with lexical information."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "msg(\"Fill the tables ... \")\n",
      "cur_id = {\n",
      "    'book': -1,\n",
      "    'chapter': - 1,\n",
      "    'verse': -1,\n",
      "    'lexicon': -1,\n",
      "}\n",
      "\n",
      "def s_esc(sql): return sql.replace(\"'\", \"''\").replace('\\\\','\\\\\\\\').replace('\\n','\\\\n')\n",
      "\n",
      "cur_verse_node = None\n",
      "cur_verse_info = []\n",
      "cur_verse_first_m = None\n",
      "cur_verse_last_m = None\n",
      "cur_lex_values = {}\n",
      "\n",
      "lex_index = {}\n",
      "lex_not_found = collections.defaultdict(lambda: collections.Counter())\n",
      "tables = collections.defaultdict(lambda: [])\n",
      "field_sizes = collections.defaultdict(lambda: collections.defaultdict(lambda: 0))\n",
      "\n",
      "Fotypev = F.otype.v\n",
      "Fmonadsv = F.monads.v\n",
      "Fmin = F.minmonad.v\n",
      "Fmax = F.maxmonad.v\n",
      "Ftextv = F.g_word_utf8.v\n",
      "Foccv = F.g_cons.v\n",
      "Flexv = F.lex.v\n",
      "Flanguagev = F.language.v\n",
      "Ftrailerv = F.trailer_utf8.v\n",
      "\n",
      "dqf = outfile('etcbc4-lexicon.tsv')\n",
      "dqf.write('{}\\n'.format('\\t'.join(x[2] for x in lex_fields)))\n",
      "\n",
      "def compute_fields(lan, entry, lid, lexfeats):\n",
      "    cur_lex_values.clear()\n",
      "    return tuple(compute_field(lan, entry, lid, lexfeats, f) for f in lex_fields)\n",
      "\n",
      "def compute_field(lan, entry, lid, lexfeats, f):\n",
      "    (source, method, name, transform, datatype, datasize, dataoption) = f\n",
      "    val = None\n",
      "    if method == 'lan': val = lan\n",
      "    elif method == 'entry': val = entry\n",
      "    elif method == 'id': val = lid\n",
      "    elif method =='lex':\n",
      "        val = s_esc(lexfeats.get(f[0], ''))\n",
      "        if transform != None and val in transform: val = transform[val]\n",
      "    else: val = method(cur_lex_values[f[0]])\n",
      "    cur_lex_values[f[2]] = val\n",
      "    if f[2] in field_limits['lexicon']:\n",
      "        field_sizes['lexicon'][f[2]] = max(len(val), field_sizes['lexicon'][f[2]])\n",
      "    return val\n",
      "\n",
      "for lan in sorted(lex_entries):\n",
      "    for entry in sorted(lex_entries[lan]):\n",
      "        cur_id['lexicon'] += 1\n",
      "        format_str = '({})'.format(','.join('{}' if f[4] == 'int' else \"'{}'\" for f in lex_fields))\n",
      "        lex_index[(lan, entry)] = cur_id['lexicon']\n",
      "        entry_info = compute_fields(lan, entry, cur_id['lexicon'], lex_entries[lan][entry])\n",
      "        dqf.write('{}\\n'.format('\\t'.join(str(x) for x in entry_info)))\n",
      "        tables['lexicon'].append(format_str.format(\n",
      "            *entry_info\n",
      "        ))\n",
      "dqf.close()\n",
      "\n",
      "def do_verse(node):\n",
      "    global cur_verse_node, cur_verse_info, max_len_text, max_len_xml\n",
      "    if cur_verse_node != None:\n",
      "        this_text = ''.join('{}{}'.format(x[0], x[1]) for x in cur_verse_info)\n",
      "        this_xml = ''.join(\n",
      "            '''<w m=\"{}\" t=\"{}\" l=\"{}\">{}</w>'''.format(\n",
      "                x[2], x[1].replace('\\n', '&#xa;'), x[4], x[0]\n",
      "            ) for x in cur_verse_info)\n",
      "        field_sizes['verse']['text'] = max((len(this_text), field_sizes['verse_node']['text']))\n",
      "        field_sizes['verse']['xml'] = max((len(this_xml), field_sizes['verse_node']['xml']))\n",
      "        tables['verse'].append(\"({},{},{},{},{},'{}','{}')\".format(\n",
      "            cur_id['verse'], \n",
      "            cur_verse_first_m, \n",
      "            cur_verse_last_m, \n",
      "            cur_id['chapter'], F.verse.v(cur_verse_node), s_esc(this_text), s_esc(this_xml),\n",
      "        ))\n",
      "        for x in cur_verse_info:\n",
      "            tables['word_verse'].append(\"({}, {}, {})\".format(\n",
      "                x[2], x[3], x[4]\n",
      "            ))\n",
      "        cur_verse_info = []\n",
      "    cur_verse_node = node\n",
      "    \n",
      "\n",
      "for node in NN():\n",
      "    otype = Fotypev(node)\n",
      "    if otype == 'word':\n",
      "        text = Ftextv(node)\n",
      "        trailer = Ftrailerv(node)\n",
      "        lex = Flexv(node)\n",
      "        lang = Flanguagev(node)\n",
      "        lid = lex_index.get((lang_map[lang], lex), None)\n",
      "        if lid == None:\n",
      "            lex_not_found[(lang_map[lang], lex)][Foccv(node)] += 1\n",
      "        cur_verse_info.append((\n",
      "            text,\n",
      "            trailer,\n",
      "            Fmonadsv(node), \n",
      "            cur_id['verse'],\n",
      "            lid,\n",
      "        ))\n",
      "    elif otype == 'verse':\n",
      "        do_verse(node)\n",
      "        cur_id['verse'] += 1\n",
      "        cur_verse_first_m = Fmin(node)\n",
      "        cur_verse_last_m = Fmax(node)\n",
      "    elif otype == 'chapter':\n",
      "        do_verse(None)\n",
      "        cur_id['chapter'] += 1\n",
      "        tables['chapter'].append(\"({},{},{},{},{})\".format(\n",
      "            cur_id['chapter'], Fmin(node), Fmax(node), cur_id['book'], F.chapter.v(node),\n",
      "        ))\n",
      "    elif otype == 'book':\n",
      "        do_verse(None)\n",
      "        cur_id['book'] += 1\n",
      "        name = F.book.v(node)\n",
      "        field_sizes['book']['name'] = max((len(name), field_sizes['book']['name']))\n",
      "        tables['book'].append(\"({},{},{},'{}')\".format(\n",
      "            cur_id['book'], Fmin(node), Fmax(node), s_esc(name)\n",
      "        ))\n",
      "do_verse(None)\n",
      "\n",
      "for tb in sorted(field_limits):\n",
      "    for fl in sorted(field_limits[tb]):\n",
      "        limit = field_limits[tb][fl]\n",
      "        actual = field_sizes[tb][fl]\n",
      "        exceeded = actual > limit\n",
      "        outp = sys.stderr if exceeded else sys.stdout\n",
      "        outp.write('{:<5} {:<15}{:<15}: max size = {:>7} of {:>5}\\n'.format(\n",
      "            'ERROR' if exceeded else 'OK',\n",
      "            tb, fl, actual, limit,\n",
      "        ))\n",
      "\n",
      "msg(\"Done\")\n",
      "if len(lex_not_found):\n",
      "    sys.stderr.write('Text lexemes not found in lexicon: {}x\\n'.format(len(lex_not_found)))\n",
      "    for l in sorted(lex_not_found):\n",
      "        sys.stderr.write('{} {}\\n'.format(*l))\n",
      "        for (o, n) in sorted(lex_not_found[l].items(), key=lambda x: (-x[1], x[0])):\n",
      "            sys.stderr.write('\\t{}: {}x\\n'.format(o, n))\n",
      "else:\n",
      "    print('All lexemes have been found in the lexicon')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    23s Fill the tables ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    30s Done\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "OK    book           name           : max size =      13 of    64\n",
        "OK    lexicon        entry          : max size =      14 of    32\n",
        "OK    lexicon        entry_heb      : max size =      14 of    32\n",
        "OK    lexicon        entryid        : max size =      15 of    32\n",
        "OK    lexicon        g_entry        : max size =      32 of    32\n",
        "OK    lexicon        g_entry_heb    : max size =      31 of    32\n",
        "OK    lexicon        gloss          : max size =      26 of   128\n",
        "OK    lexicon        lan            : max size =       3 of     4\n",
        "OK    lexicon        nametype       : max size =      14 of    32\n",
        "OK    lexicon        pos            : max size =       4 of    32\n",
        "OK    lexicon        root           : max size =       4 of    32\n",
        "OK    lexicon        subpos         : max size =       4 of    32\n",
        "OK    verse          text           : max size =     249 of  1024\n",
        "OK    verse          xml            : max size =    1465 of  4096\n",
        "All lexemes have been found in the lexicon\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Extra word data\n",
      "\n",
      "Now we fetch the data needed for representing rich hebrew text.\n",
      "\n",
      "## Passage index\n",
      "When we have found our objects, we want to indicate where they occur in the bible. In order to specify the passage of a node, we have to now in what verse a node occurs. In the next code cell we create a mapping from nodes of type sentence, clause, etc to nodes of type verse. From a verse node we can read off the passage information.\n",
      "\n",
      "Conversely, we also construct an index from verses to nodes: given a verse, we make a list of all nodes belonging to that verse, in the canonical order."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "msg(\"Making verse index ...\")\n",
      "target_types = {\n",
      "    'sentence', # 'sentence_atom', \n",
      "    'clause', # 'clause_atom', \n",
      "    'phrase', # 'phrase_atom', \n",
      "    'subphrase',\n",
      "    'word',\n",
      "}\n",
      "\n",
      "object_index = collections.defaultdict(lambda: set())\n",
      "verse_monads = {}\n",
      "verse_node = {}\n",
      "\n",
      "def get_set(monads):\n",
      "    monad_set = set()\n",
      "    for rn in monads.split(','):\n",
      "        bnds = rn.split('-', 1)\n",
      "        if len(bnds) == 1:\n",
      "            monad_set.add(int(bnds[0]))\n",
      "        else: \n",
      "            monad_set |= set(range(int(bnds[0]), int(bnds[1]) + 1))\n",
      "    return frozenset(monad_set)\n",
      "\n",
      "def ranges(monadset):\n",
      "    result = []\n",
      "    cur_start = None\n",
      "    cur_end = None\n",
      "    for i in sorted(monadset):\n",
      "        if cur_start == None:\n",
      "            cur_start = i\n",
      "            cur_end = i\n",
      "        else:\n",
      "            if i == cur_end + 1:\n",
      "                cur_end += 1\n",
      "            else:\n",
      "                result.append((cur_start, cur_end))\n",
      "                cur_start = i\n",
      "                cur_end = i\n",
      "    if cur_start != None:\n",
      "        result.append((cur_start, cur_end))\n",
      "    return result\n",
      "\n",
      "for n in NN():\n",
      "    otype = F.otype.v(n)\n",
      "    if otype == 'verse':\n",
      "        verse_monads[n] = get_set(F.monads.v(n))\n",
      "    elif otype in target_types:\n",
      "        for m in get_set(F.monads.v(n)):\n",
      "            object_index[m].add(n)\n",
      "for v in verse_monads:\n",
      "    vobjects = set()\n",
      "    for m in verse_monads[v]: vobjects |= object_index[m]\n",
      "    verse_node[v] = tuple(NN(nodes=vobjects))\n",
      "msg(\"Verse index created for {} verses\".format(len(verse_node)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    33s Making verse index ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    44s Verse index created for 23213 verses\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Check\n",
      "\n",
      "We need to verify that each word occurrence does not occur in more than one object of the same type."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "msg(\"Checking whether words belong to multiple objects of the same type ...\")\n",
      "word_index = collections.defaultdict(lambda: collections.defaultdict(lambda: set()))\n",
      "for n in NN():\n",
      "    otype = F.otype.v(n)\n",
      "    if otype in target_types:\n",
      "        for m in get_set(F.monads.v(n)):\n",
      "            word_index[otype][m].add(n)\n",
      "errors = 0\n",
      "for otype in sorted(word_index):\n",
      "    good = 0\n",
      "    for monad in sorted(word_index[otype]):\n",
      "        lt = len(word_index[otype][m])\n",
      "        if lt <= 1: good += 1\n",
      "        else:\n",
      "            print(\"Monad {} belongs to {} {}s\".format(m, lt, otype))\n",
      "            errors += 1\n",
      "    print(\"{} monads occur in only one {}\".format(good, otype))\n",
      "msg(\"End checking, {} errors\".format(errors))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 22s Checking whether words belong to multiple objects of the same type ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "426555 monads occur in only one clause\n",
        "426555 monads occur in only one phrase"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "426555 monads occur in only one sentence"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "122845 monads occur in only one subphrase"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "426555 monads occur in only one word"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 37s End checking, 0 errors\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Fill the word info table with data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "msg(\"Generating word info data ...\")\n",
      "wordf = outfile('word_data.tsv')\n",
      "#wordrf = outfile('word_r_data.tsv')\n",
      "plainf = outfile('verse_plain.txt')\n",
      "wordf.write('{}\\n'.format('\\t'.join('{}_{}'.format(f[2], f[1]) for f in word_fields)))\n",
      "#wordrf.write('{}\\t{}\\t{}\\t{}\\n'.format('book', 'chapter', 'verse', '\\t'.join('{}_{}'.format(f[2], f[1]) for f in word_fields)))\n",
      "tables['word'] = []\n",
      "    \n",
      "def do_verse_info(verse):\n",
      "    vlabel = '{} {}:{}'.format(F.book.v(verse), F.chapter.v(verse), F.verse.v(verse))\n",
      "    wordf.write('!{}\\n'.format(vlabel))\n",
      "    monads = verse_monads[verse]\n",
      "    (verse_startm, verse_endm) = (min(monads), max(monads))\n",
      "    objects = verse_node[verse]\n",
      "    words = [dict() for i in range(verse_startm, verse_endm + 1)]\n",
      "    for w in words:\n",
      "        for otype in ('sentence', 'clause', 'phrase', 'subphrase', 'word'):\n",
      "            w['{}_{}'.format(otype, 'border')] = set()\n",
      "            w['{}_{}'.format(otype, 'number')] = list()\n",
      "    nwords = len(words)\n",
      "    subphrase_counter = 0\n",
      "    word_nodes = []\n",
      "    for n in objects:\n",
      "        otype = F.otype.v(n)\n",
      "        if otype == 'word': word_nodes.append(n)\n",
      "        border_prop = '{}_{}'.format(otype, 'border')\n",
      "        number_prop = '{}_{}'.format(otype, 'number')\n",
      "\n",
      "        if otype == 'subphrase': subphrase_counter += 1\n",
      "        elif otype != 'word' and 'otype' != 'subphrase': subphrase_counter = 0\n",
      "        this_info = {}\n",
      "        this_number = None\n",
      "        for f in word_fields:\n",
      "            (method, name, typ) = (f[0], '{}_{}'.format(f[2], f[1]), f[3])\n",
      "            if otype != f[2] or method == None: continue\n",
      "            if method == 'id':\n",
      "                value = subphrase_counter\n",
      "            else:\n",
      "                value = method(n)\n",
      "                if typ == 'int': value = int(value)\n",
      "            if name == number_prop:\n",
      "                this_number = value\n",
      "            else:\n",
      "                this_info[name] = value\n",
      "        if otype == 'word':\n",
      "            target = words[this_number - verse_startm]\n",
      "            target.update(this_info)\n",
      "            target[number_prop].append(this_number)            \n",
      "        else:\n",
      "            these_ranges = ranges(get_set(F.monads.v(n)))\n",
      "            nranges = len(these_ranges) - 1\n",
      "            for (e,r) in enumerate(these_ranges):\n",
      "                is_first = e == 0\n",
      "                is_last = e == nranges\n",
      "                right_border = 'rr' if is_first else 'r'\n",
      "                left_border = 'll' if is_last else 'l'\n",
      "                first_word = -1 if r[0] < verse_startm else nwords if r[0] > verse_endm else r[0] - verse_startm\n",
      "                last_word = -1 if r[1] < verse_startm else nwords if r[1] > verse_endm else r[1] - verse_startm\n",
      "                my_first_word = max(first_word, 0)\n",
      "                my_last_word = min(last_word, nwords - 1)\n",
      "                for i in range(my_first_word, my_last_word + 1):\n",
      "                    target = words[i]\n",
      "                    if not first_only[number_prop] or i == my_first_word:\n",
      "                        target[number_prop].append(this_number)\n",
      "                    for f in this_info:\n",
      "                        if not first_only[name] or i == my_first_word:\n",
      "                            words[i][f] = this_info[f]\n",
      "                    if otype == 'subphrase':\n",
      "                        words[i][border_prop].add('sy')\n",
      "                if 0 <= first_word < nwords:\n",
      "                    words[first_word][border_prop].add(right_border)\n",
      "                if 0 <= last_word < nwords:\n",
      "                    words[last_word][border_prop].add(left_border)\n",
      "    plainf.write(\"{}\\t{}\\n\".format(\n",
      "        vlabel, \n",
      "        ''.join(F.g_word_utf8.v(w) + F.trailer_utf8.v(w) for w in word_nodes).replace('\\n', '\\\\n'),\n",
      "    ))\n",
      "    for w in words:\n",
      "        row = []\n",
      "        rrow = []\n",
      "        for f in word_fields:\n",
      "            typ = f[3]\n",
      "            name = '{}_{}'.format(f[2], f[1])\n",
      "            value = w.get(name, 'NULL' if typ == 'int' else '')\n",
      "            if f[1] == 'border':\n",
      "                value = ' '.join(value)\n",
      "            elif f[1] == 'number':\n",
      "                value = ' '.join(str(v) for v in value)\n",
      "            rrow.append(str(value).replace('\\n', '\\\\n').replace('\\t', '\\\\t'))\n",
      "            if typ == 'int':\n",
      "                value = str(value)\n",
      "            else:\n",
      "                value = \"'{}'\".format(s_esc(value))\n",
      "            row.append(value)\n",
      "        tables['word'].append('({})'.format(','.join(row)))\n",
      "        wordf.write(\"{}\\n\".format('\\t'.join(rrow)))\n",
      "        #wordrf.write(\"{}\\t{}\\t{}\\t{}\\n\".format(F.book.v(verse), F.chapter.v(verse), F.verse.v(verse),'\\t'.join(rrow)))\n",
      "\n",
      "for n in NN():\n",
      "    if F.otype.v(n) == 'book':\n",
      "        msg(\"\\t{}\".format(F.book.v(n)))\n",
      "    elif F.otype.v(n) == 'verse': do_verse_info(n)\n",
      "\n",
      "wordf.close()\n",
      "#wordrf.close()\n",
      "plainf.close()\n",
      "msg(\"Done\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    54s Generating word info data ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    54s \tGenesis\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 03s \tExodus\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 11s \tLeviticus\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 17s \tNumeri\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 24s \tDeuteronomium\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 31s \tJosua\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 35s \tJudices\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 40s \tSamuel_I\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 45s \tSamuel_II\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 50s \tReges_I\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 56s \tReges_II\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 01s \tJesaia\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 10s \tJeremia\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 19s \tEzechiel\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 28s \tHosea\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 29s \tJoel\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 30s \tAmos\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 31s \tObadia\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 31s \tJona\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 31s \tMicha\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 32s \tNahum\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 32s \tHabakuk\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 32s \tZephania\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 33s \tHaggai\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 33s \tSacharia\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 34s \tMaleachi\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 35s \tPsalmi\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 44s \tIob\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 48s \tProverbia\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 51s \tRuth\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 52s \tCanticum\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 52s \tEcclesiastes\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 53s \tThreni\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 54s \tEsther\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 56s \tDaniel\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 58s \tEsra\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 3m 00s \tNehemia\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 3m 02s \tChronica_I\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 3m 08s \tChronica_II\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 3m 14s Done\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# SQL generation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "limit_row = 1000\n",
      "\n",
      "tables_head = collections.OrderedDict((\n",
      "    ('book', 'insert into book (id, first_m, last_m, name) values \\n'),\n",
      "    ('chapter', 'insert into chapter (id, first_m, last_m, book_id, chapter_num) values \\n'),\n",
      "    ('verse', 'insert into verse (id, first_m, last_m, chapter_id, verse_num, text, xml) values \\n'),\n",
      "    ('lexicon', 'insert into lexicon ({}) values \\n'.format(', '.join(f[2] for f in lex_fields))),\n",
      "    ('word', 'insert into word ({}) values \\n'.format(', '.join('{}_{}'.format(f[2], f[1]) for f in word_fields))),\n",
      "    ('word_verse', 'insert into word_verse (anchor, verse_id, lexicon_id) values \\n'),\n",
      "))\n",
      "\n",
      "sqf = outfile('etcbc4-passage.sql')\n",
      "sqf.write(text_create_sql)\n",
      "\n",
      "for table in tables_head:\n",
      "    start = tables_head[table]\n",
      "    rows = tables[table]\n",
      "    r = 0\n",
      "    while r < len(rows):\n",
      "        sqf.write(start)\n",
      "        s = min(r + limit_row, len(rows))\n",
      "        sqf.write(' {}'.format(rows[r]))\n",
      "        if r + 1 < len(rows):\n",
      "            for t in rows[r + 1:s]: sqf.write('\\n,{}'.format(t))\n",
      "        sqf.write(';\\n')\n",
      "        r = s\n",
      "        \n",
      "sqf.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}