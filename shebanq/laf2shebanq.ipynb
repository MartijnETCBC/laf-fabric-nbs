{
 "metadata": {
  "name": "",
  "signature": "sha256:f4476786b820f86862ba498c72a9460d1f9884d41c98160e4e9d21c056d98cb7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"left\" src=\"images/laf-fabric-xsmall.png\"/></a>\n",
      "<a href=\"http://emdros.org\" target=\"_blank\"><img align=\"left\" src=\"files/images/Emdros-xsmall.png\"/></a>\n",
      "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"left\"src=\"images/etcbc4easy-small.png\"/></a>\n",
      "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"right\" src=\"images/VU-ETCBC-xsmall.png\"/></a>\n",
      "<a href=\"http://tla.mpi.nl\" target=\"_blank\"><img align=\"right\" src=\"images/TLA-xsmall.png\"/></a>\n",
      "<a href=\"http://www.dans.knaw.nl\" target=\"_blank\"><img align=\"right\"src=\"images/DANS-xsmall.png\"/></a>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "LAF2SHEBANQ"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook constructs a relational database, *passage*, meant to support browsing of texts and highlighting of words.\n",
      "It contains the texts themselves, verse by verse, and it contains book and chapter information.\n",
      "The *passage* database also contains a lexicon, which is linked to the word occurrences.\n",
      "\n",
      "See the MySQL create statements below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import collections\n",
      "\n",
      "from laf.fabric import LafFabric\n",
      "from etcbc.lib import Transcription\n",
      "\n",
      "fabric = LafFabric()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s This is LAF-Fabric 4.4.4\n",
        "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
        "Feature doc: http://shebanq-doc.readthedocs.org/en/latest/texts/welcome.html\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "API = fabric.load('etcbc4', '--', 'shebanq', {\n",
      "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
      "    \"features\": ('''\n",
      "        otype monads minmonad maxmonad\n",
      "        g_word_utf8 trailer_utf8\n",
      "        book chapter verse label\n",
      "        g_cons lex language\n",
      "    ''',''),\n",
      "    \"primary\": False,\n",
      "})\n",
      "exec(fabric.localnames.format(var='fabric'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s LOADING API: please wait ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s INFO: USING DATA COMPILED AT: 2014-07-23T09-31-37\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.42s LOGFILE=/Users/dirk/laf-fabric-output/etcbc4/shebanq/__log__shebanq.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.42s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX -- FOR TASK shebanq AT 2014-10-09T13-06-06\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Data model\n",
      "\n",
      "The data model of the browsing database as as follows:\n",
      "\n",
      "There are tables ``book``, ``chapter``, ``verse``, ``word_verse``, ``lexicon``.\n",
      "\n",
      "The tables ``book``, ``chapter`` and ``verse`` contain fields ``first_m``, ``last_m``, \n",
      "denoting the first and last monad number of that book, chapter or verse.\n",
      "\n",
      "A ``book``-record contains an identifier and the name of the book.\n",
      "\n",
      "A ``chapter``-record contains an identifier, the number of the chapter, and a foreign key to the record in the ``book`` table to which the chapter belongs.\n",
      "\n",
      "A ``verse``-record contains an identifier, the number of the verse, and a foreign key to the record in the ``chapter`` table to which the verse belongs. More over, it contains the text of the whole verse in two formats:\n",
      "\n",
      "In field ``text``: the plain unicode text string of the complete verse.\n",
      "\n",
      "In field ``xml``: a sequence of ``<w>`` elements, one for each word in the verse, containing the plain unicode text string of that word as element content.\n",
      "The monad number of that word is stored in an attribute value. \n",
      "The monad number is a globally unique sequence number of a word occurrence in the Hebrew Bible, going from 1 to precisely 426,555.\n",
      "There is also a lexical identifier stored in an attribute value.\n",
      "The lexical identifier points to the lexical entry that corresponds with the word.\n",
      "\n",
      "    <w m=\"2\" l=\"3\">\u05e8\u05b5\u05d0\u05e9\u05b4\u05c1\u0596\u05d9\u05ea </w>\n",
      "\n",
      "As you see, the material between a word and the next word is appended to the first word. So, when you concatenate words, whitespace or other separators are needed.\n",
      "\n",
      "A ``word_verse``-record links a word to a verse. \n",
      "The monad number is in field ``anchor``, which is an integer, \n",
      "and the verse is specified in the field ``verse_id`` as foreign key.\n",
      "The field ``lexicon_id`` is a foreign key into the ``lexicon`` table.\n",
      "\n",
      "## Lexicon\n",
      "\n",
      "A ``lexicon`` record contains the various lexical fields, such as identifiers, entry representations,\n",
      "additional lexical properties, and a gloss.\n",
      "\n",
      "We make sure that we translate lexical feature values into values used for the etcbc4.\n",
      "We need the following information per entry:\n",
      "\n",
      "* **id** a fresh id, to be used in applications, unique over **entryid** and **lan**\n",
      "* **lan** the language of the entry, in ISO 639-3 abbreviation\n",
      "* **entryid** the string used as entry in the lexicon and as value of the ``lex`` feature in the text\n",
      "* **entry** the unpointed transliteration (= **entryid** without disambiguation marks)\n",
      "* **entry_heb** the unpointed hebrew representation, obtained by untransliterating **entry**\n",
      "* **g_entry** the pointed transliteration, without disambiguation marks, obtained from ``vc``\n",
      "* **g_entry_heb** the pointed hebrew representation, obtained by untransliterating **g_entry**\n",
      "* **root** the root, obtained from ``rt``\n",
      "* **pos** the part of speech, obtained from ``sp``\n",
      "* **nametype** the type of named entity, obtained from ``sm``\n",
      "* **subpos** subtype of part of speech, obtained from ``ls`` (aka *lexical set*)\n",
      "* **gender** the gender, in so far it is lexical, obtained from ``gn``\n",
      "* **person** the person, in so far it is lexical, obtained from ``ps`` with value transformation\n",
      "* **number** the number, in so far it is lexical, obtained from ``nu`` with value transformation\n",
      "* **gloss** the gloss from ``gl``"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Field transformation\n",
      "\n",
      "The lexical fields require a bit of attention.\n",
      "The specification in ``lex_fields`` below specifies the lexicon fields in the intended order.\n",
      "It contains instructions how to construct the field values from the lexical information obtained from the lexicon files.\n",
      "\n",
      "    (source, method, name, transformation table, data type, data size, data options)\n",
      "\n",
      "## source \n",
      "May contain one of the following:\n",
      "\n",
      "* the name of a lexical feature as shown in the lexicon files, such as ``sp``, ``vc``.\n",
      "* None. \n",
      "  In this case, **method** is a code that triggers special actions, such as getting an id or something that is available to the   program that fills the lexicon table\n",
      "* the name of an other field as shown in the **name** part of the specification. \n",
      "  In this case, **method** is a function, defined else where, that takes the value of that other field as argument. \n",
      "  The function is typically a transliteration, or a stripping action.\n",
      "\n",
      "## method\n",
      "May contain one of the following:\n",
      "\n",
      "* a code (string), indicating:\n",
      "    * ``lex``: take the value of a feature (indicated in **source**) for this entry from the lexicon file\n",
      "    * ``entry``: take the value of the entry itself as found in the lexicon file\n",
      "    * ``id``: take the id for this entry as generated by the program\n",
      "    * ``lan``: take the language of this entry\n",
      "* a function taking one argument\n",
      "    * *strip_id*: strip the non-lexeme characters at the end of the entry (the ``/ [ =`` characters)\n",
      "    * *to_heb*: transform the transliteration into real unicode Hebrew\n",
      "\n",
      "## name\n",
      "The name of the field in the to be constructed table ``lexicon`` in the database ``passage``.\n",
      "\n",
      "## data type\n",
      "The sql data type, such as ``int`` or ``varchar``, without the size and options.\n",
      "\n",
      "## data size\n",
      "The sql data size, which shows up between ``()`` after the data type\n",
      "\n",
      "## data options\n",
      "Any remaining type specification, such as `` character set utf8``.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def strip_id(entryid):\n",
      "    return entryid.rstrip('/[=')\n",
      "\n",
      "def to_heb(translit):\n",
      "    return Transcription.to_hebrew(Transcription.suffix_and_finales(translit)[0])\n",
      "\n",
      "lex_fields = (\n",
      "    (None, 'id', 'id', None, 'int', 4, ' primary key'),\n",
      "    (None, 'lan', 'lan', None, 'char', 4, ''),\n",
      "    (None, 'entry', 'entryid', None, 'varchar', 32, ''),\n",
      "    ('entryid', strip_id, 'entry', None, 'varchar', 32, ''),\n",
      "    ('entry', to_heb, 'entry_heb', None, 'varchar', 32, ' character set utf8'),\n",
      "    ('vc', 'lex', 'g_entry', None, 'varchar', 32, ''),\n",
      "    ('g_entry', to_heb, 'g_entry_heb', None, 'varchar', 32, ' character set utf8'),\n",
      "    ('rt', 'lex', 'root', None, 'varchar', 32, ''),\n",
      "    ('sp', 'lex', 'pos', None, 'varchar', 32, ''),\n",
      "    ('sm', 'lex', 'nametype', None, 'varchar', 32, ''),\n",
      "    ('ls', 'lex', 'subpos', None, 'varchar', 32, ''),\n",
      "    ('gn', 'lex', 'gender', None, 'varchar', 32, ''),\n",
      "    ('nu', 'lex', 'numbr', dict(s='sg', d='du', p='pl'), 'varchar', 32, ''),\n",
      "    ('ps', 'lex', 'person', dict((('1','p1'), ('2','p2'), ('3','p3'))), 'varchar', 32, ''),\n",
      "    ('gl', 'lex', 'gloss', None, 'varchar', 128, ' character set utf8'),\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Sanity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The texts and xml representations of verses are stored in ``varchar`` fields.\n",
      "We have to make sure that the values fit within the declared sizes of these fields.\n",
      "The code measures the maximum lengths of these fields, and it turns out that the text is maximally 434 chars and the xml 2186 chars."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "field_limits = {\n",
      "    'book': {\n",
      "        'name': 64,\n",
      "    },\n",
      "    'verse': {\n",
      "        'text': 1024,\n",
      "        'xml': 4096,\n",
      "    },\n",
      "    'lexicon': {},\n",
      "}\n",
      "for f in lex_fields:\n",
      "    if f[4].endswith('char'):\n",
      "        field_limits['lexicon'][f[2]] = f[5]\n",
      "\n",
      "config = {\n",
      "    'db': 'passage',\n",
      "}\n",
      "for tb in field_limits:\n",
      "    for fl in field_limits[tb]: config['{}_{}'.format(tb, fl)] = field_limits[tb][fl]\n",
      "\n",
      "text_create_sql = '''\n",
      "drop database if exists {db};\n",
      "\n",
      "create database {db} character set utf8;\n",
      "\n",
      "use {db};\n",
      "\n",
      "create table book(\n",
      "    id      int(4) primary key,\n",
      "    first_m int(4),\n",
      "    last_m int(4),\n",
      "    name varchar({book_name})\n",
      ");\n",
      "\n",
      "create table chapter(\n",
      "    id int(4) primary key,\n",
      "    first_m int(4),\n",
      "    last_m int(4),\n",
      "    book_id int(4),\n",
      "    chapter_num int(4),\n",
      "    foreign key (book_id) references book(id)\n",
      ");\n",
      "\n",
      "create table verse(\n",
      "    id int(4) primary key,\n",
      "    first_m int(4),\n",
      "    last_m int(4),\n",
      "    chapter_id int(4),\n",
      "    verse_num int(4),\n",
      "    text varchar({verse_text}) character set utf8,\n",
      "    xml varchar({verse_xml}) character set utf8,\n",
      "    foreign key (chapter_id) references chapter(id)\n",
      ");\n",
      "\n",
      "create table lexicon(\n",
      "    {{}}\n",
      ");\n",
      "\n",
      "create table word_verse(\n",
      "    anchor int(4) unique,\n",
      "    verse_id int(4),\n",
      "    lexicon_id int(4),\n",
      "    foreign key (verse_id) references verse(id),\n",
      "    foreign key (lexicon_id) references lexicon(id)\n",
      ");\n",
      "\n",
      "'''.format(**config).format(\n",
      "    ',\\n    '.join('{} {}({}){}'.format(f[2], f[4], f[5], f[6]) for f in lex_fields)\n",
      ")\n",
      "print(text_create_sql)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "drop database if exists passage;\n",
        "\n",
        "create database passage character set utf8;\n",
        "\n",
        "use passage;\n",
        "\n",
        "create table book(\n",
        "    id      int(4) primary key,\n",
        "    first_m int(4),\n",
        "    last_m int(4),\n",
        "    name varchar(64)\n",
        ");\n",
        "\n",
        "create table chapter(\n",
        "    id int(4) primary key,\n",
        "    first_m int(4),\n",
        "    last_m int(4),\n",
        "    book_id int(4),\n",
        "    chapter_num int(4),\n",
        "    foreign key (book_id) references book(id)\n",
        ");\n",
        "\n",
        "create table verse(\n",
        "    id int(4) primary key,\n",
        "    first_m int(4),\n",
        "    last_m int(4),\n",
        "    chapter_id int(4),\n",
        "    verse_num int(4),\n",
        "    text varchar(1024) character set utf8,\n",
        "    xml varchar(4096) character set utf8,\n",
        "    foreign key (chapter_id) references chapter(id)\n",
        ");\n",
        "\n",
        "create table lexicon(\n",
        "    id int(4) primary key,\n",
        "    lan char(4),\n",
        "    entryid varchar(32),\n",
        "    entry varchar(32),\n",
        "    entry_heb varchar(32) character set utf8,\n",
        "    g_entry varchar(32),\n",
        "    g_entry_heb varchar(32) character set utf8,\n",
        "    root varchar(32),\n",
        "    pos varchar(32),\n",
        "    nametype varchar(32),\n",
        "    subpos varchar(32),\n",
        "    gender varchar(32),\n",
        "    numbr varchar(32),\n",
        "    person varchar(32),\n",
        "    gloss varchar(128) character set utf8\n",
        ");\n",
        "\n",
        "create table word_verse(\n",
        "    anchor int(4) unique,\n",
        "    verse_id int(4),\n",
        "    lexicon_id int(4),\n",
        "    foreign key (verse_id) references verse(id),\n",
        "    foreign key (lexicon_id) references lexicon(id)\n",
        ");\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Lexicon file reading"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "langs = {'hbo', 'arc'}\n",
      "lex_base = dict((lan, '{}/{}/{}.{}'.format(API['data_dir'], 'lexicon', lan, 'etcbc')) for lan in langs)\n",
      "lang_map = {\n",
      "    'Hebrew': 'hbo',\n",
      "    'Aramaic': 'arc',\n",
      "}\n",
      "\n",
      "def read_lex(lan):\n",
      "    lex_infile = open(lex_base[lan], encoding='utf-8')\n",
      "\n",
      "    lex_items = {}\n",
      "    ln = 0\n",
      "    e = 0\n",
      "    for line in lex_infile:\n",
      "        ln += 1\n",
      "        line = line.split('#')[0]\n",
      "        line = line.rstrip()\n",
      "        if line == '': continue\n",
      "        (entry, featurestr) = line.split(sep=None, maxsplit=1)\n",
      "        entry = entry.strip('\"')\n",
      "        if entry in lex_items:\n",
      "            sys.stderr.write('duplicate lexical entry {} in line {}.\\n'.format(entry, ln))\n",
      "            e += 1\n",
      "            continue\n",
      "        if featurestr.startswith(':') and featurestr.endswith(':'):\n",
      "            featurestr = featurestr.strip(':')\n",
      "        featurestr = featurestr.replace('\\\\:', chr(254))\n",
      "        featurelst = featurestr.split(':')\n",
      "        features = {}\n",
      "        for feature in featurelst:\n",
      "            comps = feature.split('=', maxsplit=1)\n",
      "            if len(comps) == 1:\n",
      "                if feature.strip().isnumeric():\n",
      "                    comps = ('_n', feature.strip())\n",
      "                else:\n",
      "                    sys.stderr.write('feature without value for lexical entry {} in line {}: {}\\n'.format(entry, ln, feature))\n",
      "                    e += 1\n",
      "                    continue\n",
      "            (key, value) = comps\n",
      "            value = value.replace(chr(254), ':')\n",
      "            if key in features:\n",
      "                sys.stderr.write('duplicate feature for lexical entry {} in line {}: {}={}\\n'.format(entry, ln, key, value))\n",
      "                e += 1\n",
      "                continue\n",
      "            features[key] = value\n",
      "        if 'sp' in features and features['sp'] == 'verb':\n",
      "            if 'gl' in features:\n",
      "                gloss = features['gl']\n",
      "                if gloss.startswith('to '):\n",
      "                    features['gl'] = gloss[3:]\n",
      "        lex_items[entry] = features\n",
      "        \n",
      "    lex_infile.close()\n",
      "    msgstr = \"Lexicon {}: there w\".format(lan) + ('ere {} errors'.format(e) if e != 1 else 'as 1 error') + '\\n'\n",
      "    sys.stderr.write(msgstr)\n",
      "    return lex_items\n",
      "\n",
      "lex_entries = dict((lan, read_lex(lan)) for lan in sorted(langs))\n",
      "for lan in sorted(lex_entries):\n",
      "    print('Lexicon {} has {:>5} entries'.format(lan, len(lex_entries[lan])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Lexicon arc: there were 0 errors\n",
        "Lexicon hbo: there were 0 errors\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Lexicon arc has   707 entries\n",
        "Lexicon hbo has  8529 entries\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Table filling"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cur_id = {\n",
      "    'book': -1,\n",
      "    'chapter': - 1,\n",
      "    'verse': -1,\n",
      "    'lexicon': -1,\n",
      "}\n",
      "cur_verse_node = None\n",
      "cur_verse_info = []\n",
      "cur_verse_first_m = None\n",
      "cur_verse_last_m = None\n",
      "cur_lex_values = {}\n",
      "\n",
      "lex_index = {}\n",
      "lex_not_found = collections.defaultdict(lambda: collections.Counter())\n",
      "tables = collections.defaultdict(lambda: [])\n",
      "field_sizes = collections.defaultdict(lambda: collections.defaultdict(lambda: 0))\n",
      "\n",
      "Fotypev = F.otype.v\n",
      "Fmonadsv = F.monads.v\n",
      "Fmin = F.minmonad.v\n",
      "Fmax = F.maxmonad.v\n",
      "Ftextv = F.g_word_utf8.v\n",
      "Foccv = F.g_cons.v\n",
      "Flexv = F.lex.v\n",
      "Flanguagev = F.language.v\n",
      "Ftrailerv = F.trailer_utf8.v\n",
      "\n",
      "def compute_fields(lan, entry, lid, lexfeats):\n",
      "    cur_lex_values.clear()\n",
      "    return tuple(compute_field(lan, entry, lid, lexfeats, f) for f in lex_fields)\n",
      "\n",
      "def compute_field(lan, entry, lid, lexfeats, f):\n",
      "    (source, method, name, transform, datatype, datasize, dataoption) = f\n",
      "    val = None\n",
      "    if method == 'lan': val = lan\n",
      "    elif method == 'entry': val = entry\n",
      "    elif method == 'id': val = lid\n",
      "    elif method =='lex':\n",
      "        val = lexfeats.get(f[0], '').replace(\"'\", \"''\")\n",
      "        if transform != None and val in transform: val = transform[val]\n",
      "    else: val = method(cur_lex_values[f[0]])\n",
      "    cur_lex_values[f[2]] = val\n",
      "    if f[2] in field_limits['lexicon']:\n",
      "        field_sizes['lexicon'][f[2]] = max(len(val), field_sizes['lexicon'][f[2]])\n",
      "    return val\n",
      "\n",
      "for lan in sorted(lex_entries):\n",
      "    for entry in sorted(lex_entries[lan]):\n",
      "        cur_id['lexicon'] += 1\n",
      "        format_str = '({})'.format(','.join('{}' if f[4] == 'int' else \"'{}'\" for f in lex_fields))\n",
      "        lex_index[(lan, entry)] = cur_id['lexicon']\n",
      "        tables['lexicon'].append(format_str.format(\n",
      "            *compute_fields(lan, entry, cur_id['lexicon'], lex_entries[lan][entry]\n",
      "        )))\n",
      "'''\n",
      "    id int(4) primary key,\n",
      "    lan char(3),\n",
      "    entryid varchar(32),\n",
      "    entry varchar(32),\n",
      "    entry_heb varchar(32) character set utf8,\n",
      "    g_entry varchar(32),\n",
      "    g_entry_heb varchar(32) character set utf8,\n",
      "    root varchar(32),\n",
      "    pos varchar(32),\n",
      "    nametype varchar(32),\n",
      "    subpos varchar(32),\n",
      "    gender varchar(32),\n",
      "    person varchar(32),\n",
      "    number varchar(32),\n",
      "    state varchar(32),\n",
      "    gloss varchar(1000) character set utf8,\n",
      "'''\n",
      "\n",
      "def do_verse(node):\n",
      "    global cur_verse_node, cur_verse_info, max_len_text, max_len_xml\n",
      "    if cur_verse_node != None:\n",
      "        this_text = ''.join('{}{}'.format(x[0], x[1]) for x in cur_verse_info)\n",
      "        this_xml = ''.join(\n",
      "            '''<w m=\"{}\" t=\"{}\" l=\"{}\">{}</w>'''.format(\n",
      "                x[2], x[1].replace('\\n', '&#xa;'), x[4], x[0]\n",
      "            ) for x in cur_verse_info)\n",
      "        field_sizes['verse']['text'] = max((len(this_text), field_sizes['verse_node']['text']))\n",
      "        field_sizes['verse']['xml'] = max((len(this_xml), field_sizes['verse_node']['xml']))\n",
      "        tables['verse'].append(\"({},{},{},{},{},'{}','{}')\".format(\n",
      "            cur_id['verse'], \n",
      "            cur_verse_first_m, \n",
      "            cur_verse_last_m, \n",
      "            cur_id['chapter'], F.verse.v(cur_verse_node), this_text, this_xml,\n",
      "        ))\n",
      "        for x in cur_verse_info:\n",
      "            tables['word_verse'].append(\"({}, {}, {})\".format(\n",
      "                x[2], x[3], x[4]\n",
      "            ))\n",
      "        cur_verse_info = []\n",
      "    cur_verse_node = node\n",
      "    \n",
      "\n",
      "for node in NN():\n",
      "    otype = Fotypev(node)\n",
      "    if otype == 'word':\n",
      "        text = Ftextv(node)\n",
      "        trailer = Ftrailerv(node)\n",
      "        lex = Flexv(node)\n",
      "        lang = Flanguagev(node)\n",
      "        lid = lex_index.get((lang_map[lang], lex), None)\n",
      "        if lid == None:\n",
      "            lex_not_found[(lang_map[lang], lex)][Foccv(node)] += 1\n",
      "        cur_verse_info.append((\n",
      "            text,\n",
      "            trailer,\n",
      "            Fmonadsv(node), \n",
      "            cur_id['verse'],\n",
      "            lid,\n",
      "        ))\n",
      "    elif otype == 'verse':\n",
      "        do_verse(node)\n",
      "        cur_id['verse'] += 1\n",
      "        cur_verse_first_m = Fmin(node)\n",
      "        cur_verse_last_m = Fmax(node)\n",
      "    elif otype == 'chapter':\n",
      "        do_verse(None)\n",
      "        cur_id['chapter'] += 1\n",
      "        tables['chapter'].append(\"({},{},{},{},{})\".format(\n",
      "            cur_id['chapter'], Fmin(node), Fmax(node), cur_id['book'], F.chapter.v(node),\n",
      "        ))\n",
      "    elif otype == 'book':\n",
      "        do_verse(None)\n",
      "        cur_id['book'] += 1\n",
      "        name = F.book.v(node)\n",
      "        field_sizes['book']['name'] = max((len(name), field_sizes['book']['name']))\n",
      "        tables['book'].append(\"({},{},{},'{}')\".format(\n",
      "            cur_id['book'], Fmin(node), Fmax(node), name\n",
      "        ))\n",
      "do_verse(None)\n",
      "\n",
      "for tb in sorted(field_limits):\n",
      "    for fl in sorted(field_limits[tb]):\n",
      "        limit = field_limits[tb][fl]\n",
      "        actual = field_sizes[tb][fl]\n",
      "        exceeded = actual > limit\n",
      "        outp = sys.stderr if exceeded else sys.stdout\n",
      "        outp.write('{:<5} {:<15}{:<15}: max size = {:>7} of {:>5}\\n'.format(\n",
      "            'ERROR' if exceeded else 'OK',\n",
      "            tb, fl, actual, limit,\n",
      "        ))\n",
      "\n",
      "if len(lex_not_found):\n",
      "    sys.stderr.write('Text lexemes not found in lexicon: {}x\\n'.format(len(lex_not_found)))\n",
      "    for l in sorted(lex_not_found):\n",
      "        sys.stderr.write('{} {}\\n'.format(*l))\n",
      "        for (o, n) in sorted(lex_not_found[l].items(), key=lambda x: (-x[1], x[0])):\n",
      "            sys.stderr.write('\\t{}: {}x\\n'.format(o, n))\n",
      "else:\n",
      "    print('All lexemes have been found in the lexicon')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "OK    book           name           : max size =      13 of    64\n",
        "OK    lexicon        entry          : max size =      14 of    32\n",
        "OK    lexicon        entry_heb      : max size =      14 of    32\n",
        "OK    lexicon        entryid        : max size =      15 of    32\n",
        "OK    lexicon        g_entry        : max size =      32 of    32\n",
        "OK    lexicon        g_entry_heb    : max size =      31 of    32\n",
        "OK    lexicon        gender         : max size =       3 of    32\n",
        "OK    lexicon        gloss          : max size =      26 of   128\n",
        "OK    lexicon        lan            : max size =       3 of     4\n",
        "OK    lexicon        nametype       : max size =      14 of    32\n",
        "OK    lexicon        numbr          : max size =       2 of    32\n",
        "OK    lexicon        person         : max size =       2 of    32\n",
        "OK    lexicon        pos            : max size =       4 of    32\n",
        "OK    lexicon        root           : max size =       4 of    32\n",
        "OK    lexicon        subpos         : max size =       4 of    32\n",
        "OK    verse          text           : max size =     249 of  1024\n",
        "OK    verse          xml            : max size =    1465 of  4096\n",
        "All lexemes have been found in the lexicon\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# SQL generation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "limit_row = 1000\n",
      "\n",
      "tables_head = collections.OrderedDict((\n",
      "    ('book', 'insert into book (id, first_m, last_m, name) values \\n'),\n",
      "    ('chapter', 'insert into chapter (id, first_m, last_m, book_id, chapter_num) values \\n'),\n",
      "    ('verse', 'insert into verse (id, first_m, last_m, chapter_id, verse_num, text, xml) values \\n'),\n",
      "    ('lexicon', 'insert into lexicon ({}) values \\n'.format(', '.join(f[2] for f in lex_fields))),\n",
      "    ('word_verse', 'insert into word_verse (anchor, verse_id, lexicon_id) values \\n'),\n",
      "))\n",
      "\n",
      "sqf = outfile('etcbc4-passage.sql')\n",
      "sqf.write(text_create_sql)\n",
      "\n",
      "for table in tables_head:\n",
      "    start = tables_head[table]\n",
      "    rows = tables[table]\n",
      "    r = 0\n",
      "    while r < len(rows):\n",
      "        sqf.write(start)\n",
      "        s = min(r + limit_row, len(rows))\n",
      "        sqf.write(' {}'.format(rows[r]))\n",
      "        if r + 1 < len(rows):\n",
      "            for t in rows[r + 1:s]: sqf.write('\\n,{}'.format(t))\n",
      "        sqf.write(';\\n')\n",
      "        r = s\n",
      "        \n",
      "sqf.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}