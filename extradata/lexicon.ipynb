{
 "metadata": {
  "name": "",
  "signature": "sha256:ab0412ee6f24bc93911d67d95bd5c11a6b955a573cc12dd0feaa1c9421a634dd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"left\" src=\"images/laf-fabric-xsmall.png\"/></a>\n",
      "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"left\" src=\"images/VU-ETCBC-xsmall.png\"/></a>\n",
      "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"left\"src=\"images/etcbc4easy-small.png\"/></a>\n",
      "<a href=\"http://tla.mpi.nl\" target=\"_blank\"><img align=\"right\" src=\"images/TLA-xsmall.png\"/></a>\n",
      "<a href=\"http://www.dans.knaw.nl\" target=\"_blank\"><img align=\"right\"src=\"images/DANS-xsmall.png\"/></a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# The ETCBC lexicon files\n",
      "\n",
      "We examine the ETCBC lexicon files, we check their internal consistency and their correspondence\n",
      "with the ETCBC4 database, \n",
      "and finally we will add the lexicon as extra features to the ETCBC4 text database.\n",
      "\n",
      "The lexical features of a lexeme are added to each word occurrence in the text that is associated with that lexeme.\n",
      "\n",
      "Of course, this is redundant, but for text processing it is convenient. \n",
      "Alternatively, we could transform this data to\n",
      "[LMF (Lexical Markup Framework)](http://www.lexicalmarkupframework.org) [ISO-24613:2008](http://www.iso.org/iso/catalogue_detail.htm?csnumber=37327) for usage next to the LAF version of the Hebrew Text Database.\n",
      "An introduction to LMF can be found on [Wikipedia](http://en.wikipedia.org/wiki/Lexical_Markup_Framework).\n",
      "[Generate schemas](https://tla.mpi.nl/relish/lmf/).\n",
      "See also [this paper (pdf)](http://www.lrec-conf.org/proceedings/lrec2014/pdf/154_Paper.pdf) by Menzo Windhouwer et. al.\n",
      "May be later.\n",
      "\n",
      "# Biblical languages\n",
      "Among the ETCBC data are lexicon files for the biblical languages.\n",
      "We want to put that data in LMF files, and put them to use in LAF-Fabric and SHEBANQ.\n",
      "\n",
      "We have lexicon files for the following languages (given in [ISO 639-3 codes and names](http://www-01.sil.org/iso639-3/default.asp))\n",
      "\n",
      "* ``hbo`` *Ancient Hebrew*\n",
      "* ``arc`` *Official Aramaic; Imperial Aramaic*\n",
      "* ``syc`` *Classical Syriac*\n",
      "\n",
      "In the sequel we will consistently and exclusively refer to these languages by their ISO 639-3 abbreviations.\n",
      "For the ETCBC4 database we use ``hbo`` and ``arc`` only.\n",
      "\n",
      "# Feature explanation\n",
      "\n",
      "Here is a list of features encountered in the lexicon.\n",
      "Some of them correspond to features encountered in the text.\n",
      "\n",
      "## entry \n",
      "corresponds to the ``lex`` feature of word occurrences.\n",
      "\n",
      "## vocalized lexeme\n",
      "``vc``  looks like the ``g_lex`` feature of word occurrence, but they are not the same. The ``vc`` is an idealized (aka *paradigmatic*), pointed representation of the lexeme, which may or may not occur in the text. The ``g_lex`` is the realized lexeme in a concrete occurrence, of which it may be a part.\n",
      "\n",
      "## root\n",
      "``rt`` does not correspond to a feature of word occurrences.\n",
      "It contains the abstract root of a lexeme, in the sense of the form from which the lexeme has been *derived*.\n",
      "\n",
      "## part of speech\n",
      "``sp`` corresponds exactly to ``sp`` on word occurrences.\n",
      "\n",
      "## subpart of speech (lexical set)\n",
      "``ls`` corresponds to exactly to ``ls`` on word occurrences.\n",
      "\n",
      "## gender\n",
      "``gn`` corresponds to ``gn`` on word occurrences, except for words whose gender is not lexically determined such as verbs and adjectives. The policy is to include this feature in the lexicon only when the gender is not obvious from the set of its textual occurrences.\n",
      "\n",
      "## number\n",
      "``nu`` corresponds to ``nu`` on word occurrences, but in general number is not lexically determined.\n",
      "\n",
      "## person\n",
      "``ps`` corresponds to ``ps`` on word occurrences, but in general person is not lexically detemined.\n",
      "It occurs on a small number of ``pspr`` s in the lexicon.\n",
      "\n",
      "## state\n",
      "``st`` corresponds to ``st`` on word occurrences. It is only marked for a few ``nmpr`` s in the lexicon.\n",
      "\n",
      "## reference type\n",
      "``sm``  does not correspond to a textual feature. It is the reference type of proper nouns and personal pronouns (rarely), and consists of a comma separated list of the following values:\n",
      "\n",
      "* ``gens`` people or tribe name\n",
      "* ``mens`` month name\n",
      "* ``pers`` person name\n",
      "* ``ppde`` possible demonstrative pronoun, occurs only for personal pronouns\n",
      "* ``topo`` place name\n",
      "\n",
      "## comment or corrections\n",
      "``co`` does not correspond to any feature on word occurrences. Comments are used to express alternatives for the information entered. Comments are very rare in the lexicon.\n",
      "\n",
      "## kb\n",
      "``kb`` unclear. In *arc* there is one occurrence containing a transliterated words, in *hbo* there are a few occurrences containing a different value for ``sp``.\n",
      "\n",
      "## fc\n",
      "``fc`` unclear.\n",
      "\n",
      "# Lexicon data\n",
      "\n",
      "We add the lexicon data as extra features to the ETCBC4 database.\n",
      "\n",
      "For every word, we add *lexical* features, i.e. features that are obtained from looking up the word in the lexicon and then reading off extra attributes from the lexical entry.\n",
      "\n",
      "We add these features into the ``etcbc4`` annotation space, with label ``lex``.\n",
      "These are the extra features:\n",
      "\n",
      "* **lid** a fresh id, to be used in applications, unique over **entryid** and **lan**\n",
      "* **lan** the language of the entry, in ISO 639-3 abbreviation\n",
      "* **entryid** (= ``entry``) the string used as entry in the lexicon and as value of the ``lex`` feature in the text\n",
      "* **entry** the unpointed transliteration (= **entryid** without disambiguation marks)\n",
      "* **entry_heb** the unpointed hebrew representation, obtained by untransliterating **entry**\n",
      "* (shortly) **g_entry** (= ``vc``) the pointed transliteration, without disambiguation marks\n",
      "* (shortly) **g_entry_heb** the pointed hebrew representation, obtained by untransliterating **g_entry**\n",
      "* (shortly) **root** (= ``rt``) the root\n",
      "* **gloss** (= ``gl``) the gloss\n",
      "* **pos** (= ``sp``) the part of speech\n",
      "* **nametype** (= ``sm``) the type of named entity\n",
      "* **subpos** (= ``ls``) subtype of part of speech (aka *lexical set*)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import collections\n",
      "from IPython.display import display, HTML, FileLinks\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "import laf\n",
      "from laf.fabric import LafFabric\n",
      "from etcbc.preprocess import prepare\n",
      "from etcbc.extra import ExtraData\n",
      "from etcbc.lib import Transcription\n",
      "fabric = LafFabric()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s This is LAF-Fabric 4.4.6\n",
        "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
        "Feature doc: http://shebanq-doc.readthedocs.org/en/latest/texts/welcome.html\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Create annotations from lexicon file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "API=fabric.load('etcbc4s', '--', 'lexicon', {\n",
      "    \"xmlids\": {\"node\": True, \"edge\": False},\n",
      "    \"features\": ('''\n",
      "        oid otype\n",
      "        g_word g_word_utf8 language lex lex_utf8 g_lex g_lex_utf8\n",
      "        sp ls gn nu ps st    ''',\n",
      "    '''\n",
      "    '''),\n",
      "    \"prepare\": prepare,\n",
      "}, verbose='DETAIL')\n",
      "exec(fabric.localnames.format(var='fabric'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s LOADING API: please wait ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING m: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING a: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.01s DETAIL: load main: G.node_anchor_min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.07s DETAIL: load main: G.node_anchor_max\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.15s DETAIL: load main: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.20s DETAIL: load main: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.62s DETAIL: load main: G.edges_from\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.69s DETAIL: load main: G.edges_to\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.77s DETAIL: load main: X. [node]  -> \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.96s DETAIL: load main: X. [node]  <- \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.82s DETAIL: load main: F.etcbc4_db_oid [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.58s DETAIL: load main: F.etcbc4_db_otype [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.38s DETAIL: load main: F.etcbc4_ft_g_lex [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  4.68s DETAIL: load main: F.etcbc4_ft_g_lex_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.00s DETAIL: load main: F.etcbc4_ft_g_word [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.25s DETAIL: load main: F.etcbc4_ft_g_word_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.58s DETAIL: load main: F.etcbc4_ft_gn [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.73s DETAIL: load main: F.etcbc4_ft_language [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.93s DETAIL: load main: F.etcbc4_ft_lex [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  6.13s DETAIL: load main: F.etcbc4_ft_lex_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  6.41s DETAIL: load main: F.etcbc4_ft_ls [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  6.62s DETAIL: load main: F.etcbc4_ft_nu [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  6.82s DETAIL: load main: F.etcbc4_ft_ps [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  7.02s DETAIL: load main: F.etcbc4_ft_sp [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  7.23s DETAIL: load main: F.etcbc4_ft_st [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  7.39s LOGFILE=/Users/dirk/laf-fabric-output/etcbc4s/lexicon/__log__lexicon.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  7.40s DETAIL: prep prep: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  7.46s DETAIL: prep prep: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  7.95s INFO: DATA LOADED FROM SOURCE etcbc4s AND ANNOX -- FOR TASK lexicon AT 2014-10-24T09-12-24\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Read and check the lexical files\n",
      "\n",
      "First we read the lexicon, and perform some internal consistency checks, e.g. whether there are duplicate lexical entries."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "langs = {'hbo', 'arc'}\n",
      "lex_base = dict((lan, '{}/{}/{}.{}'.format(API['data_dir'], 'lexicon', lan, 'etcbc4s')) for lan in langs)\n",
      "lang_map = {\n",
      "    'Hebrew': 'hbo',\n",
      "    'Aramaic': 'arc',\n",
      "}\n",
      "\n",
      "def read_lex(lan):\n",
      "    lex_infile = open(lex_base[lan], encoding='utf-8')\n",
      "    lex_outfile = outfile('{}.txt'.format(lan))\n",
      "    lex_errfile = outfile('{}.err.txt'.format(lan))\n",
      "\n",
      "    lex_items = {}\n",
      "    ln = 0\n",
      "    e = 0\n",
      "    for line in lex_infile:\n",
      "        ln += 1\n",
      "        line = line.rstrip()\n",
      "        line = line.split('#')[0]\n",
      "        if line == '': continue\n",
      "        (entry, featurestr) = line.split(sep=None, maxsplit=1)\n",
      "        entry = entry.strip('\"')\n",
      "        if entry in lex_items:\n",
      "            lex_errfile.write('duplicate lexical entry {} in line {}.\\n'.format(entry, ln))\n",
      "            e += 1\n",
      "            continue\n",
      "        featurestr = featurestr.strip(':')\n",
      "        featurestr = featurestr.replace('\\\\:', chr(254))\n",
      "        featurelst = featurestr.split(':')\n",
      "        features = {}\n",
      "        for feature in featurelst:\n",
      "            comps = feature.split('=', maxsplit=1)\n",
      "            if len(comps) == 1:\n",
      "                if feature.strip().isnumeric():\n",
      "                    comps = ('_n', feature.strip())\n",
      "                else:\n",
      "                    lex_errfile.write('feature without value for lexical entry {} in line {}: {}\\n'.format(entry, ln, feature))\n",
      "                    e += 1\n",
      "                    continue\n",
      "            (key, value) = comps\n",
      "            value = value.replace(chr(254), ':')\n",
      "            if key in features:\n",
      "                lex_errfile.write('duplicate feature for lexical entry {} in line {}: {}={}\\n'.format(entry, ln, key, value))\n",
      "                e += 1\n",
      "                continue\n",
      "            features[key] = value\n",
      "        if 'sp' in features and features['sp'] == 'verb':\n",
      "            if 'gl' in features:\n",
      "                gloss = features['gl']\n",
      "                if gloss.startswith('to '):\n",
      "                    features['gl'] = gloss[3:]\n",
      "        lex_items[entry] = features\n",
      "        lex_outfile.write('{}\\t{}\\n'.format(entry, features))\n",
      "        \n",
      "    lex_infile.close()\n",
      "    lex_outfile.close()\n",
      "    lex_errfile.close()\n",
      "    msgstr = \"Lexicon {}: there w\".format(lan) + ('ere {} errors'.format(e) if e != 1 else 'as 1 error')\n",
      "    print(msgstr)\n",
      "    return lex_items\n",
      "\n",
      "msg(\"Reading lexicon ...\")\n",
      "lex_entries = dict((lan, read_lex(lan)) for lan in sorted(langs))\n",
      "for lan in sorted(lex_entries):\n",
      "    print('Lexicon {} has {:>5} entries'.format(lan, len(lex_entries[lan])))\n",
      "msg(\"Done\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    19s Reading lexicon ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Lexicon arc: there were 0 errors\n",
        "Lexicon hbo: there were 0 errors"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    19s Done\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Lexicon arc has   707 entries\n",
        "Lexicon hbo has  8520 entries\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Gather the lexemes from the etcbc4 database\n",
      "\n",
      "We inspect all word occurrences of the etcbc4 database, inspect their language and lexeme values, and construct sets of lexemes that belong to each of the two languages, ``hbo`` and ``arc``."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lex_text = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(lambda: set())))\n",
      "do_value_compare = {'sp', 'ls', 'gn', 'ps', 'nu', 'st'}\n",
      "text_value_set = collections.defaultdict(lambda: set())\n",
      "node_lex = {}\n",
      "\n",
      "msg(\"Reading ETCBC database ...\")\n",
      "text_langs = set()\n",
      "for n in NN():\n",
      "    otype = F.otype.v(n)\n",
      "    if otype != 'word': continue\n",
      "    lan = lang_map[F.language.v(n)]\n",
      "    text_langs.add(lan)\n",
      "    lex = F.lex.v(n)\n",
      "    node_lex[n] = (lan,lex)\n",
      "    lex_text[lan][lex]['sp'].add(F.sp.v(n))\n",
      "    lex_text[lan][lex]['ls'].add(F.ls.v(n))\n",
      "    lex_text[lan][lex]['gn'].add(F.gn.v(n))\n",
      "    lex_text[lan][lex]['nu'].add(F.nu.v(n))\n",
      "    lex_text[lan][lex]['ps'].add(F.ps.v(n))\n",
      "    lex_text[lan][lex]['vc'].add(F.g_lex.v(n))\n",
      "    for p in do_value_compare:\n",
      "        text_value_set[p].add(F.item[p].v(n))        \n",
      "\n",
      "tf = outfile('text_lexemes.txt')\n",
      "for lan in sorted(lex_text):\n",
      "    for lex in sorted(lex_text[lan]):\n",
      "        tf.write('{} \"{}\"\\n'.format(lan, lex))\n",
      "tf.close()\n",
      "msg(\"Done\")\n",
      "for lan in sorted(lex_text):\n",
      "    print('Language {} has {:>5} lexemes in the etcbc4 text'.format(lan, len(lex_text[lan])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    28s Reading ETCBC database ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    38s Done\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Language arc has   707 lexemes in the etcbc4 text\n",
        "Language hbo has  8520 lexemes in the etcbc4 text\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# More checks\n",
      "\n",
      "We check the following matters.\n",
      "\n",
      "## Intersection between Hebrew and Aramaic\n",
      "\n",
      "Are there entries that are Hebrew and Aramaic?\n",
      "\n",
      "We check \n",
      "* whether the etcbc4 database has marked some lexemes belonging to ``hbo`` as well as belonging to ``arc``\n",
      "* whether the lexica for ``hbo`` and ``arc`` share lexeme entries\n",
      "* whether the lexical intersection of ``hbo`` and ``arc`` is equal to the textual intersection of ``hbo`` and ``arc``."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "arc_lex = set(lex_entries['arc'])\n",
      "hbo_lex = set(lex_entries['hbo'])\n",
      "\n",
      "arc_text = set(lex_text['arc'])\n",
      "hbo_text = set(lex_text['hbo'])\n",
      "\n",
      "hbo_and_arc_text = arc_text & hbo_text\n",
      "hbo_and_arc_lex = arc_lex & hbo_lex\n",
      "\n",
      "lex_min_text = hbo_and_arc_lex - hbo_and_arc_text\n",
      "text_min_lex = hbo_and_arc_text - hbo_and_arc_lex\n",
      "\n",
      "\n",
      "print('The intersection of hbo and arc in the etcbc4 text contains {} lexemes'.format(len(hbo_and_arc_text)))\n",
      "print('The intersection of hbo and arc in the lexicon     contains {} lexemes'.format(len(hbo_and_arc_lex)))\n",
      "print(\"Lexemes in the lexical intersection of hbo and arc but not in the textual intersection: {}x: {}\".format(\n",
      "    len(lex_min_text), lex_min_text)\n",
      ")\n",
      "print(\"Lexemes in the textual intersection of hbo and arc but not in the lexical intersection: {}x: {}\".format(\n",
      "    len(text_min_lex), text_min_lex)\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The intersection of hbo and arc in the etcbc4 text contains 459 lexemes\n",
        "The intersection of hbo and arc in the lexicon     contains 459 lexemes\n",
        "Lexemes in the lexical intersection of hbo and arc but not in the textual intersection: 0x: set()\n",
        "Lexemes in the textual intersection of hbo and arc but not in the lexical intersection: 0x: set()\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Match between lexicon and text\n",
      "Let us now check whether all lexemes in the text occur in the lexicon and vice versa."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "arc_text_min_lex = arc_text - arc_lex\n",
      "arc_lex_min_text = arc_lex - arc_text\n",
      "\n",
      "hbo_text_min_lex = hbo_text - hbo_lex\n",
      "hbo_lex_min_text = hbo_lex - hbo_text\n",
      "\n",
      "for (myset, mymsg) in (\n",
      "    (arc_text_min_lex, 'arc: lexemes in text but not in lexicon'),\n",
      "    (arc_lex_min_text, 'arc: lexemes in lexicon but not in text'),\n",
      "    (hbo_text_min_lex, 'hbo: lexemes in text but not in lexicon'),\n",
      "    (hbo_lex_min_text, 'hbo: lexemes in lexicon but not in text'),\n",
      "):\n",
      "    print('{}: {}x{}'.format(mymsg, len(myset), '' if not myset else '\\n\\t{}'.format(', '.join(sorted(myset)))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "arc: lexemes in text but not in lexicon: 0x\n",
        "arc: lexemes in lexicon but not in text: 0x\n",
        "hbo: lexemes in text but not in lexicon: 0x\n",
        "hbo: lexemes in lexicon but not in text: 0x\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Feature richness\n",
      "\n",
      "Which features do the entries have, and what percentage of the entries has those features?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_count = collections.defaultdict(lambda: collections.Counter())\n",
      "inspect_prop = collections.defaultdict(\n",
      "    lambda: collections.defaultdict(lambda: collections.defaultdict(lambda: collections.Counter()))\n",
      ")\n",
      "lex_value_set = collections.defaultdict(lambda: set())\n",
      "\n",
      "close_inspect = {'sp', 'sm', 'ls', 'gn', 'ps', 'nu', 'st'} # , 'co', 'kb', 'fc'\n",
      "\n",
      "for lan in lex_entries:\n",
      "    entries = lex_entries[lan]\n",
      "    for entry in entries:\n",
      "        features = entries[entry]\n",
      "        for feature in features:\n",
      "            feature_count[lan][feature] += 1\n",
      "        for p in close_inspect:\n",
      "            if p in features:\n",
      "                inspect_prop[lan][p][features[p]][features['sp']] += 1\n",
      "        if lan != 'syc':\n",
      "            for p in do_value_compare:\n",
      "                if p in features:\n",
      "                    lex_value_set[p].add(features[p])\n",
      "\n",
      "for lan in feature_count:\n",
      "    nentries = len(lex_entries[lan])\n",
      "    for feature in feature_count[lan]:\n",
      "        fv = feature_count[lan][feature]\n",
      "        feature_count[lan][feature] = fv * 100 / nentries\n",
      "\n",
      "print(\"Feature occurrences in the lexicon\")\n",
      "for lan in sorted(feature_count):\n",
      "    feature_spec = '\\n'.join('\\t{:<8}: {:>6.2f}%'.format(f, v) for (f,v) in sorted(feature_count[lan].items(), key=lambda x: (-x[1], x[0])))\n",
      "    print(\"{}\\n{}\\t\".format(lan, feature_spec))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Feature occurrences in the lexicon\n",
        "arc\n",
        "\tgl      : 100.00%\n",
        "\tsp      : 100.00%\n",
        "\tvc      : 100.00%\n",
        "\tgn      :  36.35%\n",
        "\tls      :   6.79%\n",
        "\tsm      :   5.80%\n",
        "\tnu      :   2.69%\n",
        "\tps      :   1.41%\n",
        "\trt      :   0.71%\n",
        "\tco      :   0.14%\n",
        "\tkb      :   0.14%\t\n",
        "hbo\n",
        "\tgl      : 100.00%\n",
        "\tsp      : 100.00%\n",
        "\tvc      :  91.31%\n",
        "\tgn      :  44.54%\n",
        "\tsm      :  30.27%\n",
        "\tfc      :  23.54%\n",
        "\trt      :   4.31%\n",
        "\tls      :   3.77%\n",
        "\tnu      :   0.27%\n",
        "\tst      :   0.23%\n",
        "\tps      :   0.16%\n",
        "\tkb      :   0.04%\n",
        "\tco      :   0.01%\t\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Detail feature values and occurrences\")\n",
      "for lan in sorted(inspect_prop):\n",
      "    print(\"{}\\n\".format(lan))\n",
      "    for p in sorted(inspect_prop[lan]):\n",
      "        print(\"\\t{}\\n\".format(p))\n",
      "        for value in sorted(inspect_prop[lan][p]):\n",
      "            inspect_spec = '\\n'.join('\\t\\t\\t{:<6}: {:>3}x'.format(f, v) for (f,v) in sorted(\n",
      "                inspect_prop[lan][p][value].items(), key=lambda x: (-x[1], x[0])\n",
      "            ))\n",
      "            print(\"\\t\\t{}\\n{}\\t\".format(value, inspect_spec))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Detail feature values and occurrences\n",
        "arc\n",
        "\n",
        "\tgn\n",
        "\n",
        "\t\tf\n",
        "\t\t\tsubs  :  43x\n",
        "\t\t\tprde  :   2x\n",
        "\t\t\tprps  :   2x\n",
        "\t\t\tadjv  :   1x\t\n",
        "\t\tm\n",
        "\t\t\tsubs  : 171x\n",
        "\t\t\tnmpr  :  29x\n",
        "\t\t\tprps  :   6x\n",
        "\t\t\tprde  :   2x\n",
        "\t\t\tadjv  :   1x\t\n",
        "\tls\n",
        "\n",
        "\t\tcard\n",
        "\t\t\tsubs  :  12x\t\n",
        "\t\tcjad\n",
        "\t\t\tadvb  :   1x\t\n",
        "\t\tfocp\n",
        "\t\t\tadvb  :   1x\n",
        "\t\t\tinrg  :   1x\t\n",
        "\t\tgntl\n",
        "\t\t\tadjv  :  10x\t\n",
        "\t\tmult\n",
        "\t\t\tsubs  :   1x\t\n",
        "\t\tnmcp\n",
        "\t\t\tsubs  :   1x\t\n",
        "\t\tnmdi\n",
        "\t\t\tsubs  :   3x\t\n",
        "\t\tordn\n",
        "\t\t\tadjv  :   4x\t\n",
        "\t\tpadv\n",
        "\t\t\tsubs  :   2x\t\n",
        "\t\tppre\n",
        "\t\t\tsubs  :   9x\t\n",
        "\t\tquot\n",
        "\t\t\tverb  :   2x\t\n",
        "\t\tvbcp\n",
        "\t\t\tverb  :   1x\t\n",
        "\tnu\n",
        "\n",
        "\t\tp\n",
        "\t\t\tprps  :   6x\n",
        "\t\t\tprde  :   5x\t\n",
        "\t\ts\n",
        "\t\t\tprde  :   4x\n",
        "\t\t\tprps  :   4x\t\n",
        "\tps\n",
        "\n",
        "\t\t1\n",
        "\t\t\tprps  :   2x\t\n",
        "\t\t2\n",
        "\t\t\tprps  :   2x\t\n",
        "\t\t3\n",
        "\t\t\tprps  :   6x\t\n",
        "\tsm\n",
        "\n",
        "\t\tgens\n",
        "\t\t\tnmpr  :   1x\t\n",
        "\t\tmens\n",
        "\t\t\tnmpr  :   1x\t\n",
        "\t\tpers\n",
        "\t\t\tnmpr  :  29x\t\n",
        "\t\tppde\n",
        "\t\t\tprps  :   2x\t\n",
        "\t\ttopo\n",
        "\t\t\tnmpr  :   8x\t\n",
        "\tsp\n",
        "\n",
        "\t\tadjv\n",
        "\t\t\tadjv  :  51x\t\n",
        "\t\tadvb\n",
        "\t\t\tadvb  :  19x\t\n",
        "\t\tconj\n",
        "\t\t\tconj  :   3x\t\n",
        "\t\tinrg\n",
        "\t\t\tinrg  :   1x\t\n",
        "\t\tintj\n",
        "\t\t\tintj  :   4x\t\n",
        "\t\tnega\n",
        "\t\t\tnega  :   2x\t\n",
        "\t\tnmpr\n",
        "\t\t\tnmpr  :  39x\t\n",
        "\t\tprde\n",
        "\t\t\tprde  :   9x\t\n",
        "\t\tprep\n",
        "\t\t\tprep  :  10x\t\n",
        "\t\tprin\n",
        "\t\t\tprin  :   2x\t\n",
        "\t\tprps\n",
        "\t\t\tprps  :  10x\t\n",
        "\t\tsubs\n",
        "\t\t\tsubs  : 370x\t\n",
        "\t\tverb\n",
        "\t\t\tverb  : 187x\t\n",
        "hbo\n",
        "\n",
        "\tgn\n",
        "\n",
        "\t\tf\n",
        "\t\t\tsubs  : 254x\n",
        "\t\t\tnmpr  : 108x\n",
        "\t\t\tprps  :   5x\n",
        "\t\t\tprde  :   4x\t\n",
        "\t\tm\n",
        "\t\t\tsubs  : 1880x\n",
        "\t\t\tnmpr  : 1531x\n",
        "\t\t\tprps  :   5x\n",
        "\t\t\tprde  :   2x\t\n",
        "\t\tm,f\n",
        "\t\t\tnmpr  :   6x\t\n",
        "\tls\n",
        "\n",
        "\t\tafad\n",
        "\t\t\tadvb  :   1x\t\n",
        "\t\tcard\n",
        "\t\t\tsubs  :  16x\t\n",
        "\t\tcjad\n",
        "\t\t\tadvb  :   2x\t\n",
        "\t\tfocp\n",
        "\t\t\tadvb  :   4x\n",
        "\t\t\tinrg  :   1x\t\n",
        "\t\tgntl\n",
        "\t\t\tadjv  : 211x\t\n",
        "\t\tmult\n",
        "\t\t\tsubs  :   2x\t\n",
        "\t\tnmcp\n",
        "\t\t\tsubs  :   5x\t\n",
        "\t\tnmdi\n",
        "\t\t\tsubs  :   6x\t\n",
        "\t\tordn\n",
        "\t\t\tadjv  :  10x\t\n",
        "\t\tpadv\n",
        "\t\t\tsubs  :  39x\t\n",
        "\t\tppre\n",
        "\t\t\tsubs  :  20x\t\n",
        "\t\tquot\n",
        "\t\t\tverb  :   2x\t\n",
        "\t\tvbcp\n",
        "\t\t\tverb  :   2x\t\n",
        "\tnu\n",
        "\n",
        "\t\tp\n",
        "\t\t\tprps  :   8x\n",
        "\t\t\tprde  :   2x\t\n",
        "\t\ts\n",
        "\t\t\tprde  :   7x\n",
        "\t\t\tprps  :   6x\t\n",
        "\tps\n",
        "\n",
        "\t\t1\n",
        "\t\t\tprps  :   4x\t\n",
        "\t\t2\n",
        "\t\t\tprps  :   5x\t\n",
        "\t\t3\n",
        "\t\t\tprps  :   5x\t\n",
        "\tsm\n",
        "\n",
        "\t\tgens\n",
        "\t\t\tnmpr  :  52x\t\n",
        "\t\tgens,topo\n",
        "\t\t\tnmpr  :   2x\t\n",
        "\t\tmens\n",
        "\t\t\tnmpr  :   9x\t\n",
        "\t\tpers\n",
        "\t\t\tnmpr  : 1641x\t\n",
        "\t\tpers,gens\n",
        "\t\t\tnmpr  :  13x\t\n",
        "\t\tpers,gens,topo\n",
        "\t\t\tnmpr  :  19x\t\n",
        "\t\tppde\n",
        "\t\t\tprps  :   5x\t\n",
        "\t\ttopo\n",
        "\t\t\tnmpr  : 838x\t\n",
        "\tsp\n",
        "\n",
        "\t\tadjv\n",
        "\t\t\tadjv  : 558x\t\n",
        "\t\tadvb\n",
        "\t\t\tadvb  :  33x\t\n",
        "\t\tart\n",
        "\t\t\tart   :   1x\t\n",
        "\t\tconj\n",
        "\t\t\tconj  :  12x\t\n",
        "\t\tinrg\n",
        "\t\t\tinrg  :  17x\t\n",
        "\t\tintj\n",
        "\t\t\tintj  :  23x\t\n",
        "\t\tnega\n",
        "\t\t\tnega  :   4x\t\n",
        "\t\tnmpr\n",
        "\t\t\tnmpr  : 2581x\t\n",
        "\t\tprde\n",
        "\t\t\tprde  :   9x\t\n",
        "\t\tprep\n",
        "\t\t\tprep  :  15x\t\n",
        "\t\tprin\n",
        "\t\t\tprin  :   3x\t\n",
        "\t\tprps\n",
        "\t\t\tprps  :  14x\t\n",
        "\t\tsubs\n",
        "\t\t\tsubs  : 3698x\t\n",
        "\t\tverb\n",
        "\t\t\tverb  : 1552x\t\n",
        "\tst\n",
        "\n",
        "\t\ta\n",
        "\t\t\tnmpr  :  20x\t\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Consistency of text features\n",
      "\n",
      "Multiple occurrences in the text point to the same lexeme. \n",
      "Some properties of those occurrences are in fact properties of the lexeme, e.g. the part of speech.\n",
      "\n",
      "The question arises: is the property assigned to the word occurrences in such a way that all occurrences of the same lexeme have the same value for that property?\n",
      "\n",
      "We will see that this is in general not the case.\n",
      "There are features, that have a definition in the lexicon, but that can be overridden on word occurrences.\n",
      "\n",
      "However, some features are more consistent than others: the features **pos** (=``sp``), **subpos** (= ``ls``), **gender**.\n",
      "\n",
      "It becomes also clear that the lexical property **g_entry** (= ``vc``) (aka *vocalized lexeme*) is mostly different from the\n",
      "the textual feature **g_lex** (aka *graphical lexeme*).\n",
      "The output file *inconsistent.csv* shows exactly what is going on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "consistent_props = {'sp', 'ls', 'gn', 'vc'}\n",
      "variable_gender = {'verb', 'adjv'}\n",
      "\n",
      "exceptions = collections.defaultdict(lambda: collections.defaultdict(lambda: set()))\n",
      "exceptions_gn = collections.defaultdict(lambda: collections.Counter())\n",
      "\n",
      "incons = outfile('inconsistent.csv')\n",
      "for lan in sorted(lex_text):\n",
      "    lexemes = lex_text[lan]\n",
      "    for lexeme in sorted(lexemes):\n",
      "        properties = lexemes[lexeme]\n",
      "        for prop in consistent_props:\n",
      "            if prop in properties:\n",
      "                values = properties[prop]\n",
      "                psp = list(properties['sp'])[0]\n",
      "                if len(values) > 1:\n",
      "                    if prop == 'gn':\n",
      "                        if (len(set(properties['sp']) & variable_gender) != 0): continue\n",
      "                        exceptions_gn[lan][psp] += 1\n",
      "                    exceptions[lan][prop].add(lexeme)\n",
      "                    incons.write('\"{}\";\"{}\";\"{}\";\"{}\";{};\"{}\"\\n'.format(lan, prop, lexeme, psp, len(values), '\";\"'.join(values)))\n",
      "incons.close()\n",
      "for lan in sorted(text_langs):\n",
      "    print(\"{}\\n\".format(lan))\n",
      "    for prop in sorted(consistent_props):\n",
      "        extra = ''\n",
      "        if prop == 'gn':\n",
      "            for psp in exceptions_gn[lan]:\n",
      "                extra += '\\n{}{}: {}x'.format(' ' * 8, psp, exceptions_gn[lan][psp])\n",
      "        print(\"{}{:<8}: {:>4} inconsistent lexemes{}\".format(' ' * 4, prop, len(exceptions.get(lan, {}).get(prop, set())), extra))        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "arc\n",
        "\n",
        "    gn      :   10 inconsistent lexemes\n",
        "        subs: 10x\n",
        "    ls      :    0 inconsistent lexemes\n",
        "    sp      :    0 inconsistent lexemes\n",
        "    vc      :  262 inconsistent lexemes\n",
        "hbo\n",
        "\n",
        "    gn      :   63 inconsistent lexemes\n",
        "        subs: 52x\n",
        "        nmpr: 11x\n",
        "    ls      :    0 inconsistent lexemes\n",
        "    sp      :    0 inconsistent lexemes\n",
        "    vc      : 3862 inconsistent lexemes\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Consistency of feature values between lexicon and text\n",
      "\n",
      "Are the *values* of features used in the text database consistent with the values used in the lexicon?\n",
      "\n",
      "If not, we will apply a value transformation that harmonizes the values in lexical entries with those in textual features.\n",
      "We adapt the lexical values to the textual ones. We do this only for features whose value domains are enumerations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for p in do_value_compare:\n",
      "    print(p)\n",
      "    text_not_lex = sorted(text_value_set[p] - lex_value_set[p])\n",
      "    lex_not_text = sorted(lex_value_set[p] - text_value_set[p])\n",
      "    print('\\tin text and not in lex: {}: {}'.format(len(text_not_lex), text_not_lex))\n",
      "    print('\\tin lex and not in text: {}: {}'.format(len(lex_not_text), lex_not_text))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ls\n",
        "\tin text and not in lex: 2: ['none', 'ques']\n",
        "\tin lex and not in text: 0: []\n",
        "ps\n",
        "\tin text and not in lex: 5: ['NA', 'p1', 'p2', 'p3', 'unknown']\n",
        "\tin lex and not in text: 3: ['1', '2', '3']\n",
        "gn\n",
        "\tin text and not in lex: 2: ['NA', 'unknown']\n",
        "\tin lex and not in text: 1: ['m,f']\n",
        "nu\n",
        "\tin text and not in lex: 5: ['NA', 'du', 'pl', 'sg', 'unknown']\n",
        "\tin lex and not in text: 2: ['p', 's']\n",
        "sp\n",
        "\tin text and not in lex: 0: []\n",
        "\tin lex and not in text: 0: []\n",
        "st\n",
        "\tin text and not in lex: 3: ['NA', 'c', 'e']\n",
        "\tin lex and not in text: 0: []\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Composing a lexical data file\n",
      "\n",
      "The specification in ``lex_fields`` below specifies the lexicon fields in the intended order.\n",
      "It contains instructions how to construct the field values from the lexical information obtained from the lexicon files.\n",
      "\n",
      "    (source, method, name, transformation table, data type, data size, data options)\n",
      "\n",
      "## source \n",
      "May contain one of the following:\n",
      "\n",
      "* the name of a lexical feature as shown in the lexicon files, such as ``sp``, ``vc``.\n",
      "* None. \n",
      "  In this case, **method** is a code that triggers special actions, such as getting an id or something that is available to the   program that fills the lexicon table\n",
      "* the name of an other field as shown in the **name** part of the specification. \n",
      "  In this case, **method** must be a function, defined else where, that takes the value of that other field as argument. \n",
      "  The function is typically a transliteration, or a stripping action.\n",
      "\n",
      "## method\n",
      "May contain one of the following:\n",
      "\n",
      "* a code (string), indicating:\n",
      "    * ``lex``: take the value of a feature (indicated in **source**) for this entry from the lexicon file\n",
      "    * ``entry``: take the value of the entry itself as found in the lexicon file\n",
      "    * ``id``: take the id for this entry as generated by the program\n",
      "    * ``lan``: take the language of this entry\n",
      "* a function taking one argument\n",
      "    * *strip_id*: strip the non-lexeme characters at the end of the entry (the ``/ [ =`` characters)\n",
      "    * *to_heb*: transform the transliteration into real unicode Hebrew\n",
      "\n",
      "## name\n",
      "The name of the field in the to be constructed annotation file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def strip_id(entryid):\n",
      "    return entryid.rstrip('/[=')\n",
      "\n",
      "def to_heb(translit):\n",
      "    return Transcription.to_hebrew(Transcription.suffix_and_finales(translit)[0])\n",
      "\n",
      "lex_fields = (\n",
      "    (None, 'id', 'id', None),\n",
      "    (None, 'lan', 'lan', None),\n",
      "    (None, 'entry', 'entryid', None),\n",
      "    ('entryid', strip_id, 'entry', None),\n",
      "    ('entry', to_heb, 'entry_heb', None),\n",
      "    ('vc', 'lex', 'g_entry', None),\n",
      "    ('g_entry', to_heb, 'g_entry_heb', None),\n",
      "    ('rt', 'lex', 'root', None),\n",
      "    ('sp', 'lex', 'pos', None),\n",
      "    ('sm', 'lex', 'nametype', None),\n",
      "    ('ls', 'lex', 'subpos', None),\n",
      "    ('gl', 'lex', 'gloss', None),\n",
      ")\n",
      "\n",
      "cur_lex_values = {}\n",
      "\n",
      "def compute_fields(lan, entry, lid, lexfeats):\n",
      "    cur_lex_values.clear()\n",
      "    return tuple(compute_field(lan, entry, lid, lexfeats, f) for f in lex_fields)\n",
      "\n",
      "def compute_field(lan, entry, lid, lexfeats, f):\n",
      "    (source, method, name, transform) = f\n",
      "    val = None\n",
      "    if method == 'lan': val = lan\n",
      "    elif method == 'entry': val = entry\n",
      "    elif method == 'id': val = lid\n",
      "    elif method =='lex':\n",
      "        val = lexfeats.get(f[0], '')\n",
      "        if transform != None and val in transform: val = transform[val]\n",
      "    else: val = method(cur_lex_values[f[0]])\n",
      "    cur_lex_values[f[2]] = val\n",
      "    return val\n",
      "\n",
      "lex_index = {}\n",
      "cur_id = -1\n",
      "for lan in sorted(lex_entries):\n",
      "    for entry in sorted(lex_entries[lan]):\n",
      "        cur_id += 1\n",
      "        entry_info = compute_fields(lan, entry, cur_id, lex_entries[lan][entry])\n",
      "        lex_index[(lan, entry)] = entry_info"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lex = ExtraData(API)\n",
      "\n",
      "def get_lex(dummy):\n",
      "    data = []\n",
      "    for n in sorted(node_lex):\n",
      "        this_info = lex_index[node_lex[n]]\n",
      "        data.append((n,) + this_info)\n",
      "    return data\n",
      "\n",
      "msg(\"Writing annotation file ...\")\n",
      "lex.deliver_annots('lexicon/lex_data', 'lexicon', 'lex', get_lex, (\n",
      "        ('etcbc4', 'lex', 'id'),\n",
      "        ('etcbc4', 'lex', 'lan'),\n",
      "        ('etcbc4', 'lex', 'entryid'),\n",
      "        ('etcbc4', 'lex', 'entry'),\n",
      "        ('etcbc4', 'lex', 'entry_heb'),\n",
      "        ('etcbc4', 'lex', 'g_entry'),\n",
      "        ('etcbc4', 'lex', 'g_entry_heb'),\n",
      "        ('etcbc4', 'lex', 'root'),\n",
      "        ('etcbc4', 'lex', 'pos'),\n",
      "        ('etcbc4', 'lex', 'nametype'),\n",
      "        ('etcbc4', 'lex', 'subpos'),\n",
      "        ('etcbc4', 'lex', 'gloss'),\n",
      "    ),\n",
      "    {'title': 'Lexicon lookups', 'date': '2014'},\n",
      ")\n",
      "msg(\"Done\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 57s Writing annotation file ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 2m 19s Done\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Checking: loading the new features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "API=fabric.load('etcbc4s', 'lexicon', 'gloss', {\n",
      "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
      "    \"features\": ('''\n",
      "        otype\n",
      "        book chapter verse\n",
      "        g_cons_utf8 g_word_utf8 g_word lex g_entry gloss\n",
      "    ''',\n",
      "    '''\n",
      "    '''),\n",
      "    \"prepare\": prepare,\n",
      "}, verbose='DETAIL')\n",
      "exec(fabric.localnames.format(var='fabric'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s LOADING API: please wait ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s DETAIL: COMPILING m: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.01s BEGIN COMPILE a: lexicon\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.01s DETAIL: load main: X. [node]  -> \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.17s DETAIL: load main: X. [e]  -> \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.96s DETAIL: load main: G.node_anchor_min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.02s DETAIL: load main: G.node_anchor_max\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.07s DETAIL: load main: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.12s DETAIL: load main: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.55s DETAIL: load main: G.edges_from\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.61s DETAIL: load main: G.edges_to\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.69s LOGFILE=/Users/dirk/laf-fabric-data/etcbc4s/bin/A/lexicon/__log__compile__.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.69s PARSING ANNOTATION FILES\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.70s INFO: parsing lex.xml\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 14s INFO: END PARSING\n",
        "         0 good   regions  and     0 faulty ones\n",
        "         0 linked nodes    and     0 unlinked ones\n",
        "         0 good   edges    and     0 faulty ones\n",
        "    426567 good   annots   and     0 faulty ones\n",
        "   5118804 good   features and     0 faulty ones\n",
        "    426567 distinct xml identifiers\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 14s MODELING RESULT FILES\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 14s INFO: CONNECTIVITY\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 14s WRITING RESULT FILES for a\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 14s DETAIL: write annox: F.etcbc4_lex_entry [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 15s DETAIL: write annox: F.etcbc4_lex_entry_heb [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 15s DETAIL: write annox: F.etcbc4_lex_entryid [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 16s DETAIL: write annox: F.etcbc4_lex_g_entry [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 16s DETAIL: write annox: F.etcbc4_lex_g_entry_heb [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 17s DETAIL: write annox: F.etcbc4_lex_gloss [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 17s DETAIL: write annox: F.etcbc4_lex_id [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 18s DETAIL: write annox: F.etcbc4_lex_lan [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 18s DETAIL: write annox: F.etcbc4_lex_nametype [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 18s DETAIL: write annox: F.etcbc4_lex_pos [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: write annox: F.etcbc4_lex_root [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: write annox: F.etcbc4_lex_subpos [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s END   COMPILE a: lexicon\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s INFO: USING DATA COMPILED AT: 2014-10-24T09-16-58\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear main: G.node_anchor_min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear main: G.node_anchor_max\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear main: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear main: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear main: G.edges_from\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear main: G.edges_to\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear main: F.etcbc4_db_otype [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear main: F.etcbc4_ft_g_cons_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear main: F.etcbc4_ft_g_word [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear main: F.etcbc4_ft_g_word_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear main: F.etcbc4_ft_lex [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear main: F.etcbc4_lex_g_entry [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear main: F.etcbc4_lex_gloss [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear main: F.etcbc4_sft_book [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear main: F.etcbc4_sft_chapter [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear main: F.etcbc4_sft_verse [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear annox: F.etcbc4_db_otype [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear annox: F.etcbc4_ft_g_cons_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear annox: F.etcbc4_ft_g_word [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear annox: F.etcbc4_ft_g_word_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear annox: F.etcbc4_ft_lex [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear annox: F.etcbc4_lex_g_entry [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear annox: F.etcbc4_lex_gloss [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear annox: F.etcbc4_sft_book [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear annox: F.etcbc4_sft_chapter [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: clear annox: F.etcbc4_sft_verse [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 19s DETAIL: load main: G.node_anchor_min\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 20s DETAIL: load main: G.node_anchor_max\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 20s DETAIL: load main: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 20s DETAIL: load main: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 20s DETAIL: load main: G.edges_from\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 20s DETAIL: load main: G.edges_to\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 20s DETAIL: load main: F.etcbc4_db_otype [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 21s DETAIL: load main: F.etcbc4_ft_g_cons_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 22s DETAIL: load main: F.etcbc4_ft_g_word [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 22s DETAIL: load main: F.etcbc4_ft_g_word_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 22s DETAIL: load main: F.etcbc4_ft_lex [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 23s DETAIL: load main: F.etcbc4_lex_g_entry [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 23s DETAIL: load main: F.etcbc4_lex_gloss [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 23s DETAIL: load main: F.etcbc4_sft_book [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 23s DETAIL: load main: F.etcbc4_sft_chapter [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 23s DETAIL: load main: F.etcbc4_sft_verse [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 23s DETAIL: load annox: F.etcbc4_db_otype [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 23s DETAIL: load annox: F.etcbc4_ft_g_cons_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 23s DETAIL: load annox: F.etcbc4_ft_g_word [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 23s DETAIL: load annox: F.etcbc4_ft_g_word_utf8 [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 23s DETAIL: load annox: F.etcbc4_ft_lex [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 23s DETAIL: load annox: F.etcbc4_lex_g_entry [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 23s DETAIL: load annox: F.etcbc4_lex_gloss [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 23s DETAIL: load annox: F.etcbc4_sft_book [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 23s DETAIL: load annox: F.etcbc4_sft_chapter [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 23s DETAIL: load annox: F.etcbc4_sft_verse [node] \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 23s DETAIL: prep prep: G.node_sort\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 23s DETAIL: prep prep: G.node_sort_inv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 24s INFO: DATA LOADED FROM SOURCE etcbc4s AND ANNOX lexicon FOR TASK gloss AT 2014-10-24T09-17-03\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Making an interlinear glossed text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "msg(\"Making interlinear glossed text ...\")\n",
      "\n",
      "trans_file = outfile('glossed.txt')\n",
      "cur_label = None\n",
      "cur_words = []\n",
      "\n",
      "(s_len, sg, se, sv, sw, sh) = (8,\n",
      "        'gloss = ', \n",
      "        'lexeme= ',\n",
      "        'voclex= ',\n",
      "        'trans = ',\n",
      "        'hebrew= ',\n",
      ")\n",
      "\n",
      "LL = 120\n",
      "\n",
      "def set_verse():\n",
      "    (first, cur_len, cg, ce, cv, cw, ch) = (True, s_len, sg, se, sv, sw, sh)\n",
      "\n",
      "    def set_line():\n",
      "        nonlocal first, cur_len, cur_len, cg, ce, cv, cw, ch\n",
      "        if cur_len != s_len:\n",
      "            trans_file.write('{}\\n'.format(('=' if first else '-') * cur_len))\n",
      "            for l in (ch, cw, cv, ce, cg):\n",
      "                trans_file.write('{}\\n'.format(l))\n",
      "        (first, cur_len, cg, ce, cv, cw, ch) = (False, s_len, sg, se, sv, sw, sh)\n",
      "\n",
      "    for n in cur_words:\n",
      "        (g, e, v, w, h) = (F.gloss.v(n), F.lex.v(n), F.g_entry.v(n), F.g_word.v(n), F.g_word_utf8.v(n))\n",
      "        (lg, le, lv, lw, lh) = tuple(len(x) for x in (g, e, v, w, h))\n",
      "        lb = max((lg, le, lv, lw, lh))\n",
      "        if cur_len + lb + 1 > LL: set_line()\n",
      "        cur_len += lb + 1\n",
      "        fmtstr = '{{:<{}}}|'.format(lb)\n",
      "        rfmtstr = '{{:>{}}}|'.format(lb)\n",
      "        cg += fmtstr.format(g)\n",
      "        ce += fmtstr.format(e)\n",
      "        cv += fmtstr.format(v)\n",
      "        cw += fmtstr.format(w)\n",
      "        ch += rfmtstr.format(h)\n",
      "        cur_len += 1\n",
      "    set_line()\n",
      "    cur_words.clear()\n",
      "\n",
      "for n in NN():\n",
      "    otype = F.otype.v(n)\n",
      "    if otype == 'verse':\n",
      "        set_verse()\n",
      "        cur_label = '{} {}:{}'.format(F.book.v(n), F.chapter.v(n), F.verse.v(n))\n",
      "        trans_file.write('\\n{}\\n'.format(cur_label))\n",
      "    elif otype == 'word':\n",
      "        cur_words.append(n)\n",
      "set_verse()\n",
      "\n",
      "trans_file.close()\n",
      "\n",
      "msg(\"Done\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 39s Making interlinear glossed text ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 48s Done\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -n 100 {my_file('glossed.txt')}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "Genesis 1:1\r\n",
        "========================================================================================================================\r\n",
        "hebrew=  \u05d1\u05b0\u05bc| \u05e8\u05b5\u05d0\u05e9\u05b4\u05c1\u0596\u05d9\u05ea| \u05d1\u05b8\u05bc\u05e8\u05b8\u05a3\u05d0|  \u05d0\u05b1\u05dc\u05b9\u05d4\u05b4\u0591\u05d9\u05dd|           \u05d0\u05b5\u05a5\u05ea| \u05d4\u05b7| \u05e9\u05b8\u05bc\u05c1\u05de\u05b7\u0596\u05d9\u05b4\u05dd| \u05d5\u05b0|           \u05d0\u05b5\u05a5\u05ea| \u05d4\u05b8|   \u05d0\u05b8\u05bd\u05e8\u05b6\u05e5|\r\n",
        "trans = B.:-|R;>CI73JT|B.@R@74>|>:ELOHI92JM|>;71T          |HA-|C.@MA73JIM|W:-|>;71T          |H@-|>@75REY00|\r\n",
        "voclex= B.: |R;>CIJT  |B.@R@>  |>:ELOHIJM  |>;T            |HA |C@MAJIM   |W: |>;T            |HA |>EREY    |\r\n",
        "lexeme= B   |R>CJT/   |BR>[    |>LHJM/     |>T             |H  |CMJM/     |W  |>T             |H  |>RY/     |\r\n",
        "gloss = in  |beginning|create  |god(s)     |<object marker>|the|heavens   |and|<object marker>|the|earth    |\r\n",
        "\r\n",
        "Genesis 1:2\r\n",
        "=====================================================================================================================\r\n",
        "hebrew=  \u05d5\u05b0| \u05d4\u05b8| \u05d0\u05b8\u0597\u05e8\u05b6\u05e5| \u05d4\u05b8\u05d9\u05b0\u05ea\u05b8\u05a5\u05d4|  \u05ea\u05b9\u05a8\u05d4\u05d5\u05bc\u0599| \u05d5\u05b8|   \u05d1\u05b9\u0594\u05d4\u05d5\u05bc| \u05d5\u05b0| \u05d7\u05b9\u0596\u05e9\u05b6\u05c1\u05da\u05b0| \u05e2\u05b7\u05dc| \u05e4\u05b0\u05bc\u05e0\u05b5\u05a3\u05d9|       \u05ea\u05b0\u05d4\u05b9\u0591\u05d5\u05dd| \u05d5\u05b0|\r\n",
        "trans = W:-|H@-|>@81REY|H@J:T@71H|TO33HW.03|W@-|BO80HW.  |W:-|XO73CEK:|<AL&|P.:N;74J|T:HO92WM      |W:-|\r\n",
        "voclex= W: |HA |>EREY  |H@J@H    |T.OHW.   |W: |B.OHW.   |W: |XOCEK:  |<AL |P.@NEH  |T.:HOWM       |W: |\r\n",
        "lexeme= W  |H  |>RY/   |HJH[     |THW/     |W  |BHW/     |W  |XCK/    |<L  |PNH/    |THWM/         |W  |\r\n",
        "gloss = and|the|earth  |be       |emptiness|and|emptiness|and|darkness|upon|face    |primeval ocean|and|\r\n",
        "----------------------------------------------------------------------------\r\n",
        "hebrew=  \u05e8\u05a3\u05d5\u05bc\u05d7\u05b7|  \u05d0\u05b1\u05dc\u05b9\u05d4\u05b4\u0594\u05d9\u05dd| \u05de\u05b0\u05e8\u05b7\u05d7\u05b6\u0596\u05e4\u05b6\u05ea| \u05e2\u05b7\u05dc| \u05e4\u05b0\u05bc\u05e0\u05b5\u05a5\u05d9| \u05d4\u05b7|   \u05de\u05b8\u05bc\u05bd\u05d9\u05b4\u05dd|\r\n",
        "trans = R74W.XA|>:ELOHI80JM|M:RAXE73PET|<AL&|P.:N;71J|HA-|M.@75JIM00|\r\n",
        "voclex= RW.XA  |>:ELOHIJM  |           |<AL |P.@NEH  |HA |MAJIM     |\r\n",
        "lexeme= RWX/   |>LHJM/     |RXP[       |<L  |PNH/    |H  |MJM/      |\r\n",
        "gloss = wind   |god(s)     |shake      |upon|face    |the|water     |\r\n",
        "\r\n",
        "Genesis 1:3\r\n",
        "===============================================================================\r",
        "\r\n",
        "hebrew=  \u05d5\u05b7| \u05d9\u05b9\u05bc\u05a5\u05d0\u05de\u05b6\u05e8|  \u05d0\u05b1\u05dc\u05b9\u05d4\u05b4\u0596\u05d9\u05dd| \u05d9\u05b0\u05d4\u05b4\u05a3\u05d9| \u05d0\u05b9\u0591\u05d5\u05e8|  \u05d5\u05b7\u05bd| \u05d9\u05b0\u05d4\u05b4\u05d9|   \u05d0\u05b9\u05bd\u05d5\u05e8|\r\n",
        "trans = WA-|J.O71>MER|>:ELOHI73JM|J:HI74J|>O92WR|WA75-|J:HIJ&|>O75WR00|\r\n",
        "voclex= W: |>@MAR    |>:ELOHIJM  |H@J@H  |>OWR  |W:   |H@J@H |>OWR    |\r\n",
        "lexeme= W  |>MR[     |>LHJM/     |HJH[   |>WR/  |W    |HJH[  |>WR/    |\r\n",
        "gloss = and|say      |god(s)     |be     |light |and  |be    |light   |\r\n",
        "\r\n",
        "Genesis 1:4\r\n",
        "=================================================================================================================\r\n",
        "hebrew=  \u05d5\u05b7| \u05d9\u05b7\u05bc\u05a7\u05e8\u05b0\u05d0|  \u05d0\u05b1\u05dc\u05b9\u05d4\u05b4\u059b\u05d9\u05dd|            \u05d0\u05b6\u05ea| \u05d4\u05b8| \u05d0\u05b9\u0596\u05d5\u05e8| \u05db\u05b4\u05bc\u05d9|  \u05d8\u05b9\u0591\u05d5\u05d1| \u05d5\u05b7| \u05d9\u05b7\u05bc\u05d1\u05b0\u05d3\u05b5\u05bc\u05a3\u05dc|  \u05d0\u05b1\u05dc\u05b9\u05d4\u05b4\u0594\u05d9\u05dd|\r\n",
        "trans = WA-|J.A94R:>|>:ELOHI91JM|>ET&           |H@-|>O73WR|K.IJ&|VO92WB |WA-|J.AB:D.;74L|>:ELOHI80JM|\r\n",
        "voclex= W: |R@>@H   |>:ELOHIJM  |>;T            |HA |>OWR  |K.IJ |VOWB   |W: |B.@DAL     |>:ELOHIJM  |\r\n",
        "lexeme= W  |R>H[    |>LHJM/     |>T             |H  |>WR/  |KJ   |VWB[   |W  |BDL[       |>LHJM/     |\r\n",
        "gloss = and|see     |god(s)     |<object marker>|the|light |that |be good|and|separate   |god(s)     |\r\n",
        "---------------------------------------------------------------\r\n",
        "hebrew=   \u05d1\u05b5\u05bc\u05a5\u05d9\u05df| \u05d4\u05b8| \u05d0\u05b9\u0596\u05d5\u05e8| \u05d5\u05bc|   \u05d1\u05b5\u05a5\u05d9\u05df| \u05d4\u05b7|   \u05d7\u05b9\u05bd\u05e9\u05b6\u05c1\u05da\u05b0|\r\n",
        "trans = B.;71JN |H@-|>O73WR|W.-|B;71JN  |HA-|XO75CEK:00|\r\n",
        "voclex= B.AJIN  |HA |>OWR  |W: |B.AJIN  |HA |XOCEK:    |\r\n",
        "lexeme= BJN/    |H  |>WR/  |W  |BJN/    |H  |XCK/      |\r\n",
        "gloss = interval|the|light |and|interval|the|darkness  |\r\n",
        "\r\n",
        "Genesis 1:5\r\n",
        "======================================================================================================================\r\n",
        "hebrew=  \u05d5\u05b7| \u05d9\u05b4\u05bc\u05e7\u05b0\u05e8\u05b8\u05a8\u05d0|    \u05d0\u05b1\u05dc\u05b9\u05d4\u05b4\u05a4\u05d9\u05dd| \u05dc\u05b8|   | \u05d0\u05b9\u05d5\u05e8\u0599| \u05d9\u05b9\u0594\u05d5\u05dd| \u05d5\u05b0| \u05dc\u05b7|   | \u05d7\u05b9\u0596\u05e9\u05b6\u05c1\u05da\u05b0| \u05e7\u05b8\u05a3\u05e8\u05b8\u05d0| \u05dc\u05b8\u0591\u05d9\u05b0\u05dc\u05b8\u05d4|  \u05d5\u05b7\u05bd|\r\n",
        "trans = WA-|J.IQ:R@63>|>:ELOHI70JM05|L@-|-  |>OWR03|JO80WM|W:-|LA-|-  |XO73CEK:|Q@74R@>|L@92J:L@H|WA75-|\r\n",
        "voclex= W: |Q@R@>     |>:ELOHIJM    |L: |HA |>OWR  |JOWM  |W: |L: |HA |XOCEK:  |Q@R@>  |LAJ:L@H  |W:   |\r\n",
        "lexeme= W  |QR>[      |>LHJM/       |L  |H  |>WR/  |JWM/  |W  |L  |H  |XCK/    |QR>[   |LJLH/    |W    |\r\n",
        "gloss = and|call      |god(s)       |to |the|light |day   |and|to |the|darkness|call   |night    |and  |\r\n",
        "----------------------------------------------------------------------\r\n",
        "hebrew=  \u05d9\u05b0\u05d4\u05b4\u05d9| \u05e2\u05b6\u05a5\u05e8\u05b6\u05d1|  \u05d5\u05b7\u05bd| \u05d9\u05b0\u05d4\u05b4\u05d9| \u05d1\u05b9\u0596\u05e7\u05b6\u05e8| \u05d9\u05b9\u05a5\u05d5\u05dd|     \u05d0\u05b6\u05d7\u05b8\u05bd\u05d3|\r\n",
        "trans = J:HIJ&|<E71REB|WA75-|J:HIJ&|BO73QER|JO71WM|>EX@75D00_P|\r\n",
        "voclex= H@J@H |<EREB  |W:   |H@J@H |B.OQER |JOWM  |>EX@D      |\r\n",
        "lexeme= HJH[  |<RB/   |W    |HJH[  |BQR=/  |JWM/  |>XD/       |\r\n",
        "gloss = be    |evening|and  |be    |morning|day   |one        |\r\n",
        "\r\n",
        "Genesis 1:6\r\n",
        "=================================================================================================================\r\n",
        "hebrew=  \u05d5\u05b7| \u05d9\u05b9\u05bc\u05a3\u05d0\u05de\u05b6\u05e8|  \u05d0\u05b1\u05dc\u05b9\u05d4\u05b4\u0594\u05d9\u05dd| \u05d9\u05b0\u05d4\u05b4\u05a5\u05d9| \u05e8\u05b8\u05e7\u05b4\u0596\u05d9\u05e2\u05b7| \u05d1\u05b0\u05bc| \u05ea\u05b9\u05a3\u05d5\u05da\u05b0| \u05d4\u05b7| \u05de\u05b8\u05bc\u0591\u05d9\u05b4\u05dd| \u05d5\u05b4| \u05d9\u05d4\u05b4\u05a3\u05d9| \u05de\u05b7\u05d1\u05b0\u05d3\u05b4\u05bc\u0594\u05d9\u05dc|\r\n",
        "trans = WA-|J.O74>MER|>:ELOHI80JM|J:HI71J|R@QI73J<A|B.:-|TO74WK:|HA-|M.@92JIM|WI-|JHI74J|MAB:D.I80JL|\r\n",
        "voclex= W: |>@MAR    |>:ELOHIJM  |H@J@H  |R@QIJ<A  |B.: |T.@WEK:|HA |MAJIM   |W: |H@J@H |B.@DAL     |\r\n",
        "lexeme= W  |>MR[     |>LHJM/     |HJH[   |RQJ</    |B   |TWK/   |H  |MJM/    |W  |HJH[  |BDL[       |\r\n",
        "gloss = and|say      |god(s)     |be     |firmament|in  |midst  |the|water   |and|be    |separate   |\r\n",
        "-------------------------------------------\r\n",
        "hebrew=   \u05d1\u05b5\u05bc\u05a5\u05d9\u05df| \u05de\u05b7\u0596\u05d9\u05b4\u05dd| \u05dc\u05b8|   \u05de\u05b8\u05bd\u05d9\u05b4\u05dd|\r\n",
        "trans = B.;71JN |MA73JIM|L@-|M@75JIM00|\r\n",
        "voclex= B.AJIN  |MAJIM  |L: |MAJIM    |\r\n",
        "lexeme= BJN/    |MJM/   |L  |MJM/     |\r\n",
        "gloss = interval|water  |to |water    |\r\n",
        "\r\n",
        "Genesis 1:7\r\n",
        "==================================================================================================================\r\n",
        "hebrew=  \u05d5\u05b7| \u05d9\u05b7\u05bc\u05a3\u05e2\u05b7\u05e9\u05c2|  \u05d0\u05b1\u05dc\u05b9\u05d4\u05b4\u05d9\u05dd\u05ae|            \u05d0\u05b6\u05ea| \u05d4\u05b8| \u05e8\u05b8\u05e7\u05b4\u05d9\u05e2\u05b7\u0592| \u05d5\u05b7| \u05d9\u05b7\u05bc\u05d1\u05b0\u05d3\u05b5\u05bc\u0597\u05dc|  \u05d1\u05b5\u05bc\u05a4\u05d9\u05df| \u05d4\u05b7|  \u05de\u05b7\u05bc\u05a8\u05d9\u05b4\u05dd\u0599|\r\n",
        "trans = WA-|J.A74<AF|>:ELOHIJM02|>ET&           |H@-|R@QIJ<A01|WA-|J.AB:D.;81L|B.;70JN |HA-|M.A33JIM03|\r\n",
        "voclex= W: |<@F@H   |>:ELOHIJM  |>;T            |HA |R@QIJ<A  |W: |B.@DAL     |B.AJIN  |HA |MAJIM     |\r\n",
        "lexeme= W  |<FH[    |>LHJM/     |>T             |H  |RQJ</    |W  |BDL[       |BJN/    |H  |MJM/      |\r\n",
        "gloss = and|make    |god(s)     |<object marker>|the|firmament|and|separate   |interval|the|water     |\r\n",
        "-----------------------------------------------------------------------------------------------------------------------\r\n",
        "hebrew=     \u05d0\u05b2\u05e9\u05b6\u05c1\u05e8\u0599|  \u05de\u05b4|   \u05ea\u05b7\u05bc\u05a3\u05d7\u05b7\u05ea| \u05dc\u05b8|   | \u05e8\u05b8\u05e7\u05b4\u0594\u05d9\u05e2\u05b7| \u05d5\u05bc|   \u05d1\u05b5\u05a3\u05d9\u05df| \u05d4\u05b7| \u05de\u05b7\u05bc\u0594\u05d9\u05b4\u05dd|    \u05d0\u05b2\u05e9\u05b6\u05c1\u0596\u05e8|  \u05de\u05b5| \u05e2\u05b7\u05a3\u05dc| \u05dc\u05b8|\r\n",
        "trans = >:ACER03  |MI- |T.A74XAT  |L@-|-  |R@QI80J<A|W.-|B;74JN  |HA-|M.A80JIM|>:ACE73R  |M;- |<A74L|L@-|\r\n",
        "voclex= >:ACER    |MIN |T.AXAT    |L: |HA |R@QIJ<A  |W: |B.AJIN  |HA |MAJIM   |>:ACER    |MIN |<AL  |L: |\r\n",
        "lexeme= >CR       |MN  |TXT/      |L  |H  |RQJ</    |W  |BJN/    |H  |MJM/    |>CR       |MN  |<L   |L  |\r\n",
        "gloss = <relative>|from|under part|to |the|firmament|and|interval|the|water   |<relative>|from|upon |to |\r\n",
        "------------------------------------------------\r\n",
        "hebrew=    | \u05e8\u05b8\u05e7\u05b4\u0591\u05d9\u05e2\u05b7|  \u05d5\u05b7\u05bd| \u05d9\u05b0\u05d4\u05b4\u05d9|   \u05db\u05b5\u05bd\u05df|\r\n",
        "trans = -  |R@QI92J<A|WA75-|J:HIJ&|K;75N00|\r\n",
        "voclex= HA |R@QIJ<A  |W:   |H@J@H |K.;N   |\r\n",
        "lexeme= H  |RQJ</    |W    |HJH[  |KN     |\r\n",
        "gloss = the|firmament|and  |be    |thus   |\r\n",
        "\r\n",
        "Genesis 1:8\r\n",
        "========================================================================================================================\r\n",
        "hebrew=  \u05d5\u05b7| \u05d9\u05b4\u05bc\u05e7\u05b0\u05e8\u05b8\u05a7\u05d0|  \u05d0\u05b1\u05dc\u05b9\u05d4\u05b4\u059b\u05d9\u05dd|  \u05dc\u05b8\u05bd|   | \u05e8\u05b8\u05e7\u05b4\u0596\u05d9\u05e2\u05b7| \u05e9\u05b8\u05c1\u05de\u05b8\u0591\u05d9\u05b4\u05dd|  \u05d5\u05b7\u05bd| \u05d9\u05b0\u05d4\u05b4\u05d9| \u05e2\u05b6\u05a5\u05e8\u05b6\u05d1|  \u05d5\u05b7\u05bd| \u05d9\u05b0\u05d4\u05b4\u05d9| \u05d1\u05b9\u0596\u05e7\u05b6\u05e8|\r\n",
        "trans = WA-|J.IQ:R@94>|>:ELOHI91JM|L@75-|-  |R@QI73J<A|C@M@92JIM|WA75-|J:HIJ&|<E71REB|WA75-|J:HIJ&|BO73QER|\r\n",
        "voclex= W: |Q@R@>     |>:ELOHIJM  |L:   |HA |R@QIJ<A  |C@MAJIM  |W:   |H@J@H |<EREB  |W:   |H@J@H |B.OQER |\r\n",
        "lexeme= W  |QR>[      |>LHJM/     |L    |H  |RQJ</    |CMJM/    |W    |HJH[  |<RB/   |W    |HJH[  |BQR=/  |\r\n",
        "gloss = and|call      |god(s)     |to   |the|firmament|heavens  |and  |be    |evening|and  |be    |morning|\r\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}